{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Documentation \u00b6 This directory hosts the build script for Simmate's online API documentation. You can access the live docs here . If you would like to host our documentation locally for offline access, then you can run the following commands: mkdocs build You can then open up the generated files to view the offline documentation.","title":"Documentation"},{"location":"#documentation","text":"This directory hosts the build script for Simmate's online API documentation. You can access the live docs here . If you would like to host our documentation locally for offline access, then you can run the following commands: mkdocs build You can then open up the generated files to view the offline documentation.","title":"Documentation"},{"location":"change_log/","text":"Check your installed version \u00b6 When you first import simmate and connect to the database, a warning is printed if you are not using the most current version available. You can also check this in the command-line: simmate version # example output Installed version : v0.10.0 Newest available : v0.11.1 Updating to a newer version \u00b6 We highly recommended that you install simmate into a clean conda environment, rather than updating simmate within your existing one: conda create -n my_env -c conda-forge python = 3 .10 simmate Make sure you check that the expected version is now installed: simmate version Rebuild your database with one compatible with the new installation: simmate database reset Tip If you use the same environment name as an existing one, conda will ask if you'd like to delete the existing environment before creating the new one. This will save you time from having to delete the env in a separate command. Version numbers & their meaning \u00b6 Our releases follow semantic versioning . This means versions (e.g. v1.2.3 ) correspond to MAJOR.MINOR.PATCH . Each version number increases after the following changes: MAJOR = incompatible API changes MINOR = new functionality is added (w/o API changes) PATCH = bug fixes and documentation updates (w/o API changes) There is one key exception to the rules above -- and that is with MAJOR =0 releases. Any v0.x.y release is considered developmental where APIs are subject to change and should not be considered stable. This is consistent with the semantic version spec (see point 4 ). Upcoming Release \u00b6 Tip For ongoing changes that have not been finalized/merged yet, view our active pull-requests on github Enhancements add relax_bulk and relax_endpoints parameters to optionally turn off pre-relaxations in NEB add CLEASE app for cluster expanison calculations (these workflows are highly experimental at the moment - so use with caution) update \"bad-elf\" workflow to accept an empty-atom template structure or a list of empty sites Fixes fix site ordering in NEB supercell structures v0.12.0 (2022.10.23) \u00b6 Enhancements add structure creators for ASE , GASP , PyXtal , AIRSS , CALYPSO , USPEX , and XtalOpt as well as documentation for creators. add simmate version command changelog and update guide added to documentation website add show-stats , delete-finished , and delete-all commands to workflow-engine add Cluster base class + commands that allow submitting a steady-state cluster via subprocesses or slurm add started_at , created_at , total_time , and queue_time columns to Calculation tables add exlcude_from_archives field to workflows to optionally delete files when compressing outputs to zip archives various improvements added for evolutionary search workflows, such as parameter optimization, new output files, and website views add Fingerprint database table and integrate it with Fingerprint validator support >2 element hull diargrams and complex chemical systems Refactors optimize get_series method of relaxation.vasp.staged reorganize selectors module for evolutionary structure prediction Fixes fix dynamic loading of toolkit structures from third-party databases fix race condition with workers and empty queues increases default query rate for state.result() to lessen database load v0.11.0 (2022.09.10) \u00b6 Enhancements REST API fields can now be specified directly with the api_filters attribute of any DatabaseTable class & fields from mix-ins are automatically added add archive_fields attribute that sets the \"raw data\" for the database table & fields from mix-ins are automatically added accept TOML input files in addition to YAML convergence plots and extras are now written for many workflow types (such as relaxations) when use_database=True , output files are automatically written and the workup method is directly paired with the database table. NEB workflow now accepts parameters to tune how distinct pathways are determined, including the max pathway length and cutoffs at 1D percolation. add MatplotlibFigure and PlotlyFigure classes to help with writing output files and also implementing these figures in the website UI update website to include workflow calculator types and add API links custom projects and database tables are now registered with Simmate and a intro guide has been added continued updates for structure-prediction workflows add inspection of methods for default input values and display them in metadata Refactors the website.core_components.filters module has been absorbed into the DatabaseTable class/module yaml input for custom workflows now matches the python input format workup methods are largely depreciated and now database entries are returned when a workflow has use_database=True several NEB input parameters have been renamed to accurate depict their meaning. customized workflow runs now save in the original database table under the \"-custom\" workflow name structure_string column renamed to structure to simplify api logic clean up toolkit.validators module and establish fingerprint base class calculators and workflows modules are now based on simmate apps Fixes fix bug in windows dev env where simmate run-server fails to find python path fix bug in workflows explore command where 'vasp' is the assumed calculator name fix broken example code in custom workflow docs fix broken website links and workflow views 0.11.1 (2022.09.12) fix transaction error with workers on a PostGres backend v0.10.0 (2022.08.29) \u00b6 Enhancements add NEB base classes to inherit from for making new subflows improve formatting of logging and cli using typer and rich cli now supports auto-completion to help with long commands add convergence_limit parameter to evolutionary search that works alongside limit_best_survival . This will absorb minor changes in energy with equivalent structures from prolonging the search. add ExtremeSymmetry transformation to attempt symmetry reduction on disordered structure account for structures in fixed-composition having fewer nsites than input becuase of symmetry reduction during relaxation. Also, add min_structures_exact parameter to ensure we have at least N structures with the expected number of sites add experimental variable-composition (variable refers to nsites, not stoichiometry) and binary-composition evolutionary searches allow custom workflows to run from yaml update MatProj data to new api, and add severl new columns for data (e.g. mag + band gap) Refactors isolate optional dependencies so that our install is smaller remove click in favor of higher-level package (typer) pre_standardize_structure and pre_sanitize_structure functionality is now merged in to a standardize_structure parameter that accepts different mode. symmetry_tolerance and angle_tolerance parameters can also modify the symmetry analysis done. metadata files are now numbered to allow multiple metadata files in the same directory refactor & clean up transformation module for readability remove SimmateFuture class and merge functionality into WorkItem switch from pdoc to mkdocs for documentation and remove get_doc_from_readme . Code and doc organization are now decoupled. rename run commands based on user preference. the run is now run-quick . run-yaml is now run . run-cloud now assumes a yaml input. remove tqdm dependency in favor of rich.progress refactor transformations to static methods Fixes fix module not found error by adding ASE to dependencies fix bug with postgres database trying to delete sqlite locally fix dask throwing errors with logging fix bug where fixed-composition searches fail to detect individuals that have been symmetrically reduced (and therefore have fewer nsites than expected) fix evolutionary search failures when writing output files while files are opened/locked fix NEB workflows failing due to Walltime handler fix NEB workflows hints for workup failure due to missing start/end image folders v0.9.0 (2022.08.17) \u00b6 Enhancements improve the warning associated with workflow failure because of \"command not found\" issues workers now ignore and reset tasks that fail with \"command not found\". 2 workers failing with this error will result in the WorkItem being canceled RandomWySites can now generate wyckoff combinations lazily (or up front) depending on use case add simmate utilities command group with archive-old-runs add start-cluster command for starting many local workers add structure-prediction workflows add plotting/output utilities to EvolutionarySearch and relaxation.vasp.staged Refactors evolutionary search now delay creations, transformations, and validation until runtime (used to be at time of structure submission) directory , compress_ouput , and run_id are now default input parameters for subclasses of Workflow . If these are unused, the run_config must include **kwargs add isort for organizing module imports throughout package Fixes fixed when source is not being registered by several workflows fix docker image for installing anaconda, blender, and simmate on ubuntu v0.8.0 (2022.08.11) \u00b6 Enhancements NEB workflows now accept parameters for changing supercell size and number of images used add HSE workflows for static energy, relaxation, and DOS/BS electronic states add NPT and MatProj molecular dynamics workflows add SCAN workflows for static energy and relaxation test files can be provided within zip files, fixing excessive line counts on git commits add simmate worker that can run \"out-of-box\" and requires no set up add logging for useful debugging and monitoring of workflows pinned dependencies to maximum versions and manage with dependabot Refactors to simplify the creation of new workflows, S3Task is now S3Workflow and database tables are dynamically determined using the workflow name workflows of a given type (e.g. relaxation or static-energy) now share database tables in order to simplify overall database architecture migrate from os.path to pathlib.Path throughout package isolate prefect use to separate executors updated tutorials for new workflow engine and workers remove use of setup.py in favor of pyproject.toml v0.7.0 (2022.07.19) \u00b6 Enhancements add guide for installing VASP v5 to Ubuntu v22.04 ( @scott-materials , #183 ) add simmate database load-remote-archives command and load_remote_archives utility that populates all tables from database.third_parties add load_default_sqlite3_build utility that downloads a pre-built database with all third-party data present. This is an alternative to calling load_all_remote_archives if you are using sqlite3 and saves a significant amount of time for users. standardize workflow naming. Note this breaks from python naming conventions for classes ( #150 ) dynamically determine register_kwargs and rename property to parameters_to_register add full-run unittests that call workflows and vasp (without emulation) add walltime error handler that properly shuts down calculations when a SLURM job is about to expire add option to restart workflows from a checkpoint automatically build api documentation using github actions Refactors refactor start-worker command to use prefect instead of the experimental django executor remove experimental workflow_engine.executor move contents of configuration.django.database to database.utilities upgraded to Prefect v2 (\"Orion\"). This involved the refactoring the entire workflow_engine module, and therefore the entire workflow library. ( #185 ). 0.7.1 (2022.07.19) fix incorrect handling of prefect v2 futures by workflows 0.7.2 (2022.08.03) fix missing SVG files for web UI ( #196 ). 0.7.3 (2022.08.04) fix incorrect passing of source in NEB all-paths workflow causing it to fail v0.6.0 (2022.06.25) \u00b6 Enhancements add AflowPrototypes to the database.third_parties module (only includes data distributed through pymatgen) add new modules to toolkit.structure_prediction and toolkit.creation , including ones to provide known , substitution , and prototype based structures. add created_at and updated_at columns to all database tables check if there is a newer version of Simmate available and let the user know about the update add experimental badelf workflow for determining electride character add electronic-structure workflow which carries out both DOS and BS calculations add database_obj attribute to the toolkit.Structure base class that is dynamically set Refactors standardize database_table attribute for workflows by merging calculation_table and result_table attributes ( #102 ) removed use of -s , -c , and -d shortcuts from the workflows commands refactor relaxation/staged workflow to run in single parent directory refactor evolutionary search algorithm (alpha feature) condense where parsing/deserialization of workflow parameters occurs to the refactored the load_input_and_register task. Originally, this would occur in multiple places (e.g. in the CLI module before submission, in the workflow run_cloud method, in the LoadInputAndRegister task, etc.) and involved boilerplate code. ( #173 ) refactor experimental features register_kwargs and customized workflows refactor LoadInputAndRegister and SaveOutputTask to load_input_and_register and save_result Fixes fix import for visualization.structure.blender module ( @bocklund , #180 ) fix bug where command or directory improperly passes None when they are not set in the simmate workflows run command fix bug where update_all_stabilities grabs incomplete calculations ( #177 ) fix bug where SCF calculation is not completed before the non-SCF DOS or BS calculation and causes the workflows to fail ( #171 ) fix bug for Bader workflow by registering the prebader workflow ( #174 ) fix bug where source is not determined with yaml-file submissions ( #172 ) v0.5.0 (2022.05.30) \u00b6 update CI to test all OSs and pin pytest<7.1 as temporary fix for #162 fix spelling typos in keyword_modifiers ( @laurenmm , #165 ) users can now apply their own unique keyword modifiers to Incars -- such as how we allow \"__per_atom\" or \"__smart_ismear\" tags on Incar settings. This change involved refactoring how keyword_modifiers are implemented for the vasp.inputs.Incar class. Rather than static methods attached to the base class, modifiers are now dynamically applied using the add_keyword_modifier classmethod. large update of calculators.vasp.tasks module where many new presets are reimplemented from pymatgen.io.vasp.sets . This includes robust unit testing to confirm that generated inputs match between simmate and pymatgen (see #157 for a list of presets) catch error with vasp freezing when Brmix handler switches to kerker mixing ( @becca9835 , #159 ) v0.4.0 (2022.04.24) \u00b6 add description_doc_short + show_parameters to workflows and use these to update the UI add django-allauth dependency for account management and google/github sign-ins archive directory as simmate_attempt_01.zip whenever an error handler is triggered depreciate the workflow parameter use_prev_directory in favor of copy_previous_directory v0.3.0 (2022.04.19) \u00b6 add highly customizable VASP workflow add Bader analysis and ELF workflows update module readmes to warn of experimental features reorganize toolkit module v0.2.0 (2022.04.15) \u00b6 start the CHANGELOG! refactor API views and add SimmateAPIViewSet class refactor simmate start-project command and underlying methods refactor simmate workflow-engine run-cluster command and underlying methods continue outlining file_converters module v0.1.4 (2022.04.12) \u00b6 web interface styling minor bug fixes v0.0.0 (2022.03.28) \u00b6 initial release adding tests and docs","title":"Changes & Updates"},{"location":"change_log/#check-your-installed-version","text":"When you first import simmate and connect to the database, a warning is printed if you are not using the most current version available. You can also check this in the command-line: simmate version # example output Installed version : v0.10.0 Newest available : v0.11.1","title":"Check your installed version"},{"location":"change_log/#updating-to-a-newer-version","text":"We highly recommended that you install simmate into a clean conda environment, rather than updating simmate within your existing one: conda create -n my_env -c conda-forge python = 3 .10 simmate Make sure you check that the expected version is now installed: simmate version Rebuild your database with one compatible with the new installation: simmate database reset Tip If you use the same environment name as an existing one, conda will ask if you'd like to delete the existing environment before creating the new one. This will save you time from having to delete the env in a separate command.","title":"Updating to a newer version"},{"location":"change_log/#version-numbers-their-meaning","text":"Our releases follow semantic versioning . This means versions (e.g. v1.2.3 ) correspond to MAJOR.MINOR.PATCH . Each version number increases after the following changes: MAJOR = incompatible API changes MINOR = new functionality is added (w/o API changes) PATCH = bug fixes and documentation updates (w/o API changes) There is one key exception to the rules above -- and that is with MAJOR =0 releases. Any v0.x.y release is considered developmental where APIs are subject to change and should not be considered stable. This is consistent with the semantic version spec (see point 4 ).","title":"Version numbers &amp; their meaning"},{"location":"change_log/#upcoming-release","text":"Tip For ongoing changes that have not been finalized/merged yet, view our active pull-requests on github Enhancements add relax_bulk and relax_endpoints parameters to optionally turn off pre-relaxations in NEB add CLEASE app for cluster expanison calculations (these workflows are highly experimental at the moment - so use with caution) update \"bad-elf\" workflow to accept an empty-atom template structure or a list of empty sites Fixes fix site ordering in NEB supercell structures","title":"Upcoming Release"},{"location":"change_log/#v0120-20221023","text":"Enhancements add structure creators for ASE , GASP , PyXtal , AIRSS , CALYPSO , USPEX , and XtalOpt as well as documentation for creators. add simmate version command changelog and update guide added to documentation website add show-stats , delete-finished , and delete-all commands to workflow-engine add Cluster base class + commands that allow submitting a steady-state cluster via subprocesses or slurm add started_at , created_at , total_time , and queue_time columns to Calculation tables add exlcude_from_archives field to workflows to optionally delete files when compressing outputs to zip archives various improvements added for evolutionary search workflows, such as parameter optimization, new output files, and website views add Fingerprint database table and integrate it with Fingerprint validator support >2 element hull diargrams and complex chemical systems Refactors optimize get_series method of relaxation.vasp.staged reorganize selectors module for evolutionary structure prediction Fixes fix dynamic loading of toolkit structures from third-party databases fix race condition with workers and empty queues increases default query rate for state.result() to lessen database load","title":"v0.12.0 (2022.10.23)"},{"location":"change_log/#v0110-20220910","text":"Enhancements REST API fields can now be specified directly with the api_filters attribute of any DatabaseTable class & fields from mix-ins are automatically added add archive_fields attribute that sets the \"raw data\" for the database table & fields from mix-ins are automatically added accept TOML input files in addition to YAML convergence plots and extras are now written for many workflow types (such as relaxations) when use_database=True , output files are automatically written and the workup method is directly paired with the database table. NEB workflow now accepts parameters to tune how distinct pathways are determined, including the max pathway length and cutoffs at 1D percolation. add MatplotlibFigure and PlotlyFigure classes to help with writing output files and also implementing these figures in the website UI update website to include workflow calculator types and add API links custom projects and database tables are now registered with Simmate and a intro guide has been added continued updates for structure-prediction workflows add inspection of methods for default input values and display them in metadata Refactors the website.core_components.filters module has been absorbed into the DatabaseTable class/module yaml input for custom workflows now matches the python input format workup methods are largely depreciated and now database entries are returned when a workflow has use_database=True several NEB input parameters have been renamed to accurate depict their meaning. customized workflow runs now save in the original database table under the \"-custom\" workflow name structure_string column renamed to structure to simplify api logic clean up toolkit.validators module and establish fingerprint base class calculators and workflows modules are now based on simmate apps Fixes fix bug in windows dev env where simmate run-server fails to find python path fix bug in workflows explore command where 'vasp' is the assumed calculator name fix broken example code in custom workflow docs fix broken website links and workflow views 0.11.1 (2022.09.12) fix transaction error with workers on a PostGres backend","title":"v0.11.0 (2022.09.10)"},{"location":"change_log/#v0100-20220829","text":"Enhancements add NEB base classes to inherit from for making new subflows improve formatting of logging and cli using typer and rich cli now supports auto-completion to help with long commands add convergence_limit parameter to evolutionary search that works alongside limit_best_survival . This will absorb minor changes in energy with equivalent structures from prolonging the search. add ExtremeSymmetry transformation to attempt symmetry reduction on disordered structure account for structures in fixed-composition having fewer nsites than input becuase of symmetry reduction during relaxation. Also, add min_structures_exact parameter to ensure we have at least N structures with the expected number of sites add experimental variable-composition (variable refers to nsites, not stoichiometry) and binary-composition evolutionary searches allow custom workflows to run from yaml update MatProj data to new api, and add severl new columns for data (e.g. mag + band gap) Refactors isolate optional dependencies so that our install is smaller remove click in favor of higher-level package (typer) pre_standardize_structure and pre_sanitize_structure functionality is now merged in to a standardize_structure parameter that accepts different mode. symmetry_tolerance and angle_tolerance parameters can also modify the symmetry analysis done. metadata files are now numbered to allow multiple metadata files in the same directory refactor & clean up transformation module for readability remove SimmateFuture class and merge functionality into WorkItem switch from pdoc to mkdocs for documentation and remove get_doc_from_readme . Code and doc organization are now decoupled. rename run commands based on user preference. the run is now run-quick . run-yaml is now run . run-cloud now assumes a yaml input. remove tqdm dependency in favor of rich.progress refactor transformations to static methods Fixes fix module not found error by adding ASE to dependencies fix bug with postgres database trying to delete sqlite locally fix dask throwing errors with logging fix bug where fixed-composition searches fail to detect individuals that have been symmetrically reduced (and therefore have fewer nsites than expected) fix evolutionary search failures when writing output files while files are opened/locked fix NEB workflows failing due to Walltime handler fix NEB workflows hints for workup failure due to missing start/end image folders","title":"v0.10.0 (2022.08.29)"},{"location":"change_log/#v090-20220817","text":"Enhancements improve the warning associated with workflow failure because of \"command not found\" issues workers now ignore and reset tasks that fail with \"command not found\". 2 workers failing with this error will result in the WorkItem being canceled RandomWySites can now generate wyckoff combinations lazily (or up front) depending on use case add simmate utilities command group with archive-old-runs add start-cluster command for starting many local workers add structure-prediction workflows add plotting/output utilities to EvolutionarySearch and relaxation.vasp.staged Refactors evolutionary search now delay creations, transformations, and validation until runtime (used to be at time of structure submission) directory , compress_ouput , and run_id are now default input parameters for subclasses of Workflow . If these are unused, the run_config must include **kwargs add isort for organizing module imports throughout package Fixes fixed when source is not being registered by several workflows fix docker image for installing anaconda, blender, and simmate on ubuntu","title":"v0.9.0 (2022.08.17)"},{"location":"change_log/#v080-20220811","text":"Enhancements NEB workflows now accept parameters for changing supercell size and number of images used add HSE workflows for static energy, relaxation, and DOS/BS electronic states add NPT and MatProj molecular dynamics workflows add SCAN workflows for static energy and relaxation test files can be provided within zip files, fixing excessive line counts on git commits add simmate worker that can run \"out-of-box\" and requires no set up add logging for useful debugging and monitoring of workflows pinned dependencies to maximum versions and manage with dependabot Refactors to simplify the creation of new workflows, S3Task is now S3Workflow and database tables are dynamically determined using the workflow name workflows of a given type (e.g. relaxation or static-energy) now share database tables in order to simplify overall database architecture migrate from os.path to pathlib.Path throughout package isolate prefect use to separate executors updated tutorials for new workflow engine and workers remove use of setup.py in favor of pyproject.toml","title":"v0.8.0 (2022.08.11)"},{"location":"change_log/#v070-20220719","text":"Enhancements add guide for installing VASP v5 to Ubuntu v22.04 ( @scott-materials , #183 ) add simmate database load-remote-archives command and load_remote_archives utility that populates all tables from database.third_parties add load_default_sqlite3_build utility that downloads a pre-built database with all third-party data present. This is an alternative to calling load_all_remote_archives if you are using sqlite3 and saves a significant amount of time for users. standardize workflow naming. Note this breaks from python naming conventions for classes ( #150 ) dynamically determine register_kwargs and rename property to parameters_to_register add full-run unittests that call workflows and vasp (without emulation) add walltime error handler that properly shuts down calculations when a SLURM job is about to expire add option to restart workflows from a checkpoint automatically build api documentation using github actions Refactors refactor start-worker command to use prefect instead of the experimental django executor remove experimental workflow_engine.executor move contents of configuration.django.database to database.utilities upgraded to Prefect v2 (\"Orion\"). This involved the refactoring the entire workflow_engine module, and therefore the entire workflow library. ( #185 ). 0.7.1 (2022.07.19) fix incorrect handling of prefect v2 futures by workflows 0.7.2 (2022.08.03) fix missing SVG files for web UI ( #196 ). 0.7.3 (2022.08.04) fix incorrect passing of source in NEB all-paths workflow causing it to fail","title":"v0.7.0 (2022.07.19)"},{"location":"change_log/#v060-20220625","text":"Enhancements add AflowPrototypes to the database.third_parties module (only includes data distributed through pymatgen) add new modules to toolkit.structure_prediction and toolkit.creation , including ones to provide known , substitution , and prototype based structures. add created_at and updated_at columns to all database tables check if there is a newer version of Simmate available and let the user know about the update add experimental badelf workflow for determining electride character add electronic-structure workflow which carries out both DOS and BS calculations add database_obj attribute to the toolkit.Structure base class that is dynamically set Refactors standardize database_table attribute for workflows by merging calculation_table and result_table attributes ( #102 ) removed use of -s , -c , and -d shortcuts from the workflows commands refactor relaxation/staged workflow to run in single parent directory refactor evolutionary search algorithm (alpha feature) condense where parsing/deserialization of workflow parameters occurs to the refactored the load_input_and_register task. Originally, this would occur in multiple places (e.g. in the CLI module before submission, in the workflow run_cloud method, in the LoadInputAndRegister task, etc.) and involved boilerplate code. ( #173 ) refactor experimental features register_kwargs and customized workflows refactor LoadInputAndRegister and SaveOutputTask to load_input_and_register and save_result Fixes fix import for visualization.structure.blender module ( @bocklund , #180 ) fix bug where command or directory improperly passes None when they are not set in the simmate workflows run command fix bug where update_all_stabilities grabs incomplete calculations ( #177 ) fix bug where SCF calculation is not completed before the non-SCF DOS or BS calculation and causes the workflows to fail ( #171 ) fix bug for Bader workflow by registering the prebader workflow ( #174 ) fix bug where source is not determined with yaml-file submissions ( #172 )","title":"v0.6.0 (2022.06.25)"},{"location":"change_log/#v050-20220530","text":"update CI to test all OSs and pin pytest<7.1 as temporary fix for #162 fix spelling typos in keyword_modifiers ( @laurenmm , #165 ) users can now apply their own unique keyword modifiers to Incars -- such as how we allow \"__per_atom\" or \"__smart_ismear\" tags on Incar settings. This change involved refactoring how keyword_modifiers are implemented for the vasp.inputs.Incar class. Rather than static methods attached to the base class, modifiers are now dynamically applied using the add_keyword_modifier classmethod. large update of calculators.vasp.tasks module where many new presets are reimplemented from pymatgen.io.vasp.sets . This includes robust unit testing to confirm that generated inputs match between simmate and pymatgen (see #157 for a list of presets) catch error with vasp freezing when Brmix handler switches to kerker mixing ( @becca9835 , #159 )","title":"v0.5.0 (2022.05.30)"},{"location":"change_log/#v040-20220424","text":"add description_doc_short + show_parameters to workflows and use these to update the UI add django-allauth dependency for account management and google/github sign-ins archive directory as simmate_attempt_01.zip whenever an error handler is triggered depreciate the workflow parameter use_prev_directory in favor of copy_previous_directory","title":"v0.4.0 (2022.04.24)"},{"location":"change_log/#v030-20220419","text":"add highly customizable VASP workflow add Bader analysis and ELF workflows update module readmes to warn of experimental features reorganize toolkit module","title":"v0.3.0 (2022.04.19)"},{"location":"change_log/#v020-20220415","text":"start the CHANGELOG! refactor API views and add SimmateAPIViewSet class refactor simmate start-project command and underlying methods refactor simmate workflow-engine run-cluster command and underlying methods continue outlining file_converters module","title":"v0.2.0 (2022.04.15)"},{"location":"change_log/#v014-20220412","text":"web interface styling minor bug fixes","title":"v0.1.4 (2022.04.12)"},{"location":"change_log/#v000-20220328","text":"initial release adding tests and docs","title":"v0.0.0 (2022.03.28)"},{"location":"home/","text":"Welcome! \u00b6 Warning For Simmate's workflow module, we are currently reliant on VASP, which is an expensive DFT software that can be difficult to install for beginners. We are working to move away from propriatary softwares and toward free/open-source codes like ABINIT, Quantum Espresso, or DFTK.jl. That way you can install Simmate and we will take care of the rest. This will take time though... so we recommend that fully experimental labs wait until Simmate hits this milestone. If you'd like to be notified when this occurs, send us an email at simmate.team@gmail.com. Before you begin \u00b6 If you are new to Simmate, jump over to our main website simmate.org and take a look at what we have to offer. This page is for when you're ready to use Simmate in your own research and access some advanced functionality. Our software is open-source and free to use, so come back to try it when you're ready! What is Simmate? \u00b6 The Simulated Materials Ecosystem (Simmate) is a toolbox and framework for computational materials research. It lets you explore various crystal databases, predict new materials, and quickly calculate properties (electronic, elastic, thermodynamic, and more). Computational research can be intimidating because there are so many programs to choose from, and it's hard to mix-and-match them for your specific project. Simmate aims to be the glue between all these different programs, databases, and utilities. We do the heavy lifting and explain these other programs to you along the way. Even if you consider yourself an experimentalist and have little-to-no coding experience, Simmate's barrier to entry is built to be as low as possible. Our web interface can generate property predictions with a single mouse click. And for learning how to code, we wrote our tutorials and documentation for those that have never used python before. At the other end of the spectrum, we provide an extremely powerful toolbox and API for experts. Those familiar with the field can view Simmate as an alternative to the Materials Project stack ( Atomate , PyMatGen , MatMiner , and more ), where we operate under a very different coding philosphy. Here, usability and readability are our top priorities. We therefore distribute Simmate as an \"all-in-one\" package rather than many separate programs. This includes a core material science toolkit, workflow management, database orm, and a website interface. Simmate also focuses heavily on cloud-based storage , which enables large scale collaborations and avoids researchers repeating calculations. To learn more about the different design choices made in Simmate compared to competing codes, read through our comparisons and benchmarks page . Installation \u00b6 Don't panic if you're new to coding and Python. When you're ready, head to our tutorials where we teach you everything from the beginning. If you're comfortable with Python, you can install Simmate with... conda install -c conda-forge simmate Note Simmate itself is <2MB, but when installed to a clean conda environment, the entire download for Simmate and all it's dependencies comes to ~1.2GB. Additional disk space is also needed for optional downloads -- such as third-party data . Running a Server \u00b6 Once installed, running a local test server is as simple as... # On first-time setup, you must intialize an empty database. simmate database reset # then start the server! simmate run-server After a few seconds, you can open http://127.0.0.1:8000/ in your browser to view your local server! Tip Read our website tutorials and documentation in order to switch to a production-ready server that's accessible through the internet and can be shared among a team. A Sneak-Peak of Features \u00b6 Again, take a look at our main website if you'd like to see the end-result of what Simmate has to offer. There are many more functions and utilities once you download Simmate, so this section showcases a few of those features. Prebuilt Workflows \u00b6 All of the most common material properties have workflows ready to go. These range from simple XRD pattern predictions to intensive dynamic simulations. Simmate also builds off of Prefect for orchestrating and managing workflows. This means that it's up to you whether to run jobs via (i) an advanced user-interface, (ii) the command-line, or (iii) in custom python scripts: command line yaml toml python # The command line let's you quickly run a workflow # from a structure file (CIF or POSCAR). simmate workflows run relaxation.vasp.matproj --structure NaCl.cif # Workflows can also be ran from YAML-based configuration # files, such as the one shown here (named `example.yaml`). # This would be submitted with the command: # `simmate workflows run example.yaml` workflow_name : relaxation.vasp.matproj structure : NaCl.cif command : mpirun -n 8 vasp_std > vasp.out # Workflows can also be ran from TOML-based configuration # files, such as the one shown here (named `example.toml`). # This would be submitted with the command: # `simmate workflows run example.toml` workflow_name = \"relaxation.vasp.matproj\" structure = \"NaCl.cif\" command = \"mpirun -n 8 vasp_std > vasp.out\" # Python let's you run workflows within scripts # which enables advanced setting configurations. from simmate.workflows.relaxation import Relaxation__Vasp__Matproj as workflow state = workflow . run ( structure = \"NaCl.cif\" ) result = state . result () Full-Feature Database \u00b6 Using all the data on our official site along with your own private data, you can take advantage of Simmate's extremely powerful database that is built off of Django ORM . Simmate also brings together third-party databases and their data -- including those like the COD, Materials Project, JARVIS, and others. With so much data, being able to easily download and navigate it is critical: # Be sure to follow the database tutorial where we build our # initial database with the command `simmate database reset` from simmate.database import connect from simmate.database.third_parties import MatprojStructure # EXAMPLE 1: all structures that have less than 6 sites in their unitcell structures = MatprojStructure . objects . filter ( nsites__lt = 6 ) . all () # EXAMPLE 2: complex filtering structures = MatprojStructure . objects . filter ( nsites__gte = 3 , # greater or equal to 3 sites energy__isnull = False , # the structure DOES have an energy density__range = ( 1 , 5 ), # density is between 1 and 5 elements__icontains = '\"C\"' , # the structure includes the element Carbon spacegroup__number = 167 , # the spacegroup number is 167 ) . all () # Quickly convert to excel, a pandas dataframe, or toolkit structures. df = structures . to_dataframe () structures = structures . to_toolkit () Utilities & Toolbox \u00b6 A lot of times in research, a new method is needed to analyze a structure, so a prebuilt workflow won't exist for you yet. Here, you'll need common functions ready to go (such as grabbing the volume of a crystal or running symmetry analysis). Our toolkit functions and classes largely inherit from PyMatGen , which gives you a wide variety of functions to use: # Load the structure file you'd like to use from simmate.toolkit import Structure structure = Structure . from_file ( \"NaCl.cif\" ) # Access a wide variety of properties. Here are some simple ones. structure . density structure . composition . reduced_formula structure . lattice . volume # Also access methods that run deformations or analysis on your structure. structure . make_supercell ([ 2 , 2 , 3 ]) structure . get_primitive_structure () structure . add_oxidation_state_by_guess () Scalable to Clusters \u00b6 At the beginning of a project, you may want to write and run code on a single computer and single core. But as you run into some intense calculations, you may want to use all of your CPU and GPU to run calculations. At the extreme, some projects require thousands of computers across numerous locations, including university clusters (using SLURM or PBS) and cloud computing (using Kubernetes and Docker). Simmate can meet all of these needs thanks to integration with a custom SimmateExecutor (the default), Dask , and/or Prefect : schedule jobs add remote resources # On your local computer, schedule your workflow run. # This is as easy as replacing \"run\" with \"run_cloud\". # This returns a \"future-like\" object. state = workflow . run_cloud ( ... ) # Calling result will wait until the job completes # and grab the result! Note, the job won't run # until you start a worker that is connected to the # same database (see command below) result = state . result () # In a separate terminal or even on a remote HPC cluster, you # can start a worker that will start running any scheduled jobs simmate workflow-engine start-worker Need help? \u00b6 Post your question here in our discussion section . Even if it's something like \" How do I download all structures with x, y, and z properties? \", let us help out and point you in the right direction! Extra resources \u00b6 Requesting a new feature Exploring alternatives to Simmate Citing Simmate","title":"Home"},{"location":"home/#welcome","text":"Warning For Simmate's workflow module, we are currently reliant on VASP, which is an expensive DFT software that can be difficult to install for beginners. We are working to move away from propriatary softwares and toward free/open-source codes like ABINIT, Quantum Espresso, or DFTK.jl. That way you can install Simmate and we will take care of the rest. This will take time though... so we recommend that fully experimental labs wait until Simmate hits this milestone. If you'd like to be notified when this occurs, send us an email at simmate.team@gmail.com.","title":"Welcome!"},{"location":"home/#before-you-begin","text":"If you are new to Simmate, jump over to our main website simmate.org and take a look at what we have to offer. This page is for when you're ready to use Simmate in your own research and access some advanced functionality. Our software is open-source and free to use, so come back to try it when you're ready!","title":"Before you begin"},{"location":"home/#what-is-simmate","text":"The Simulated Materials Ecosystem (Simmate) is a toolbox and framework for computational materials research. It lets you explore various crystal databases, predict new materials, and quickly calculate properties (electronic, elastic, thermodynamic, and more). Computational research can be intimidating because there are so many programs to choose from, and it's hard to mix-and-match them for your specific project. Simmate aims to be the glue between all these different programs, databases, and utilities. We do the heavy lifting and explain these other programs to you along the way. Even if you consider yourself an experimentalist and have little-to-no coding experience, Simmate's barrier to entry is built to be as low as possible. Our web interface can generate property predictions with a single mouse click. And for learning how to code, we wrote our tutorials and documentation for those that have never used python before. At the other end of the spectrum, we provide an extremely powerful toolbox and API for experts. Those familiar with the field can view Simmate as an alternative to the Materials Project stack ( Atomate , PyMatGen , MatMiner , and more ), where we operate under a very different coding philosphy. Here, usability and readability are our top priorities. We therefore distribute Simmate as an \"all-in-one\" package rather than many separate programs. This includes a core material science toolkit, workflow management, database orm, and a website interface. Simmate also focuses heavily on cloud-based storage , which enables large scale collaborations and avoids researchers repeating calculations. To learn more about the different design choices made in Simmate compared to competing codes, read through our comparisons and benchmarks page .","title":"What is Simmate?"},{"location":"home/#installation","text":"Don't panic if you're new to coding and Python. When you're ready, head to our tutorials where we teach you everything from the beginning. If you're comfortable with Python, you can install Simmate with... conda install -c conda-forge simmate Note Simmate itself is <2MB, but when installed to a clean conda environment, the entire download for Simmate and all it's dependencies comes to ~1.2GB. Additional disk space is also needed for optional downloads -- such as third-party data .","title":"Installation"},{"location":"home/#running-a-server","text":"Once installed, running a local test server is as simple as... # On first-time setup, you must intialize an empty database. simmate database reset # then start the server! simmate run-server After a few seconds, you can open http://127.0.0.1:8000/ in your browser to view your local server! Tip Read our website tutorials and documentation in order to switch to a production-ready server that's accessible through the internet and can be shared among a team.","title":"Running a Server"},{"location":"home/#a-sneak-peak-of-features","text":"Again, take a look at our main website if you'd like to see the end-result of what Simmate has to offer. There are many more functions and utilities once you download Simmate, so this section showcases a few of those features.","title":"A Sneak-Peak of Features"},{"location":"home/#prebuilt-workflows","text":"All of the most common material properties have workflows ready to go. These range from simple XRD pattern predictions to intensive dynamic simulations. Simmate also builds off of Prefect for orchestrating and managing workflows. This means that it's up to you whether to run jobs via (i) an advanced user-interface, (ii) the command-line, or (iii) in custom python scripts: command line yaml toml python # The command line let's you quickly run a workflow # from a structure file (CIF or POSCAR). simmate workflows run relaxation.vasp.matproj --structure NaCl.cif # Workflows can also be ran from YAML-based configuration # files, such as the one shown here (named `example.yaml`). # This would be submitted with the command: # `simmate workflows run example.yaml` workflow_name : relaxation.vasp.matproj structure : NaCl.cif command : mpirun -n 8 vasp_std > vasp.out # Workflows can also be ran from TOML-based configuration # files, such as the one shown here (named `example.toml`). # This would be submitted with the command: # `simmate workflows run example.toml` workflow_name = \"relaxation.vasp.matproj\" structure = \"NaCl.cif\" command = \"mpirun -n 8 vasp_std > vasp.out\" # Python let's you run workflows within scripts # which enables advanced setting configurations. from simmate.workflows.relaxation import Relaxation__Vasp__Matproj as workflow state = workflow . run ( structure = \"NaCl.cif\" ) result = state . result ()","title":"Prebuilt Workflows"},{"location":"home/#full-feature-database","text":"Using all the data on our official site along with your own private data, you can take advantage of Simmate's extremely powerful database that is built off of Django ORM . Simmate also brings together third-party databases and their data -- including those like the COD, Materials Project, JARVIS, and others. With so much data, being able to easily download and navigate it is critical: # Be sure to follow the database tutorial where we build our # initial database with the command `simmate database reset` from simmate.database import connect from simmate.database.third_parties import MatprojStructure # EXAMPLE 1: all structures that have less than 6 sites in their unitcell structures = MatprojStructure . objects . filter ( nsites__lt = 6 ) . all () # EXAMPLE 2: complex filtering structures = MatprojStructure . objects . filter ( nsites__gte = 3 , # greater or equal to 3 sites energy__isnull = False , # the structure DOES have an energy density__range = ( 1 , 5 ), # density is between 1 and 5 elements__icontains = '\"C\"' , # the structure includes the element Carbon spacegroup__number = 167 , # the spacegroup number is 167 ) . all () # Quickly convert to excel, a pandas dataframe, or toolkit structures. df = structures . to_dataframe () structures = structures . to_toolkit ()","title":"Full-Feature Database"},{"location":"home/#utilities-toolbox","text":"A lot of times in research, a new method is needed to analyze a structure, so a prebuilt workflow won't exist for you yet. Here, you'll need common functions ready to go (such as grabbing the volume of a crystal or running symmetry analysis). Our toolkit functions and classes largely inherit from PyMatGen , which gives you a wide variety of functions to use: # Load the structure file you'd like to use from simmate.toolkit import Structure structure = Structure . from_file ( \"NaCl.cif\" ) # Access a wide variety of properties. Here are some simple ones. structure . density structure . composition . reduced_formula structure . lattice . volume # Also access methods that run deformations or analysis on your structure. structure . make_supercell ([ 2 , 2 , 3 ]) structure . get_primitive_structure () structure . add_oxidation_state_by_guess ()","title":"Utilities &amp; Toolbox"},{"location":"home/#scalable-to-clusters","text":"At the beginning of a project, you may want to write and run code on a single computer and single core. But as you run into some intense calculations, you may want to use all of your CPU and GPU to run calculations. At the extreme, some projects require thousands of computers across numerous locations, including university clusters (using SLURM or PBS) and cloud computing (using Kubernetes and Docker). Simmate can meet all of these needs thanks to integration with a custom SimmateExecutor (the default), Dask , and/or Prefect : schedule jobs add remote resources # On your local computer, schedule your workflow run. # This is as easy as replacing \"run\" with \"run_cloud\". # This returns a \"future-like\" object. state = workflow . run_cloud ( ... ) # Calling result will wait until the job completes # and grab the result! Note, the job won't run # until you start a worker that is connected to the # same database (see command below) result = state . result () # In a separate terminal or even on a remote HPC cluster, you # can start a worker that will start running any scheduled jobs simmate workflow-engine start-worker","title":"Scalable to Clusters"},{"location":"home/#need-help","text":"Post your question here in our discussion section . Even if it's something like \" How do I download all structures with x, y, and z properties? \", let us help out and point you in the right direction!","title":"Need help?"},{"location":"home/#extra-resources","text":"Requesting a new feature Exploring alternatives to Simmate Citing Simmate","title":"Extra resources"},{"location":"parameters/","text":"Parameters \u00b6 Overview \u00b6 Knowing which parameters are available and how to use them is essential. We therefore outline all unique parameters for all workflows here. To see which parameters are allowed for a given workflow, you can use the explore command or workflow.show_parameters() : command line python simmate workflows explore workflow . show_parameters () If you need more details on a parameter or would like to request a new one, just let us know! You can then search for these parameters below to learn more about them. angle_tolerance \u00b6 If standardize_structure=True, then this is the cutoff value used to determine if the angles between sites are symmetrically equivalent. (in Degrees) yaml toml python angle_tolerance : 10.0 angle_tolerance = 10.0 angle_tolerance = 10.0 best_survival_cutoff \u00b6 For evolutionary searches, fixed compositions will be stopped when the best individual remains unbeaten for this number of new individuals. In order to absorb similar structures (e.g. identical structures but with minor energy differences), structures within the convergence_cutoff parameter (e.g. +1meV) are not considered when counting historical structures. This helps to prevent the search from continuing in cases where the search is likely already converged but making <0.1meV improvements. The default is typically set based on the number of atoms in the composition. yaml toml python best_survival_cutoff : 100 best_survival_cutoff = 100 best_survival_cutoff = 100 chemical_system \u00b6 The chemical system to be used in the analysis. This should be given as a string and in the format Element1-Element2-Element3-... . For example, Na-Cl , Y-C , and Y-C-F are valid chemical systems. yaml toml python chemical_system : Na-Cl chemical_system = \"Na-Cl\" chemical_system = \"Na-Cl\" Warning Some workflows only accept a chemical system with a specific number of elements. An example of this is the structure-prediction.python.binary-composition search which only allows two elements (e.g. Y-C-F would raise an error) command \u00b6 The command that will be called during execution of a program. There is typically a default set for this that you only need to change unless you'd like parallelization. For example, VASP workflows use vasp_std > vasp.out by default but you can override this to use mpirun . yaml toml python command : mpirun -n 8 vasp_std > vasp.out command = \"mpirun -n 8 vasp_std > vasp.out\" command = \"mpirun -n 8 vasp_std > vasp.out\" used for bulk crystal relaxation and static energy - cmd2 --> used for endpoint supercell relaxations - cmd3 --> used for NEB Thus, you can scale your resources for each step. Here's a full -c option: -c \"vasp_std > vasp.out; mpirun -n 12 vasp_std > vasp.out; mpirun -n 70 vasp_std > vasp.out\" --> composition \u00b6 The composition input can be anything compatible with the Composition toolkit class. Note that compositions are sensitive to atom counts / multiplicity. There is a difference between giving Ca2N and Ca4N2 in several workflows. Accepted inputs include: a string (recommended) yaml toml python composition : Ca2NF composition = \"Ca2NF\" composition = \"Ca2NF\" a dictionary that gives the composition yaml toml python composition : Ca : 2 N : 1 F : 1 [ composition ] Ca = 2 N = 1 F = 1 composition = { \"Ca\" : 2 , \"N\" : 1 , \"F\" : 1 , } a Composition object (best for advanced logic) python from simmate.toolkit import Compositon composition = Composition ( \"Ca2NF\" ) json/dictionary serialization from pymatgen compress_output \u00b6 Whether to compress the directory to a zip file at the end of the run. After compression, it will also delete the directory. The default is False. yaml toml python compress_output : true compress_output = true compress_output = True convergence_cutoff \u00b6 For evolutionary searches, the search will be considered converged when the best structure is not changing by this amount (in eV). In order to officially signal the end of the search, the best structure must survive within this convergence limit for a specific number of new individuals -- this is controlled by the best_survival_cutoff . The default of 1meV is typically sufficient and does not need to be changed. More often, users should update best_survival_cutoff instead. yaml toml python convergence_cutoff : 0.005 convergence_cutoff = 0.005 convergence_cutoff = 0.005 copy_previous_directory \u00b6 Whether to copy the directory from the previous calculation (if there is one) and then use it as a starting point for this new calculation. This is only possible if you provided an input that points to a previous calculation. For example, structure would need to use a database-like input: yaml toml python structure : database_table : Relaxation database_id : 123 copy_previous_directory : true copy_previous_directory : true [structure] database_table = \"Relaxation\" database_id = 123 structure = { \"database_table\" : \"Relaxation\" , \"database_id\" : 123 } copy_previous_directory = True The default is False , and it is not recommended to use this in existing workflows. Nested workflows that benefit from this feature use it automatically. diffusion_analysis_id \u00b6 (advanced users only) The entry id from the DiffusionAnalysis table to link the results to. This is set automatically by higher-level workflows and rarely (if ever) set by the user. directory \u00b6 The directory to run everything in -- either as a relative or full path. This is passed to the ulitities function simmate.ulitities.get_directory , which generates a unique foldername if not provided (such as simmate-task-12390u243 ). This will be converted into a pathlib.Path object. Accepted inputs include: leave as default (recommended) a string yaml toml python directory : my-new-folder-00 directory = \"my-new-folder-00\" directory = \"my-new-folder-00\" a pathlib.Path (best for advanced logic) python from pathlib import Path directory = Path ( \"my-new-folder-00\" ) directory_new \u00b6 Unique to the restart.simmate.automatic workflow, this is the folder that the workflow will be continued in. Follows the same rules/inputs as the directory parameter. directory_old \u00b6 Unique to the restart.simmate.automatic workflow, this is the original folder that should be used at the starting point. Follows the same rules/inputs as the directory parameter. fitness_field \u00b6 (advanced users only) For evolutionary searches, this is the value that should be optimized. Specifically, it should minimized this value (lower value = better fitness). The default is energy_per_atom , but you may want to set this to a custom column in a custom database table. input_parameters \u00b6 (experimental feature) Unique to customized.vasp.user-config . This is a list of parameters to pass to workflow_base . is_restart \u00b6 (experimental feature) Whether the calculation is a restarted workflow run. Default is False. If set to true, the workflow will go through the given directory (which must be provided) and see where to pick up. yaml toml python directory : my-old-calc-folder is_restart : true directory = \"my-old-calc-folder\" is_restart = true directory = \"my-old-calc-folder\" is_restart = True max_atoms \u00b6 For workflows that involve generating a supercell or random structure, this will be the maximum number of sites to allow in the generated structure(s). For example, an evolutionary search may set this to 10 atoms to limit the compositions & stoichiometries that are explored. yaml toml python max_atoms : 10 max_atoms = 10 max_atoms = 10 max_path_length \u00b6 For diffusion workflows, this the maximum length allowed for a single path. yaml toml python max_path_length : 3.5 max_path_length = 3.5 max_path_length = 3.5 max_stoich_factor \u00b6 The maximum stoichiometric ratio that will be analyzed. In a binary system evolutionary search, this only look at non-reduced compositions up to the max_stoich_factor. For example, this means Ca2N and max factor of 4 would only look up to Ca8N4 and skip any compositions with more atoms (e.g. Ca10N5 is skipped) yaml toml python max_stoich_factor : 5 max_stoich_factor = 5 max_stoich_factor = 5 max_structures \u00b6 For workflows that generate new structures (and potentially run calculations on them), this will be the maximum number of structures allowed. The workflow will end at this number of structures regardless of whether the calculation/search is converged or not. yaml toml python max_structures : 100 max_structures = 100 max_structures = 100 Warning In structure-prediction workflows, min_structure_exact takes priority over this setting, so it is possible for your search to exceed your maximum number of structures. If you want max_structures to have absolute control, you can set min_structure_exact to 0. max_supercell_atoms \u00b6 For workflows that involve generating a supercell, this will be the maximum number of sites to allow in the generated structure(s). For example, NEB workflows would set this value to something like 100 atoms to limit their supercell image sizes. yaml toml python max_supercell_atoms : 100 max_supercell_atoms = 100 max_supercell_atoms = 100 migrating_specie \u00b6 This is the atomic species/element that will be moving in the analysis (typically NEB or MD diffusion calculations). Note, oxidation states (e.g. \"Ca2+\") can be used, but this requires your input structure to be oxidation-state decorated as well. yaml toml python migrating_specie : Li migrating_specie = \"Li\" migrating_specie = \"Li\" migration_hop \u00b6 (advanced users only) The atomic path that should be analyzed. Inputs are anything compatible with the MigrationHop class of the simmate.toolkit.diffusion module. This includes: MigrationHop object a database entry in the MigrationHop table (TODO: if you'd like full examples, please ask our team to add them) migration_images \u00b6 The full set of images (including endpoint images) that should be analyzed. Inputs are anything compatible with the MigrationImages class of the simmate.toolkit.diffusion module, which is effectively a list of structure inputs. This includes: MigrationImages object a list of Structure objects a list of filenames (cif or POSCAR) yaml toml python migration_images : - image_01.cif - image_02.cif - image_03.cif - image_04.cif - image_05.cif migration_images = [ \"image_01.cif\" , \"image_02.cif\" , \"image_03.cif\" , \"image_04.cif\" , \"image_05.cif\" , ] migration_images = [ \"image_01.cif\" , \"image_02.cif\" , \"image_03.cif\" , \"image_04.cif\" , \"image_05.cif\" , ] min_atoms \u00b6 This is the opposite of max_atoms as this will be the minimum number of sites allowed in the generate structure(s). See max_atoms for details. min_structures_exact \u00b6 (experimental) The minimum number of structures that must be calculated with exactly matching nsites as specified in the fixed-composition. min_supercell_atoms \u00b6 This is the opposite of max_supercell_atoms as this will be the minimum number of sites allowed in the generated supercell structure. min_supercell_vector_lengths \u00b6 When generating a supercell, this is the minimum length for each lattice vector of the generated cell (in Angstroms). For workflows such as NEB, larger is better but more computationally expensive. yaml toml python min_supercell_vector_lengths : 7.5 min_supercell_vector_lengths = 7.5 min_supercell_vector_lengths = 7.5 nfirst_generation \u00b6 For evolutionary searches, no mutations or \"child\" individuals will be scheduled until this number of individuals have been calculated. This ensures we have a good pool of candidates calculated before we start selecting parents and mutating them. yaml toml python nfirst_generation : 15 nfirst_generation = 15 nfirst_generation = 15 nimages \u00b6 The number of images (or structures) to use in the analysis. This does NOT include the endpoint images (start/end structures). More is better, but computationally expensive. We recommend keeping this value odd in order to ensure there is an image at the midpoint. yaml toml python nimages : 5 nimages = 5 nimages = 5 Danger For calculators such as VASP, your command parameter must use a number of cores that is divisible by nimages . For example, nimages=3 and command=\"mpirun -n 10 vasp_std > vasp.out\" will fail because 10 is not divisible by 3. nsteadystate \u00b6 The number of individual workflows to have scheduled at once. This therefore sets the queue size of an evolutionary search. Note, the number of workflows ran in parallel is determined by the number of Workers started (i.e. starting 3 workers will run 3 workflows in parallel, even if 100 workflows are in the queue). The steady-state does, however, set the maximum number of parallel runs because the queue size will never exceed the nsteadystate value. This parameter is closely tied with steadystate_sources , so be sure to read about that parameter as well. yaml toml python nsteadystate : 50 nsteadystate = 50 nsteadystate = 50 nsteps \u00b6 The total number of steps to run the calculation on. For example, in molecular dynamics workflows, this will stop the simulation after this many steps. yaml toml python nsteps : 10000 nsteps = 10000 nsteps = 10000 percolation_mode \u00b6 The percolating type to detect. The default is \">1d\", which search for percolating paths up to the max_path_length . Alternatively, this can be set to \"1d\" in order to stop unique pathway finding when 1D percolation is achieved. yaml toml python percolation_mode : 1d percolation_mode = \"1d\" percolation_mode = \"1d\" relax_bulk \u00b6 Whether or not the bulk structure (typically the input structure) should be relaxed before running the rest of the workflow. yaml toml python relax_bulk : false relax_bulk : false relax_bulk : false relax_endpoints \u00b6 Whether or not the endpoint structures for an NEB diffusion pathway should be relaxed before running the rest of the workflow. yaml toml python relax_endpoints : false relax_endpoints : false relax_endpoints : false run_id \u00b6 The id assigned to a specific workflow run / calculation. If not provided this will be randomly generated, and we highly recommended leaving this at the default value. Note, this is based on unique-ids (UUID), so every id should be 100% unique and in a string format. yaml toml python run_id : my-unique-id-123 run_id = \"my-unique-id-123\" run_id = \"my-unique-id-123\" search_id \u00b6 (advanced users only) The evolutionary search that this individual is associated with. This allows us to determine which Selector , Validator , and StopCondition should be used when creating and evaluating the individual. When running a search, this is set automatically when submitting a new flow. selector_kwargs \u00b6 (advanced users only) Extra conditions to use when initializing the selector class. MySelector(**selector_kwargs) . The input should be given as a dictionary. Note, for evolutionary searches, the composition kwarg is added automatically. This is closely tied with the selector_name parameter so be sure to read that section as well. selector_name \u00b6 (experimental feature; advanced users only) The base selector class that should be used. The class will be initialized using MySelector(**selector_kwargs) . The input should be given as a string. Warning Currently, we only support truncated selection, so this should be left at its default value. singleshot_sources \u00b6 (experimental feature; advanced users only) A list of structure sources that run once and never again. This includes generating input structures from known structures (from third-party databases), prototypes, or substituiting known structures. In the current version of simmate, these features are not enabled and this input should be ignored. sleep_step \u00b6 When there is a cycle within a workflow (such as iteratively checking the number of subworkflows submitted and updating results), this is the amount of time in seconds that the workflow will shutdown before restarting the cycle. For evolutionary searches, setting this to a larger value will save on computation resources and database load, so we recommend increasing it where possible. yaml toml python run_id : 180 run_id = 180 sleep_step = 180 # 3 minutes source \u00b6 (experimental feature; advanced users only) This column indicates where the input data (and other parameters) came from. The source could be a number of things including... - a third party id - a structure from a different Simmate datbase table - a transformation of another structure - a creation method - a custom submission by the user By default, this is a dictionary to account for the many different scenarios. Here are some examples of values used in this column: python # from a thirdparty database or separate table source = { \"database_table\" : \"MatprojStructure\" , \"database_id\" : \"mp-123\" , } # from a random structure creator source = { \"creator\" : \"PyXtalStructure\" } # from a templated structure creator (e.g. substituition or prototypes) source = { \"creator\" : \"PrototypeStructure\" , \"database_table\" : \"AFLOWPrototypes\" , \"id\" : 123 , } # from a transformation source = { \"transformation\" : \"MirrorMutation\" , \"database_table\" : \"MatprojStructure\" , \"parent_ids\" : [ \"mp-12\" , \"mp-34\" ], } Typically, the source is set automatically, and users do not need to update it. standardize_structure \u00b6 In some cases, we may want to standardize the structure during our setup(). This means running symmetry analysis on the structure in order to reduce the symmetry and also convert it to some standardized form. There are three different forms to choose from and thus 3 different values that standardize_structure can be set to: primitive : for the standard primitive unitcell conventional : for the standard conventional unitcell primitive-LLL : for the standard primitive unitcell that is then LLL-reduced False : this is the default and will disable this feature We recommend using primitive-LLL when the smallest possible and most cubic unitcell is desired. We recommend using primitive when calculating band structures and ensuring we have a standardized high-symmetry path. Note,Existing band-structure workflows use this automatically. To control the tolerances used to symmetrize the structure, you can use the symmetry_precision and angle_tolerance attributes. By default, no standardization is applied. yaml toml python standardize_structure : primitive-LLL standardize_structure = \"primitive-LLL\" standardize_structure = \"primitive-LLL\" steadystate_source_id \u00b6 (advanced users only) The structure source that this individual is associated with. This allows us to determine how the new individual should be created. When running a search, this is set automatically when submitting a new flow. steadystate_sources \u00b6 (experimental feature; advanced users only) The sources that will be scheduled at a \"steady-state\", meaning there will always be a set number of individuals scheduled/running for this type of structure source. This should be defined as a dictionary where each is {\"source_name\": percent} . The percent determines the number of steady stage calculations that will be running for this at any given time. It will be a percent of the nsteadystate parameter, which sets the total number of individuals to be scheduled/running. For example, if nsteadystate=40 and we add a source of {\"RandomSymStructure\": 0.30, ...} , this means 0.25*40=10 randomly-created individuals will be running/submitted at all times. The source can be from either the toolkit.creator or toolkit.transformations modules. yaml yaml python singleshot_sources : RandomSymStructure : 0.30 from_ase.Heredity : 0.30 from_ase.SoftMutation : 0.10 from_ase.MirrorMutation : 0.10 from_ase.LatticeStrain : 0.05 from_ase.RotationalMutation : 0.05 from_ase.AtomicPermutation : 0.05 from_ase.CoordinatePerturbation : 0.05 [ singleshot_sources ] \"RandomSymStructure\" : 0.30 \"from_ase.Heredity\" : 0.30 \"from_ase.SoftMutation\" : 0.10 \"from_ase.MirrorMutation\" : 0.10 \"from_ase.LatticeStrain\" : 0.05 \"from_ase.RotationalMutation\" : 0.05 \"from_ase.AtomicPermutation\" : 0.05 \"from_ase.CoordinatePerturbation\" : 0.05 singleshot_sources = { \"RandomSymStructure\" : 0.30 , \"from_ase.Heredity\" : 0.30 , \"from_ase.SoftMutation\" : 0.10 , \"from_ase.MirrorMutation\" : 0.10 , \"from_ase.LatticeStrain\" : 0.05 , \"from_ase.RotationalMutation\" : 0.05 , \"from_ase.AtomicPermutation\" : 0.05 , \"from_ase.CoordinatePerturbation\" : 0.05 , } Note: if your percent values do not sum to 1, they will be rescaled. When calculating percent*nsteadystate , the value will be rounded to the nearest integer. We are moving towards accepting kwargs or class objects as well, but this is not yet allowed. For example, anything other than percent would be treated as a kwarg: toml [singleshot_sources.RandomSymStructure] percent : 0.30 spacegroups_exclude : [ 1 , 2 , 3 ] site_generation_method : \"MyCustomMethod\" structure \u00b6 The crystal structure to be used for the analysis. The input can be anything compatible with the Structure toolkit class. Accepted inputs include: a filename (cif or poscar) (recommended) yaml toml python structure : NaCl.cif structure = NaCl . cif structure = \"NaCl.cif\" a dictionary that points to a database entry. yaml toml python # example 1 structure : database_table : MatprojStructure database_id : mp-123 # example 2 structure : database_table : StaticEnergy database_id : 50 # example 3 structure : database_table : Relaxation database_id : 50 structure_field : structure_final # example 1 [structure] database_table : MatprojStructure database_id : mp-123 # example 2 [structure] database_table : StaticEnergy database_id : 50 # example 3 [structure] database_table : Relaxation database_id : 50 structure_field : structure_final # example 1 structure = { \"database_table\" : \"MatprojStructure\" , \"database_id\" : \"mp-123\" , } # example 2 structure = { \"database_table\" : \"StaticEnergy\" , \"database_id\" : 50 , } # example 3 structure = { \"database_table\" : \"Relaxation\" , \"database_id\" : 50 , \"structure_field\" : \"structure_final\" , } Note instead of database_id , you can also use the run_id or directory to indicate which entry to load. Further, if the database table is linked to multiple structures (e.g. relaxations have a structure_start and structure_final ), then you can also add the structure_field to specify which to choose. a Structure object (best for advanced logic) python from simmate.toolkit import Structure structure = Structure ( lattice = [ [ 2.846 , 2.846 , 0.000 ], [ 2.846 , 0.000 , 2.846 ], [ 0.000 , 2.846 , 2.846 ], ], species = [ \"Na\" , \"Cl\" ], coords = [ [ 0.5 , 0.5 , 0.5 ], [ 0.0 , 0.0 , 0.0 ], ], coords_are_cartesian = False , ) a Structure database object python structure = ExampleTable . objects . get ( id = 123 ) json/dictionary serialization from pymatgen subworkflow_kwargs \u00b6 Make sure you read about subworkflow_name parameter first. This is a dictionary of parameters to pass to each subworkflow run. For example, the workflow will be ran as subworkflow.run(**subworkflow_kwargs) . Note, many workflows that use this argument will automatically pass information that is unique to each call (such as structure ). yaml toml python subworkflow_kwargs : command : mpirun -n 4 vasp_std > vasp.out compress_output : true [subworkflow_kwargs] command = \"mpirun -n 4 vasp_std > vasp.out\" compress_output = true subworkflow_kwargs = dict ( command = \"mpirun -n 4 vasp_std > vasp.out\" , compress_output = True , ) subworkflow_name \u00b6 The name of workflow that used to evaluate structures generated. For example, in evolutionary searches, individuals are created and then relaxed using the relaxation.vasp.staged workflow. Any workflow that is registered and accessible via the get_workflow utility can be used instead. (note: in the future we will allow unregisterd flows as well). If you wish to submit extra arguments to each workflow run, you can use the subworkflow_kwargs parameter. yaml toml python subworkflow_name : relaxation.vasp.staged subworkflow_name = \"relaxation.vasp.staged\" subworkflow_name = \"relaxation.vasp.staged\" supercell_end \u00b6 The endpoint image supercell to use. This is really just a structure parameter under a different name, so everything about the structure parameter also applies here. supercell_start \u00b6 The starting image supercell to use. This is really just a structure parameter under a different name, so everything about the structure parameter also applies here. symmetry_precision \u00b6 If standardize_structure=True, then this is the cutoff value used to determine if the sites are symmetrically equivalent. (in Angstroms) yaml python symmetry_precision : 0.1 symmetry_precision = 0.1 tags \u00b6 When submitting workflows via the run_cloud command, tags are 'labels' that help control which workers are allowed to pick up and run the submitted workflow. Workers should be started with matching tags in order for these scheduled flows to run. yaml toml python tags : - my-tag-01 - my-tag-02 tags = [ \"my-tag-01\" , \"my-tag-02\" ] tags = [ \"my-tag-01\" , \"my-tag-02\" ] Warning When you have a workflow that is submitting many smaller workflows (such as structure-prediction workflows), make sure you set the tags in the subworkflow_kwargs settings: subworkflow_kwargs : tags : - my-tag-01 - my-tag-02 temperature_end \u00b6 For molecular dynamics simulations, this is the temperature to end the simulation at (in Kelvin). This temperatue will be reached through a linear transition from the temperature_start parameter. yaml toml python temperature_end : 1000 temperature_end = 1000 temperature_end = 1000 temperature_start \u00b6 For molecular dynamics simulations, this is the temperature to begin the simulation at (in Kelvin). yaml toml python temperature_start : 250 temperature_start = 250 temperature_start = 250 time_step \u00b6 For molecular dynamics simulations, this is time time between each ionic step (in femtoseconds). yaml toml python time_step : 1.5 time_step = 1.5 time_step = 1.5 updated_settings \u00b6 (experimental feature) Unique to customized.vasp.user-config . This is a list of parameters to update the workflow_base with. This often involves updating the base class attributes. vacancy_mode \u00b6 For NEB and diffusion workfows, this determines whether vacancy or interstitial diffusion is analyzed. Default of True corresponds to vacancy-based diffusion. yaml toml python vacancy_mode : false vacancy_mode = false vacancy_mode = False validator_kwargs \u00b6 (advanced users only) Extra conditions to use when initializing the validator class. MyValidator(**validator_kwargs) . The input should be given as a dictionary. Note, for evolutionary searches, the composition kwarg is added automatically. This is closely tied with the validator_name parameter so be sure to read that section as well. validator_name \u00b6 (experimental feature; advanced users only) The base validator class that should be used. The class will be initialized using MyValidator(**validator_kwargs) . The input should be given as a string. Warning Currently, we only support CrystallNNFingerprint validation, so this should be left at its default value. workflow_base \u00b6 (experimental feature) Unique to customized.vasp.user-config . This is the base workflow to use when updating critical settings. write_summary_files \u00b6 Whether or not to write output files. For some workflows, writing output files can cause excessive load on the database and possibly make the calculation IO bound. In cases such as this, you can set this to False . yaml toml python write_summary_files : false write_summary_files = false write_summary_files = False Tip Beginners can often ignore this setting. This is typically only relevant in a setup where you have many computational resources and have many evolutionary searches (>10) running at the same time.","title":"Parameters"},{"location":"parameters/#parameters","text":"","title":"Parameters"},{"location":"parameters/#overview","text":"Knowing which parameters are available and how to use them is essential. We therefore outline all unique parameters for all workflows here. To see which parameters are allowed for a given workflow, you can use the explore command or workflow.show_parameters() : command line python simmate workflows explore workflow . show_parameters () If you need more details on a parameter or would like to request a new one, just let us know! You can then search for these parameters below to learn more about them.","title":"Overview"},{"location":"parameters/#angle_tolerance","text":"If standardize_structure=True, then this is the cutoff value used to determine if the angles between sites are symmetrically equivalent. (in Degrees) yaml toml python angle_tolerance : 10.0 angle_tolerance = 10.0 angle_tolerance = 10.0","title":"angle_tolerance"},{"location":"parameters/#best_survival_cutoff","text":"For evolutionary searches, fixed compositions will be stopped when the best individual remains unbeaten for this number of new individuals. In order to absorb similar structures (e.g. identical structures but with minor energy differences), structures within the convergence_cutoff parameter (e.g. +1meV) are not considered when counting historical structures. This helps to prevent the search from continuing in cases where the search is likely already converged but making <0.1meV improvements. The default is typically set based on the number of atoms in the composition. yaml toml python best_survival_cutoff : 100 best_survival_cutoff = 100 best_survival_cutoff = 100","title":"best_survival_cutoff"},{"location":"parameters/#chemical_system","text":"The chemical system to be used in the analysis. This should be given as a string and in the format Element1-Element2-Element3-... . For example, Na-Cl , Y-C , and Y-C-F are valid chemical systems. yaml toml python chemical_system : Na-Cl chemical_system = \"Na-Cl\" chemical_system = \"Na-Cl\" Warning Some workflows only accept a chemical system with a specific number of elements. An example of this is the structure-prediction.python.binary-composition search which only allows two elements (e.g. Y-C-F would raise an error)","title":"chemical_system"},{"location":"parameters/#command","text":"The command that will be called during execution of a program. There is typically a default set for this that you only need to change unless you'd like parallelization. For example, VASP workflows use vasp_std > vasp.out by default but you can override this to use mpirun . yaml toml python command : mpirun -n 8 vasp_std > vasp.out command = \"mpirun -n 8 vasp_std > vasp.out\" command = \"mpirun -n 8 vasp_std > vasp.out\" used for bulk crystal relaxation and static energy - cmd2 --> used for endpoint supercell relaxations - cmd3 --> used for NEB Thus, you can scale your resources for each step. Here's a full -c option: -c \"vasp_std > vasp.out; mpirun -n 12 vasp_std > vasp.out; mpirun -n 70 vasp_std > vasp.out\" -->","title":"command"},{"location":"parameters/#composition","text":"The composition input can be anything compatible with the Composition toolkit class. Note that compositions are sensitive to atom counts / multiplicity. There is a difference between giving Ca2N and Ca4N2 in several workflows. Accepted inputs include: a string (recommended) yaml toml python composition : Ca2NF composition = \"Ca2NF\" composition = \"Ca2NF\" a dictionary that gives the composition yaml toml python composition : Ca : 2 N : 1 F : 1 [ composition ] Ca = 2 N = 1 F = 1 composition = { \"Ca\" : 2 , \"N\" : 1 , \"F\" : 1 , } a Composition object (best for advanced logic) python from simmate.toolkit import Compositon composition = Composition ( \"Ca2NF\" ) json/dictionary serialization from pymatgen","title":"composition"},{"location":"parameters/#compress_output","text":"Whether to compress the directory to a zip file at the end of the run. After compression, it will also delete the directory. The default is False. yaml toml python compress_output : true compress_output = true compress_output = True","title":"compress_output"},{"location":"parameters/#convergence_cutoff","text":"For evolutionary searches, the search will be considered converged when the best structure is not changing by this amount (in eV). In order to officially signal the end of the search, the best structure must survive within this convergence limit for a specific number of new individuals -- this is controlled by the best_survival_cutoff . The default of 1meV is typically sufficient and does not need to be changed. More often, users should update best_survival_cutoff instead. yaml toml python convergence_cutoff : 0.005 convergence_cutoff = 0.005 convergence_cutoff = 0.005","title":"convergence_cutoff"},{"location":"parameters/#copy_previous_directory","text":"Whether to copy the directory from the previous calculation (if there is one) and then use it as a starting point for this new calculation. This is only possible if you provided an input that points to a previous calculation. For example, structure would need to use a database-like input: yaml toml python structure : database_table : Relaxation database_id : 123 copy_previous_directory : true copy_previous_directory : true [structure] database_table = \"Relaxation\" database_id = 123 structure = { \"database_table\" : \"Relaxation\" , \"database_id\" : 123 } copy_previous_directory = True The default is False , and it is not recommended to use this in existing workflows. Nested workflows that benefit from this feature use it automatically.","title":"copy_previous_directory"},{"location":"parameters/#diffusion_analysis_id","text":"(advanced users only) The entry id from the DiffusionAnalysis table to link the results to. This is set automatically by higher-level workflows and rarely (if ever) set by the user.","title":"diffusion_analysis_id"},{"location":"parameters/#directory","text":"The directory to run everything in -- either as a relative or full path. This is passed to the ulitities function simmate.ulitities.get_directory , which generates a unique foldername if not provided (such as simmate-task-12390u243 ). This will be converted into a pathlib.Path object. Accepted inputs include: leave as default (recommended) a string yaml toml python directory : my-new-folder-00 directory = \"my-new-folder-00\" directory = \"my-new-folder-00\" a pathlib.Path (best for advanced logic) python from pathlib import Path directory = Path ( \"my-new-folder-00\" )","title":"directory"},{"location":"parameters/#directory_new","text":"Unique to the restart.simmate.automatic workflow, this is the folder that the workflow will be continued in. Follows the same rules/inputs as the directory parameter.","title":"directory_new"},{"location":"parameters/#directory_old","text":"Unique to the restart.simmate.automatic workflow, this is the original folder that should be used at the starting point. Follows the same rules/inputs as the directory parameter.","title":"directory_old"},{"location":"parameters/#fitness_field","text":"(advanced users only) For evolutionary searches, this is the value that should be optimized. Specifically, it should minimized this value (lower value = better fitness). The default is energy_per_atom , but you may want to set this to a custom column in a custom database table.","title":"fitness_field"},{"location":"parameters/#input_parameters","text":"(experimental feature) Unique to customized.vasp.user-config . This is a list of parameters to pass to workflow_base .","title":"input_parameters"},{"location":"parameters/#is_restart","text":"(experimental feature) Whether the calculation is a restarted workflow run. Default is False. If set to true, the workflow will go through the given directory (which must be provided) and see where to pick up. yaml toml python directory : my-old-calc-folder is_restart : true directory = \"my-old-calc-folder\" is_restart = true directory = \"my-old-calc-folder\" is_restart = True","title":"is_restart"},{"location":"parameters/#max_atoms","text":"For workflows that involve generating a supercell or random structure, this will be the maximum number of sites to allow in the generated structure(s). For example, an evolutionary search may set this to 10 atoms to limit the compositions & stoichiometries that are explored. yaml toml python max_atoms : 10 max_atoms = 10 max_atoms = 10","title":"max_atoms"},{"location":"parameters/#max_path_length","text":"For diffusion workflows, this the maximum length allowed for a single path. yaml toml python max_path_length : 3.5 max_path_length = 3.5 max_path_length = 3.5","title":"max_path_length"},{"location":"parameters/#max_stoich_factor","text":"The maximum stoichiometric ratio that will be analyzed. In a binary system evolutionary search, this only look at non-reduced compositions up to the max_stoich_factor. For example, this means Ca2N and max factor of 4 would only look up to Ca8N4 and skip any compositions with more atoms (e.g. Ca10N5 is skipped) yaml toml python max_stoich_factor : 5 max_stoich_factor = 5 max_stoich_factor = 5","title":"max_stoich_factor"},{"location":"parameters/#max_structures","text":"For workflows that generate new structures (and potentially run calculations on them), this will be the maximum number of structures allowed. The workflow will end at this number of structures regardless of whether the calculation/search is converged or not. yaml toml python max_structures : 100 max_structures = 100 max_structures = 100 Warning In structure-prediction workflows, min_structure_exact takes priority over this setting, so it is possible for your search to exceed your maximum number of structures. If you want max_structures to have absolute control, you can set min_structure_exact to 0.","title":"max_structures"},{"location":"parameters/#max_supercell_atoms","text":"For workflows that involve generating a supercell, this will be the maximum number of sites to allow in the generated structure(s). For example, NEB workflows would set this value to something like 100 atoms to limit their supercell image sizes. yaml toml python max_supercell_atoms : 100 max_supercell_atoms = 100 max_supercell_atoms = 100","title":"max_supercell_atoms"},{"location":"parameters/#migrating_specie","text":"This is the atomic species/element that will be moving in the analysis (typically NEB or MD diffusion calculations). Note, oxidation states (e.g. \"Ca2+\") can be used, but this requires your input structure to be oxidation-state decorated as well. yaml toml python migrating_specie : Li migrating_specie = \"Li\" migrating_specie = \"Li\"","title":"migrating_specie"},{"location":"parameters/#migration_hop","text":"(advanced users only) The atomic path that should be analyzed. Inputs are anything compatible with the MigrationHop class of the simmate.toolkit.diffusion module. This includes: MigrationHop object a database entry in the MigrationHop table (TODO: if you'd like full examples, please ask our team to add them)","title":"migration_hop"},{"location":"parameters/#migration_images","text":"The full set of images (including endpoint images) that should be analyzed. Inputs are anything compatible with the MigrationImages class of the simmate.toolkit.diffusion module, which is effectively a list of structure inputs. This includes: MigrationImages object a list of Structure objects a list of filenames (cif or POSCAR) yaml toml python migration_images : - image_01.cif - image_02.cif - image_03.cif - image_04.cif - image_05.cif migration_images = [ \"image_01.cif\" , \"image_02.cif\" , \"image_03.cif\" , \"image_04.cif\" , \"image_05.cif\" , ] migration_images = [ \"image_01.cif\" , \"image_02.cif\" , \"image_03.cif\" , \"image_04.cif\" , \"image_05.cif\" , ]","title":"migration_images"},{"location":"parameters/#min_atoms","text":"This is the opposite of max_atoms as this will be the minimum number of sites allowed in the generate structure(s). See max_atoms for details.","title":"min_atoms"},{"location":"parameters/#min_structures_exact","text":"(experimental) The minimum number of structures that must be calculated with exactly matching nsites as specified in the fixed-composition.","title":"min_structures_exact"},{"location":"parameters/#min_supercell_atoms","text":"This is the opposite of max_supercell_atoms as this will be the minimum number of sites allowed in the generated supercell structure.","title":"min_supercell_atoms"},{"location":"parameters/#min_supercell_vector_lengths","text":"When generating a supercell, this is the minimum length for each lattice vector of the generated cell (in Angstroms). For workflows such as NEB, larger is better but more computationally expensive. yaml toml python min_supercell_vector_lengths : 7.5 min_supercell_vector_lengths = 7.5 min_supercell_vector_lengths = 7.5","title":"min_supercell_vector_lengths"},{"location":"parameters/#nfirst_generation","text":"For evolutionary searches, no mutations or \"child\" individuals will be scheduled until this number of individuals have been calculated. This ensures we have a good pool of candidates calculated before we start selecting parents and mutating them. yaml toml python nfirst_generation : 15 nfirst_generation = 15 nfirst_generation = 15","title":"nfirst_generation"},{"location":"parameters/#nimages","text":"The number of images (or structures) to use in the analysis. This does NOT include the endpoint images (start/end structures). More is better, but computationally expensive. We recommend keeping this value odd in order to ensure there is an image at the midpoint. yaml toml python nimages : 5 nimages = 5 nimages = 5 Danger For calculators such as VASP, your command parameter must use a number of cores that is divisible by nimages . For example, nimages=3 and command=\"mpirun -n 10 vasp_std > vasp.out\" will fail because 10 is not divisible by 3.","title":"nimages"},{"location":"parameters/#nsteadystate","text":"The number of individual workflows to have scheduled at once. This therefore sets the queue size of an evolutionary search. Note, the number of workflows ran in parallel is determined by the number of Workers started (i.e. starting 3 workers will run 3 workflows in parallel, even if 100 workflows are in the queue). The steady-state does, however, set the maximum number of parallel runs because the queue size will never exceed the nsteadystate value. This parameter is closely tied with steadystate_sources , so be sure to read about that parameter as well. yaml toml python nsteadystate : 50 nsteadystate = 50 nsteadystate = 50","title":"nsteadystate"},{"location":"parameters/#nsteps","text":"The total number of steps to run the calculation on. For example, in molecular dynamics workflows, this will stop the simulation after this many steps. yaml toml python nsteps : 10000 nsteps = 10000 nsteps = 10000","title":"nsteps"},{"location":"parameters/#percolation_mode","text":"The percolating type to detect. The default is \">1d\", which search for percolating paths up to the max_path_length . Alternatively, this can be set to \"1d\" in order to stop unique pathway finding when 1D percolation is achieved. yaml toml python percolation_mode : 1d percolation_mode = \"1d\" percolation_mode = \"1d\"","title":"percolation_mode"},{"location":"parameters/#relax_bulk","text":"Whether or not the bulk structure (typically the input structure) should be relaxed before running the rest of the workflow. yaml toml python relax_bulk : false relax_bulk : false relax_bulk : false","title":"relax_bulk"},{"location":"parameters/#relax_endpoints","text":"Whether or not the endpoint structures for an NEB diffusion pathway should be relaxed before running the rest of the workflow. yaml toml python relax_endpoints : false relax_endpoints : false relax_endpoints : false","title":"relax_endpoints"},{"location":"parameters/#run_id","text":"The id assigned to a specific workflow run / calculation. If not provided this will be randomly generated, and we highly recommended leaving this at the default value. Note, this is based on unique-ids (UUID), so every id should be 100% unique and in a string format. yaml toml python run_id : my-unique-id-123 run_id = \"my-unique-id-123\" run_id = \"my-unique-id-123\"","title":"run_id"},{"location":"parameters/#search_id","text":"(advanced users only) The evolutionary search that this individual is associated with. This allows us to determine which Selector , Validator , and StopCondition should be used when creating and evaluating the individual. When running a search, this is set automatically when submitting a new flow.","title":"search_id"},{"location":"parameters/#selector_kwargs","text":"(advanced users only) Extra conditions to use when initializing the selector class. MySelector(**selector_kwargs) . The input should be given as a dictionary. Note, for evolutionary searches, the composition kwarg is added automatically. This is closely tied with the selector_name parameter so be sure to read that section as well.","title":"selector_kwargs"},{"location":"parameters/#selector_name","text":"(experimental feature; advanced users only) The base selector class that should be used. The class will be initialized using MySelector(**selector_kwargs) . The input should be given as a string. Warning Currently, we only support truncated selection, so this should be left at its default value.","title":"selector_name"},{"location":"parameters/#singleshot_sources","text":"(experimental feature; advanced users only) A list of structure sources that run once and never again. This includes generating input structures from known structures (from third-party databases), prototypes, or substituiting known structures. In the current version of simmate, these features are not enabled and this input should be ignored.","title":"singleshot_sources"},{"location":"parameters/#sleep_step","text":"When there is a cycle within a workflow (such as iteratively checking the number of subworkflows submitted and updating results), this is the amount of time in seconds that the workflow will shutdown before restarting the cycle. For evolutionary searches, setting this to a larger value will save on computation resources and database load, so we recommend increasing it where possible. yaml toml python run_id : 180 run_id = 180 sleep_step = 180 # 3 minutes","title":"sleep_step"},{"location":"parameters/#source","text":"(experimental feature; advanced users only) This column indicates where the input data (and other parameters) came from. The source could be a number of things including... - a third party id - a structure from a different Simmate datbase table - a transformation of another structure - a creation method - a custom submission by the user By default, this is a dictionary to account for the many different scenarios. Here are some examples of values used in this column: python # from a thirdparty database or separate table source = { \"database_table\" : \"MatprojStructure\" , \"database_id\" : \"mp-123\" , } # from a random structure creator source = { \"creator\" : \"PyXtalStructure\" } # from a templated structure creator (e.g. substituition or prototypes) source = { \"creator\" : \"PrototypeStructure\" , \"database_table\" : \"AFLOWPrototypes\" , \"id\" : 123 , } # from a transformation source = { \"transformation\" : \"MirrorMutation\" , \"database_table\" : \"MatprojStructure\" , \"parent_ids\" : [ \"mp-12\" , \"mp-34\" ], } Typically, the source is set automatically, and users do not need to update it.","title":"source"},{"location":"parameters/#standardize_structure","text":"In some cases, we may want to standardize the structure during our setup(). This means running symmetry analysis on the structure in order to reduce the symmetry and also convert it to some standardized form. There are three different forms to choose from and thus 3 different values that standardize_structure can be set to: primitive : for the standard primitive unitcell conventional : for the standard conventional unitcell primitive-LLL : for the standard primitive unitcell that is then LLL-reduced False : this is the default and will disable this feature We recommend using primitive-LLL when the smallest possible and most cubic unitcell is desired. We recommend using primitive when calculating band structures and ensuring we have a standardized high-symmetry path. Note,Existing band-structure workflows use this automatically. To control the tolerances used to symmetrize the structure, you can use the symmetry_precision and angle_tolerance attributes. By default, no standardization is applied. yaml toml python standardize_structure : primitive-LLL standardize_structure = \"primitive-LLL\" standardize_structure = \"primitive-LLL\"","title":"standardize_structure"},{"location":"parameters/#steadystate_source_id","text":"(advanced users only) The structure source that this individual is associated with. This allows us to determine how the new individual should be created. When running a search, this is set automatically when submitting a new flow.","title":"steadystate_source_id"},{"location":"parameters/#steadystate_sources","text":"(experimental feature; advanced users only) The sources that will be scheduled at a \"steady-state\", meaning there will always be a set number of individuals scheduled/running for this type of structure source. This should be defined as a dictionary where each is {\"source_name\": percent} . The percent determines the number of steady stage calculations that will be running for this at any given time. It will be a percent of the nsteadystate parameter, which sets the total number of individuals to be scheduled/running. For example, if nsteadystate=40 and we add a source of {\"RandomSymStructure\": 0.30, ...} , this means 0.25*40=10 randomly-created individuals will be running/submitted at all times. The source can be from either the toolkit.creator or toolkit.transformations modules. yaml yaml python singleshot_sources : RandomSymStructure : 0.30 from_ase.Heredity : 0.30 from_ase.SoftMutation : 0.10 from_ase.MirrorMutation : 0.10 from_ase.LatticeStrain : 0.05 from_ase.RotationalMutation : 0.05 from_ase.AtomicPermutation : 0.05 from_ase.CoordinatePerturbation : 0.05 [ singleshot_sources ] \"RandomSymStructure\" : 0.30 \"from_ase.Heredity\" : 0.30 \"from_ase.SoftMutation\" : 0.10 \"from_ase.MirrorMutation\" : 0.10 \"from_ase.LatticeStrain\" : 0.05 \"from_ase.RotationalMutation\" : 0.05 \"from_ase.AtomicPermutation\" : 0.05 \"from_ase.CoordinatePerturbation\" : 0.05 singleshot_sources = { \"RandomSymStructure\" : 0.30 , \"from_ase.Heredity\" : 0.30 , \"from_ase.SoftMutation\" : 0.10 , \"from_ase.MirrorMutation\" : 0.10 , \"from_ase.LatticeStrain\" : 0.05 , \"from_ase.RotationalMutation\" : 0.05 , \"from_ase.AtomicPermutation\" : 0.05 , \"from_ase.CoordinatePerturbation\" : 0.05 , } Note: if your percent values do not sum to 1, they will be rescaled. When calculating percent*nsteadystate , the value will be rounded to the nearest integer. We are moving towards accepting kwargs or class objects as well, but this is not yet allowed. For example, anything other than percent would be treated as a kwarg: toml [singleshot_sources.RandomSymStructure] percent : 0.30 spacegroups_exclude : [ 1 , 2 , 3 ] site_generation_method : \"MyCustomMethod\"","title":"steadystate_sources"},{"location":"parameters/#structure","text":"The crystal structure to be used for the analysis. The input can be anything compatible with the Structure toolkit class. Accepted inputs include: a filename (cif or poscar) (recommended) yaml toml python structure : NaCl.cif structure = NaCl . cif structure = \"NaCl.cif\" a dictionary that points to a database entry. yaml toml python # example 1 structure : database_table : MatprojStructure database_id : mp-123 # example 2 structure : database_table : StaticEnergy database_id : 50 # example 3 structure : database_table : Relaxation database_id : 50 structure_field : structure_final # example 1 [structure] database_table : MatprojStructure database_id : mp-123 # example 2 [structure] database_table : StaticEnergy database_id : 50 # example 3 [structure] database_table : Relaxation database_id : 50 structure_field : structure_final # example 1 structure = { \"database_table\" : \"MatprojStructure\" , \"database_id\" : \"mp-123\" , } # example 2 structure = { \"database_table\" : \"StaticEnergy\" , \"database_id\" : 50 , } # example 3 structure = { \"database_table\" : \"Relaxation\" , \"database_id\" : 50 , \"structure_field\" : \"structure_final\" , } Note instead of database_id , you can also use the run_id or directory to indicate which entry to load. Further, if the database table is linked to multiple structures (e.g. relaxations have a structure_start and structure_final ), then you can also add the structure_field to specify which to choose. a Structure object (best for advanced logic) python from simmate.toolkit import Structure structure = Structure ( lattice = [ [ 2.846 , 2.846 , 0.000 ], [ 2.846 , 0.000 , 2.846 ], [ 0.000 , 2.846 , 2.846 ], ], species = [ \"Na\" , \"Cl\" ], coords = [ [ 0.5 , 0.5 , 0.5 ], [ 0.0 , 0.0 , 0.0 ], ], coords_are_cartesian = False , ) a Structure database object python structure = ExampleTable . objects . get ( id = 123 ) json/dictionary serialization from pymatgen","title":"structure"},{"location":"parameters/#subworkflow_kwargs","text":"Make sure you read about subworkflow_name parameter first. This is a dictionary of parameters to pass to each subworkflow run. For example, the workflow will be ran as subworkflow.run(**subworkflow_kwargs) . Note, many workflows that use this argument will automatically pass information that is unique to each call (such as structure ). yaml toml python subworkflow_kwargs : command : mpirun -n 4 vasp_std > vasp.out compress_output : true [subworkflow_kwargs] command = \"mpirun -n 4 vasp_std > vasp.out\" compress_output = true subworkflow_kwargs = dict ( command = \"mpirun -n 4 vasp_std > vasp.out\" , compress_output = True , )","title":"subworkflow_kwargs"},{"location":"parameters/#subworkflow_name","text":"The name of workflow that used to evaluate structures generated. For example, in evolutionary searches, individuals are created and then relaxed using the relaxation.vasp.staged workflow. Any workflow that is registered and accessible via the get_workflow utility can be used instead. (note: in the future we will allow unregisterd flows as well). If you wish to submit extra arguments to each workflow run, you can use the subworkflow_kwargs parameter. yaml toml python subworkflow_name : relaxation.vasp.staged subworkflow_name = \"relaxation.vasp.staged\" subworkflow_name = \"relaxation.vasp.staged\"","title":"subworkflow_name"},{"location":"parameters/#supercell_end","text":"The endpoint image supercell to use. This is really just a structure parameter under a different name, so everything about the structure parameter also applies here.","title":"supercell_end"},{"location":"parameters/#supercell_start","text":"The starting image supercell to use. This is really just a structure parameter under a different name, so everything about the structure parameter also applies here.","title":"supercell_start"},{"location":"parameters/#symmetry_precision","text":"If standardize_structure=True, then this is the cutoff value used to determine if the sites are symmetrically equivalent. (in Angstroms) yaml python symmetry_precision : 0.1 symmetry_precision = 0.1","title":"symmetry_precision"},{"location":"parameters/#tags","text":"When submitting workflows via the run_cloud command, tags are 'labels' that help control which workers are allowed to pick up and run the submitted workflow. Workers should be started with matching tags in order for these scheduled flows to run. yaml toml python tags : - my-tag-01 - my-tag-02 tags = [ \"my-tag-01\" , \"my-tag-02\" ] tags = [ \"my-tag-01\" , \"my-tag-02\" ] Warning When you have a workflow that is submitting many smaller workflows (such as structure-prediction workflows), make sure you set the tags in the subworkflow_kwargs settings: subworkflow_kwargs : tags : - my-tag-01 - my-tag-02","title":"tags"},{"location":"parameters/#temperature_end","text":"For molecular dynamics simulations, this is the temperature to end the simulation at (in Kelvin). This temperatue will be reached through a linear transition from the temperature_start parameter. yaml toml python temperature_end : 1000 temperature_end = 1000 temperature_end = 1000","title":"temperature_end"},{"location":"parameters/#temperature_start","text":"For molecular dynamics simulations, this is the temperature to begin the simulation at (in Kelvin). yaml toml python temperature_start : 250 temperature_start = 250 temperature_start = 250","title":"temperature_start"},{"location":"parameters/#time_step","text":"For molecular dynamics simulations, this is time time between each ionic step (in femtoseconds). yaml toml python time_step : 1.5 time_step = 1.5 time_step = 1.5","title":"time_step"},{"location":"parameters/#updated_settings","text":"(experimental feature) Unique to customized.vasp.user-config . This is a list of parameters to update the workflow_base with. This often involves updating the base class attributes.","title":"updated_settings"},{"location":"parameters/#vacancy_mode","text":"For NEB and diffusion workfows, this determines whether vacancy or interstitial diffusion is analyzed. Default of True corresponds to vacancy-based diffusion. yaml toml python vacancy_mode : false vacancy_mode = false vacancy_mode = False","title":"vacancy_mode"},{"location":"parameters/#validator_kwargs","text":"(advanced users only) Extra conditions to use when initializing the validator class. MyValidator(**validator_kwargs) . The input should be given as a dictionary. Note, for evolutionary searches, the composition kwarg is added automatically. This is closely tied with the validator_name parameter so be sure to read that section as well.","title":"validator_kwargs"},{"location":"parameters/#validator_name","text":"(experimental feature; advanced users only) The base validator class that should be used. The class will be initialized using MyValidator(**validator_kwargs) . The input should be given as a string. Warning Currently, we only support CrystallNNFingerprint validation, so this should be left at its default value.","title":"validator_name"},{"location":"parameters/#workflow_base","text":"(experimental feature) Unique to customized.vasp.user-config . This is the base workflow to use when updating critical settings.","title":"workflow_base"},{"location":"parameters/#write_summary_files","text":"Whether or not to write output files. For some workflows, writing output files can cause excessive load on the database and possibly make the calculation IO bound. In cases such as this, you can set this to False . yaml toml python write_summary_files : false write_summary_files = false write_summary_files = False Tip Beginners can often ignore this setting. This is typically only relevant in a setup where you have many computational resources and have many evolutionary searches (>10) running at the same time.","title":"write_summary_files"},{"location":"contributing/creating_and_submitting_changes/","text":"Creating and submitting changes \u00b6 Make sure you have contacted our team on the changes you wish to make. This lets everyone know what is being working on and allows other contributors to give feedback/advice. You can request features, report bugs, etc. in our Issues page . General ideas and questions are better suited for our Discussions page . Make sure your fork is up to date with our main code at jacksund/simmate . This is only important if it's been more than a couple days since your last update. So if you just completed the previous section, you should be good to go here. Have the Simmate directory open in Spyder. This was done in steo 8 of the previous section. In Spyder, open the file you want and make your edits! Make sure you saved all file changes in Spyder. Simmate requires code to be nicely formatted and readable. To do this, we use the black formatter and isort for organzing imports. Make sure you are in the ~/Documents/github/simmate directory when you run these two commands: isort . black . Simmate has test cases to make sure new changes don't break any of Simmate's existing features. These are written using pytest . Run these to check your changes. Make sure you are in the ~/Documents/github/simmate directory when you run this command: # you can optionally run tests in parallel # with a command such as \"pytest -n 4\" pytest If everything passes, your changes can be accepted into Simmate! Open GitKraken, and you should then see your changes listed. For changes that you are happy with, stage and then commit them to the main branch of your repo ( yourname/simmate ). Feel free to add an emoji to the start of your commit message. These icons simply tell us that you read the tutorial (and also emojis are a fun distraction). Now open a pull-request to merge your changes into our main code (currently at jacksund/simmate ). We will review your changes and merge them if they pass all of our checks. Tip you can optionally format files as you code with Spyder too. Tools -> Preferences -> Completion and Linting -> Code Style and Formatting > Under code formatting dropdown, select black . To format a file you have open in Spyder, use the the Ctrl+Shift+I shortcut. Note Spyder does not yet have a plugin for pytest. We are still waiting on this feature to be built alongside their Unittest plugin .","title":"Creating & submitting changes"},{"location":"contributing/creating_and_submitting_changes/#creating-and-submitting-changes","text":"Make sure you have contacted our team on the changes you wish to make. This lets everyone know what is being working on and allows other contributors to give feedback/advice. You can request features, report bugs, etc. in our Issues page . General ideas and questions are better suited for our Discussions page . Make sure your fork is up to date with our main code at jacksund/simmate . This is only important if it's been more than a couple days since your last update. So if you just completed the previous section, you should be good to go here. Have the Simmate directory open in Spyder. This was done in steo 8 of the previous section. In Spyder, open the file you want and make your edits! Make sure you saved all file changes in Spyder. Simmate requires code to be nicely formatted and readable. To do this, we use the black formatter and isort for organzing imports. Make sure you are in the ~/Documents/github/simmate directory when you run these two commands: isort . black . Simmate has test cases to make sure new changes don't break any of Simmate's existing features. These are written using pytest . Run these to check your changes. Make sure you are in the ~/Documents/github/simmate directory when you run this command: # you can optionally run tests in parallel # with a command such as \"pytest -n 4\" pytest If everything passes, your changes can be accepted into Simmate! Open GitKraken, and you should then see your changes listed. For changes that you are happy with, stage and then commit them to the main branch of your repo ( yourname/simmate ). Feel free to add an emoji to the start of your commit message. These icons simply tell us that you read the tutorial (and also emojis are a fun distraction). Now open a pull-request to merge your changes into our main code (currently at jacksund/simmate ). We will review your changes and merge them if they pass all of our checks. Tip you can optionally format files as you code with Spyder too. Tools -> Preferences -> Completion and Linting -> Code Style and Formatting > Under code formatting dropdown, select black . To format a file you have open in Spyder, use the the Ctrl+Shift+I shortcut. Note Spyder does not yet have a plugin for pytest. We are still waiting on this feature to be built alongside their Unittest plugin .","title":"Creating and submitting changes"},{"location":"contributing/extra/","text":"Extra notes and tips \u00b6 Searching the source code \u00b6 If you changed a method significantly, you may need to find all places in Simmate that it is used. You can easily search for these using Spyders Find window. Use these steps to set up this window: In Spyder, go to the View tab (top of window) > Panes > check Find In the top-right window of spyder, you should now see the Find option. This will share a window with your Help window and Variable Explorer In the Find window, set Exclude to the following: (we don't want to search these files) *.csv, *.dat, *.log, *.tmp, *.bak, *.orig, *.egg-info, *.svg, *.xml, OUTCAR, *.js, *.html Set Search in to the src/simmate directory so that you only search source code. Git in the command-line \u00b6 While we recommand sticking with GitKraken , you may need to use the git command-line in some scenarios. Github has extensive guides on how to do this, but we outline the basics here. For configuring 2-factor-auth, we follow directions from here . To summarize: 1. Go to Profile >> Settings >> Account Security 2. Select \"Enable two-factor\" authentication 3. Follow the prompt to set up (I used SMS and save my codes to BitWarden) For configuring your API token, we follow directions from here . To summarize: 1. Go to Profile >> Settings >> Developer Settings >> Personal Access tokens 2. Generate new token for 90 days and with the \"repo\" scope and \"read:org\" 3. You now use this token as your password when running git commands And to configure permissions with git on the command-line (using this guide ): 1. make sure github cli is installed (`conda install -c conda-forge gh`) 2. run `gh auth login` and follow prompts to paste in personal token from above Lastly, some common commands include... # to copy a remote directory to your local disk git clone <GITHUB-URL> # while in a git directory, this pulls a specific branch (main here) git pull origin main # To remove all changes and reset your branch git restore .","title":"Extra notes & tips"},{"location":"contributing/extra/#extra-notes-and-tips","text":"","title":"Extra notes and tips"},{"location":"contributing/extra/#searching-the-source-code","text":"If you changed a method significantly, you may need to find all places in Simmate that it is used. You can easily search for these using Spyders Find window. Use these steps to set up this window: In Spyder, go to the View tab (top of window) > Panes > check Find In the top-right window of spyder, you should now see the Find option. This will share a window with your Help window and Variable Explorer In the Find window, set Exclude to the following: (we don't want to search these files) *.csv, *.dat, *.log, *.tmp, *.bak, *.orig, *.egg-info, *.svg, *.xml, OUTCAR, *.js, *.html Set Search in to the src/simmate directory so that you only search source code.","title":"Searching the source code"},{"location":"contributing/extra/#git-in-the-command-line","text":"While we recommand sticking with GitKraken , you may need to use the git command-line in some scenarios. Github has extensive guides on how to do this, but we outline the basics here. For configuring 2-factor-auth, we follow directions from here . To summarize: 1. Go to Profile >> Settings >> Account Security 2. Select \"Enable two-factor\" authentication 3. Follow the prompt to set up (I used SMS and save my codes to BitWarden) For configuring your API token, we follow directions from here . To summarize: 1. Go to Profile >> Settings >> Developer Settings >> Personal Access tokens 2. Generate new token for 90 days and with the \"repo\" scope and \"read:org\" 3. You now use this token as your password when running git commands And to configure permissions with git on the command-line (using this guide ): 1. make sure github cli is installed (`conda install -c conda-forge gh`) 2. run `gh auth login` and follow prompts to paste in personal token from above Lastly, some common commands include... # to copy a remote directory to your local disk git clone <GITHUB-URL> # while in a git directory, this pulls a specific branch (main here) git pull origin main # To remove all changes and reset your branch git restore .","title":"Git in the command-line"},{"location":"contributing/first_time_setup/","text":"First-time setup \u00b6 Tip If you are a student or teacher, we recommend using your Github account with Github's free Student/Teacher packages . This includes Github Pro and many other softwares that you may find useful. This is optional. Fork the Simmate repo to your Github profile (e.g. yourname/simmate ) Clone yourname/simmate to your local desktop. To do this, we recommend using GitKraken and cloning to a folder named ~/Documents/github/ . Note, Gitkraken is free for public repos (which includes Simmate), but also available with Github's free Student/Teacher packages . Their 6 minute beginner video will get you started. Navigate to where you cloned the Simmate repo: cd ~/Documents/github/simmate Create your conda env using our conda file. Note, this will install Spyder for you and name your new environment simmate_dev . We highly recommend you use Spyder as your IDE so that you have the same overall setup as the rest of the team. conda env update -f .github/environment.yaml conda install -n simmate_dev -c conda-forge spyder -y conda activate simmate_dev Install Simmate in developmental mode to your simmate_dev env. pip install -e . When resetting your database, make sure you do NOT use the prebuilt database. Pre-builts are only made for new releases and the dev database may differ from the most recent release. simmate database reset --confirm-delete --no-use-prebuilt Make sure everything works properly by running our tests # you can optionally run tests in parallel # with a command such as \"pytest -n 4\" pytest In GitKraken, make sure you have the main branch of your repo ( yourname/simmate ) checked out. In Spyder, go Projects > New Project... . Check existing directory , select your ~/Documents/github/simmate directory, and then create your Project! You can now explore the source code and add/edit files! Move to the next section on how to format, test, and submit these changes to our team.","title":"First time setup"},{"location":"contributing/first_time_setup/#first-time-setup","text":"Tip If you are a student or teacher, we recommend using your Github account with Github's free Student/Teacher packages . This includes Github Pro and many other softwares that you may find useful. This is optional. Fork the Simmate repo to your Github profile (e.g. yourname/simmate ) Clone yourname/simmate to your local desktop. To do this, we recommend using GitKraken and cloning to a folder named ~/Documents/github/ . Note, Gitkraken is free for public repos (which includes Simmate), but also available with Github's free Student/Teacher packages . Their 6 minute beginner video will get you started. Navigate to where you cloned the Simmate repo: cd ~/Documents/github/simmate Create your conda env using our conda file. Note, this will install Spyder for you and name your new environment simmate_dev . We highly recommend you use Spyder as your IDE so that you have the same overall setup as the rest of the team. conda env update -f .github/environment.yaml conda install -n simmate_dev -c conda-forge spyder -y conda activate simmate_dev Install Simmate in developmental mode to your simmate_dev env. pip install -e . When resetting your database, make sure you do NOT use the prebuilt database. Pre-builts are only made for new releases and the dev database may differ from the most recent release. simmate database reset --confirm-delete --no-use-prebuilt Make sure everything works properly by running our tests # you can optionally run tests in parallel # with a command such as \"pytest -n 4\" pytest In GitKraken, make sure you have the main branch of your repo ( yourname/simmate ) checked out. In Spyder, go Projects > New Project... . Check existing directory , select your ~/Documents/github/simmate directory, and then create your Project! You can now explore the source code and add/edit files! Move to the next section on how to format, test, and submit these changes to our team.","title":"First-time setup"},{"location":"contributing/maintainer_notes/","text":"Maintainer notes \u00b6 Note currently this section is only relevant to @jacksund To make a new release, you must follow these steps: Update the Simmate version number in pyproject.toml ( here ) Update the changelog with the new release and date Ensure all tests pass when using the pre-built database. Otherwise, you need to (i) make a new one using the commands below, (ii) rename your db file to something like prebuild-2022-07-05.sqlite3 , (iii) compress the db file to a zip file, (iv) upload it to the Simmate CDN, and (iii) update the archive_filename in simmate.database.third_parties.utilites.load_default_sqlite3_build . simmate database reset --confirm-delete --use-prebuilt false simmate database load-remote-archives Make a release on Github (which will automatically release to pypi) Wait for the autotick bot to open a pull request for the simmate feedstock . This can take up to 24hrs, but you can check the status here (under \"Queued\"). Make sure the autotick bot made the proper changes before merging. If there were any major changes, you can use grayskull to help update the version number, sha256, and dependencies. After merging, it takes the conda-forge channels 30min or so to update their indexes. Afterwards, you can test the conda install with: # for a normal release conda create -n my_env -c conda-forge simmate -y # as an extra, make sure spyder can also be installed in the same env conda install -n my_env -c conda-forge spyder -y","title":"Maintainer notes"},{"location":"contributing/maintainer_notes/#maintainer-notes","text":"Note currently this section is only relevant to @jacksund To make a new release, you must follow these steps: Update the Simmate version number in pyproject.toml ( here ) Update the changelog with the new release and date Ensure all tests pass when using the pre-built database. Otherwise, you need to (i) make a new one using the commands below, (ii) rename your db file to something like prebuild-2022-07-05.sqlite3 , (iii) compress the db file to a zip file, (iv) upload it to the Simmate CDN, and (iii) update the archive_filename in simmate.database.third_parties.utilites.load_default_sqlite3_build . simmate database reset --confirm-delete --use-prebuilt false simmate database load-remote-archives Make a release on Github (which will automatically release to pypi) Wait for the autotick bot to open a pull request for the simmate feedstock . This can take up to 24hrs, but you can check the status here (under \"Queued\"). Make sure the autotick bot made the proper changes before merging. If there were any major changes, you can use grayskull to help update the version number, sha256, and dependencies. After merging, it takes the conda-forge channels 30min or so to update their indexes. Afterwards, you can test the conda install with: # for a normal release conda create -n my_env -c conda-forge simmate -y # as an extra, make sure spyder can also be installed in the same env conda install -n my_env -c conda-forge spyder -y","title":"Maintainer notes"},{"location":"contributing/overview/","text":"Contributing to Simmate \u00b6 In this guide, you will learn how to contribute new features to Simmate's code. Learning about core dependencies \u00b6 When you first join our community, you may only be comfortable fixing typos and adding small utilities/functions. But if you would like to make larger changes/contributions, we highly recommend learning about our core dependencies. This includes... Django for our database and website Prefect for our workflows and scheduler Dask for clusters and execution You don't need to be an expert -- just aware of these packages and their features.","title":"Overview"},{"location":"contributing/overview/#contributing-to-simmate","text":"In this guide, you will learn how to contribute new features to Simmate's code.","title":"Contributing to Simmate"},{"location":"contributing/overview/#learning-about-core-dependencies","text":"When you first join our community, you may only be comfortable fixing typos and adding small utilities/functions. But if you would like to make larger changes/contributions, we highly recommend learning about our core dependencies. This includes... Django for our database and website Prefect for our workflows and scheduler Dask for clusters and execution You don't need to be an expert -- just aware of these packages and their features.","title":"Learning about core dependencies"},{"location":"full_guides/overview/","text":"The full Simmate guides & API reference \u00b6 Before you begin \u00b6 To get started, make sure you have either completed our introduction tutorials or are comfortable with python. Organization of guides vs. code \u00b6 Though we try to keep the organization of our guides and code as close as possible, they do not exactly follow the same structure. We learned over time that code/guide organization needs to be handled separately in order to help new users use Simmate without having mastered all of it components. documentation \u00b6 We try to organize our guides by difficultly level and how a user would normally begin using Simmate features. We expect users will start with highest-level features (e.g. the website interface) and then work their way to the lowest level ones from there (e.g. the toolkit and python objects). We therefore have the following up front: graph LR A[Website] --> B[Workflows]; B --> C[Database]; C --> D[Toolkit]; D --> E[Extras]; Tip Advanced topics are located at the end of each section. Unlike the getting-started guides, you do not need to complete a section in order to move on to the next one. python modules \u00b6 simmate is the base module and contains all of the code that our package runs on. Within each subfolder (aka each python \u201cmodule\u201d), you\u2019ll find more details on what it contains. But as a brief summary... calculators = third-party programs that run analyses for us (e.g. VASP which runs DFT calculations) command_line = makes some common functions available as commands in the terminal configuration = the defualt Simmate settings and how to update them database = defines how all Simmate data is organized into tables and let\u2019s you access it file_converters = reformat to/from file types (e.g. POSCAR \u2013> CIF) toolkit = the fundamental functions and classes for Simmate (e.g. the Structure class) utilities = contains simple functions that are used throughout the other modules visualization = visualizing structures, 3D data, and simple plots website = runs the simmate.org website workflow_engine = tools and utilities that help submit calculations as well as handle errors workflows = common analyses used in materials chemistry There is also one extra file\u2026 conftest = this is for running Simmate tests and only for contributing devs","title":"Overview"},{"location":"full_guides/overview/#the-full-simmate-guides-api-reference","text":"","title":"The full Simmate guides &amp; API reference"},{"location":"full_guides/overview/#before-you-begin","text":"To get started, make sure you have either completed our introduction tutorials or are comfortable with python.","title":"Before you begin"},{"location":"full_guides/overview/#organization-of-guides-vs-code","text":"Though we try to keep the organization of our guides and code as close as possible, they do not exactly follow the same structure. We learned over time that code/guide organization needs to be handled separately in order to help new users use Simmate without having mastered all of it components.","title":"Organization of guides vs. code"},{"location":"full_guides/overview/#documentation","text":"We try to organize our guides by difficultly level and how a user would normally begin using Simmate features. We expect users will start with highest-level features (e.g. the website interface) and then work their way to the lowest level ones from there (e.g. the toolkit and python objects). We therefore have the following up front: graph LR A[Website] --> B[Workflows]; B --> C[Database]; C --> D[Toolkit]; D --> E[Extras]; Tip Advanced topics are located at the end of each section. Unlike the getting-started guides, you do not need to complete a section in order to move on to the next one.","title":"documentation"},{"location":"full_guides/overview/#python-modules","text":"simmate is the base module and contains all of the code that our package runs on. Within each subfolder (aka each python \u201cmodule\u201d), you\u2019ll find more details on what it contains. But as a brief summary... calculators = third-party programs that run analyses for us (e.g. VASP which runs DFT calculations) command_line = makes some common functions available as commands in the terminal configuration = the defualt Simmate settings and how to update them database = defines how all Simmate data is organized into tables and let\u2019s you access it file_converters = reformat to/from file types (e.g. POSCAR \u2013> CIF) toolkit = the fundamental functions and classes for Simmate (e.g. the Structure class) utilities = contains simple functions that are used throughout the other modules visualization = visualizing structures, 3D data, and simple plots website = runs the simmate.org website workflow_engine = tools and utilities that help submit calculations as well as handle errors workflows = common analyses used in materials chemistry There is also one extra file\u2026 conftest = this is for running Simmate tests and only for contributing devs","title":"python modules"},{"location":"full_guides/toolkit/","text":"The Simmate Toolkit \u00b6 Danger Many classes in this module are highly experimental. We strongly recommend using pymatgen and ase for toolkit functionality until Simmate hits v1.0.0. For developers, this means many of the classes are undocumented and untested at the moment -- this facilitates our team trying different APIs/setups without spending a large amount of time reformating tests and rewriting guides. We include this experimental code on our main branch because higher-level functions (e.g. workflows) still rely on some of these features. Higher-level functions are well tested and documentated to account for changes in this module. The toolkit module is ment to be an extension of pymatgen and ase . It includes low-level classes and functions -- such as the Structure class and analyses ran on it. This module is entirely in python and does not involve calling third-party DFT programs (see the simmate.calculators module for those). The most commonly used classes from this toolkit are the Structure and Composition classes, which are located base_data_types module. For convenience, we also allow importing these classes directly with... from simmate.toolkit import Structure , Composition Outline of submodules \u00b6 base_data_types = defines fundamental classes for materials science creators = creates structures, lattices, and periodic sites featurizers = makes properties into numerical descriptors for machine-learning structure_prediction = predicts reasonable crystal structures given a composition symmetry = contains tools/metadata for symmetry, such as spacegroups and wyckoff sites transformations = define transformations/mutations that can be applied to Structures validators = evulate structures/lattices/etc. to see if they meet given criteria","title":"Overview"},{"location":"full_guides/toolkit/#the-simmate-toolkit","text":"Danger Many classes in this module are highly experimental. We strongly recommend using pymatgen and ase for toolkit functionality until Simmate hits v1.0.0. For developers, this means many of the classes are undocumented and untested at the moment -- this facilitates our team trying different APIs/setups without spending a large amount of time reformating tests and rewriting guides. We include this experimental code on our main branch because higher-level functions (e.g. workflows) still rely on some of these features. Higher-level functions are well tested and documentated to account for changes in this module. The toolkit module is ment to be an extension of pymatgen and ase . It includes low-level classes and functions -- such as the Structure class and analyses ran on it. This module is entirely in python and does not involve calling third-party DFT programs (see the simmate.calculators module for those). The most commonly used classes from this toolkit are the Structure and Composition classes, which are located base_data_types module. For convenience, we also allow importing these classes directly with... from simmate.toolkit import Structure , Composition","title":"The Simmate Toolkit"},{"location":"full_guides/toolkit/#outline-of-submodules","text":"base_data_types = defines fundamental classes for materials science creators = creates structures, lattices, and periodic sites featurizers = makes properties into numerical descriptors for machine-learning structure_prediction = predicts reasonable crystal structures given a composition symmetry = contains tools/metadata for symmetry, such as spacegroups and wyckoff sites transformations = define transformations/mutations that can be applied to Structures validators = evulate structures/lattices/etc. to see if they meet given criteria","title":"Outline of submodules"},{"location":"full_guides/database/basic_use/","text":"Basic Database Access \u00b6 Outline of all steps \u00b6 Accessing and analyzing data typically involves the following steps: Connect to your database Load your database table Query and filter data Convert data to desired format Modify data via simmate.toolkit or pandas.Dataframe The sections below will guide you on performing each of these steps. But to place everything up-front, your final script may look something like this: # Connect to your database from simmate.database import connect # Load your database table from simmate.database.third_parties import MatprojStructure # Query and filter data results = MatprojStructure . objects . filter ( nsites = 3 , is_gap_direct = False , spacegroup = 166 , ) . all () # Convert data to desired format structures = results . to_toolkit () dataframe = results . to_dataframe () # Modify data for structure in structures : # run your anaylsis/modifications here! Connect to your database \u00b6 For interactive use, Django settings must be configured before any of these submodules can be imported. This can be done with... # connect to the database from simmate.database import connect # and now you can import tables in this module from simmate.database.workflow_results import MITStaticEnergy If this is not done, you will recieve the following error: ImproperlyConfigured : Requested setting INSTALLED_APPS , but settings are not configured . You must either define the environment variable DJANGO_SETTINGS_MODULE or call settings . configure () before accessing settings . Load your database table \u00b6 The location of your table will depend on what data you're trying to access. To search, you can explore the other modules within this one (see top of this page where there are list). Using Materials Project as an example, we can load the table using... from simmate.database.third_parties import MatprojStructure If you are accessing data from a specific workflow, then in addition to loading from the workflow_results module, most workflows have a database_table attribute that let you access the table as well: # There are two ways to load a table from calculation results... ######## METHOD 1 ######## from simmate.workflows.static_energy import mit_workflow table = mit_workflow . database_table ######## METHOD 2 ######## from simmate.database import connect from simmate.database.workflow_results import MITStaticEnergy # This line proves these tables are the same! In practice, you only need to # load the table via one of these two methods -- whichever you prefer. assert table == MITStaticEnergy Query and filter data \u00b6 Simmate uses Django ORM under the hood, so it follows the same API for making queries . Below we reiterate the most basic functionality, but full features are discussed in the Django's Model-layer documentation . All rows of the database table are available via the objects attribute: MITStaticEnergy . objects . all () All columns of the database table can be printed via the show_columns methods: MITStaticEnergy . show_columns () To filter rows with exact-value matches in a column: MITStaticEnergy . objects . filter ( nsites = 3 , is_gap_direct = False , spacegroup = 166 , ) . all () To filter rows based on conditions, chain the column name with two underscores. Conditions supported are listed here , but the most commonly used ones are: contains , in , gt , gte , lt , lte , range , isnull An example query with conditional filters: MITStaticEnergy . objects . filter ( nsites__gte = 3 , # greater or equal to 3 sites energy__isnull = False , # the structure DOES have an energy density__range = ( 1 , 5 ), # density is between 1 and 5 elements__icontains = '\"C\"' , # the structure includes the element Carbon spacegroup__number = 167 , # the spacegroup number is 167 ) . all () Note, for the filtering condition elements__icontains , we used some odd quotations when querying for carbon: '\"C\"' . This is not a typo! The quotes ensure we don't accidentally grab Ca, Cs, Ce, Cl, and so on. This is an issue when you are using SQLite (the default datbase backend). If you are using Postgres, this line can change to the cleaner version elements__contains=\"C\" . Convert data to desired format \u00b6 By default, Django returns your query results as a queryset (or SearchResults in simmate). This is a list of database objects. It is more useful to convert them to a pandas dataframe or to toolkit objects. # Gives a pandas dataframe df = MITStaticEnergy . objects . filter ( ... ) . to_dataframe () # Gives a list of toolkit Structure objects df = MITStaticEnergy . objects . filter ( ... ) . to_toolkit () Modify data \u00b6 To modify and analyze data, see the pandas and simmate.toolkit documentation for more info.","title":"Basic use"},{"location":"full_guides/database/basic_use/#basic-database-access","text":"","title":"Basic Database Access"},{"location":"full_guides/database/basic_use/#outline-of-all-steps","text":"Accessing and analyzing data typically involves the following steps: Connect to your database Load your database table Query and filter data Convert data to desired format Modify data via simmate.toolkit or pandas.Dataframe The sections below will guide you on performing each of these steps. But to place everything up-front, your final script may look something like this: # Connect to your database from simmate.database import connect # Load your database table from simmate.database.third_parties import MatprojStructure # Query and filter data results = MatprojStructure . objects . filter ( nsites = 3 , is_gap_direct = False , spacegroup = 166 , ) . all () # Convert data to desired format structures = results . to_toolkit () dataframe = results . to_dataframe () # Modify data for structure in structures : # run your anaylsis/modifications here!","title":"Outline of all steps"},{"location":"full_guides/database/basic_use/#connect-to-your-database","text":"For interactive use, Django settings must be configured before any of these submodules can be imported. This can be done with... # connect to the database from simmate.database import connect # and now you can import tables in this module from simmate.database.workflow_results import MITStaticEnergy If this is not done, you will recieve the following error: ImproperlyConfigured : Requested setting INSTALLED_APPS , but settings are not configured . You must either define the environment variable DJANGO_SETTINGS_MODULE or call settings . configure () before accessing settings .","title":"Connect to your database"},{"location":"full_guides/database/basic_use/#load-your-database-table","text":"The location of your table will depend on what data you're trying to access. To search, you can explore the other modules within this one (see top of this page where there are list). Using Materials Project as an example, we can load the table using... from simmate.database.third_parties import MatprojStructure If you are accessing data from a specific workflow, then in addition to loading from the workflow_results module, most workflows have a database_table attribute that let you access the table as well: # There are two ways to load a table from calculation results... ######## METHOD 1 ######## from simmate.workflows.static_energy import mit_workflow table = mit_workflow . database_table ######## METHOD 2 ######## from simmate.database import connect from simmate.database.workflow_results import MITStaticEnergy # This line proves these tables are the same! In practice, you only need to # load the table via one of these two methods -- whichever you prefer. assert table == MITStaticEnergy","title":"Load your database table"},{"location":"full_guides/database/basic_use/#query-and-filter-data","text":"Simmate uses Django ORM under the hood, so it follows the same API for making queries . Below we reiterate the most basic functionality, but full features are discussed in the Django's Model-layer documentation . All rows of the database table are available via the objects attribute: MITStaticEnergy . objects . all () All columns of the database table can be printed via the show_columns methods: MITStaticEnergy . show_columns () To filter rows with exact-value matches in a column: MITStaticEnergy . objects . filter ( nsites = 3 , is_gap_direct = False , spacegroup = 166 , ) . all () To filter rows based on conditions, chain the column name with two underscores. Conditions supported are listed here , but the most commonly used ones are: contains , in , gt , gte , lt , lte , range , isnull An example query with conditional filters: MITStaticEnergy . objects . filter ( nsites__gte = 3 , # greater or equal to 3 sites energy__isnull = False , # the structure DOES have an energy density__range = ( 1 , 5 ), # density is between 1 and 5 elements__icontains = '\"C\"' , # the structure includes the element Carbon spacegroup__number = 167 , # the spacegroup number is 167 ) . all () Note, for the filtering condition elements__icontains , we used some odd quotations when querying for carbon: '\"C\"' . This is not a typo! The quotes ensure we don't accidentally grab Ca, Cs, Ce, Cl, and so on. This is an issue when you are using SQLite (the default datbase backend). If you are using Postgres, this line can change to the cleaner version elements__contains=\"C\" .","title":"Query and filter data"},{"location":"full_guides/database/basic_use/#convert-data-to-desired-format","text":"By default, Django returns your query results as a queryset (or SearchResults in simmate). This is a list of database objects. It is more useful to convert them to a pandas dataframe or to toolkit objects. # Gives a pandas dataframe df = MITStaticEnergy . objects . filter ( ... ) . to_dataframe () # Gives a list of toolkit Structure objects df = MITStaticEnergy . objects . filter ( ... ) . to_toolkit ()","title":"Convert data to desired format"},{"location":"full_guides/database/basic_use/#modify-data","text":"To modify and analyze data, see the pandas and simmate.toolkit documentation for more info.","title":"Modify data"},{"location":"full_guides/database/contributing_data/","text":"Adding your data to Simmate \u00b6 Warning This module is only for the Simmate dev team or third-party contributors that want to add their own data! Users should instead use the load_remote_archive method to access data. See the database docs . This module is for pulling data from various databases into Simmate using third-party codes. This can then be used to build archives that users may access. Benefits of adding your data to Simmate \u00b6 When deciding whether your team should use Simmate, we can break down discussion to two key questions: Can you benefit from converting data into a Simmate format? Can you benefit from distributing an archive? (private or public) We will answer these questions in the next two sections. Converting data into a Simmate format \u00b6 Whether your data is open-source or proprietary, the answer to question 1 will be the same: Providers can benefit from using Simmate's database module because it... automatically builds an API and ORM for your data greatly reduces the file size of your archives By providing raw data (like a structure or energy), Simmate will automatically expand your data into the most useful columns, and you can then use our ORM to query data rapidly. For example, Simmate can use an energy column/field to create columns for energy_above_hull , formation_energy , decomposes_to , and more -- then you can filter through your data using these new columns. See the \"Querying Data\" section in the simmate.database module for examples of this query language. Using the concepts of \"raw data\" vs \"secondary columns\" (columns that can be rapidly remade/calculated using the raw data), Simmate can efficiently compress your data to a small format. To see just how small, check out the file sizes for archives of current providers: Provider Number of Structures Av. Sites per Structure Archive Size JARVIS 55,712 ~10 8.0 MB Materials Project 137,885 ~30 45.2 MB COD 471,664 ~248 1.16 GB OQMD 1,013,521 ~7 79.2 MB AFLOW n/a n/a n/a (Note, COD experiences poor compression because Simmate has not yet optimized storage for disordered structures.) These small file sizes will make it much easier for downloading and sharing your data. This can have major savings on your database server as well. Hosting & distributing the archive \u00b6 Here is where being a private vs. open-source provider becomes important. Simmate lets you to decide how others access your data. If your data can only be accessible to among your own team members or subscribers, then you can be in charge of distruting the data (via a CDN, dropbox, etc.). Simmate does not require that you distribute your data freely -- though we do encourage open-source data. Either way, you can benefit from... lessening the load on your own web APIs Server load can be reduced because, in Simmate, users download your archive once and then have the data stored locally for as long as they'd like. New users often want to download a massive portion a database (or all of it) -- and also do so repeatedly as they learn about APIs, so using Simmate archives upfront can save your team from these large and often-repeated queries. If you are fine with making your data freely available, you can further benefit by... skipping the setup of up your own server and instead use Simmate's for free exposing your data to the Simmate user base Providers that permit redistribution are welcome to use our CDN for their archives. This only requires contacting our team and making this request. Further, once your archive is configured, all Simmate users will be able to easily access your data. How to add your data or a new provider \u00b6 Tip If you want to avoid this guide, you can just contact our team! Open a github issue to get our attention. In most cases, we only need a CSV or JSON file of your data (in any data format you'd like), and we can handle the rest for you. If you'd like to contribute the data on your own, keep reading. The end goal for each provider is to allow a user do the following: from simmate.database import connect from simmate.third_parties import ExampleProviderData ExampleProviderData . load_remote_archive () search_results = ExampleProviderData . objects . filter ( ... ) . all () # plus all to_dataframe / to_toolkit features discussed elsewhere The key part that providers must understand is the load_remote_archive method. This method... loads an archive of available data (as a zip file from some CDN) unpacks the data into the Simmate format saves everything to the user's database (by defualt this is ~/simmate/database.sqlite3 ). This guide serves to make the first step work! Specifically, providers must make the archive that load_remote_archive will load in step 1 and make it downloadable by a CDN or API endpoint. It is up to the provider whether they personally distribute the archive or allow Simmate to distribute it for them. Outline of steps \u00b6 To illustrate how this is done, we will walk through the required steps: Define a Simmate table Download data into the Simmate format (i.e. populate the Simmate table) Compress the data to archive file Make the archive available via a CDN Link the CDN to the Simmate table Tip these steps involve contributing changes to Simmate's code, so we recommend opening a github issue before starting too. That way, our team can help you through this process. If you are new to Github and contributing, be sure to read our tutorial for contributors too. Step 1: Define a Simmate table \u00b6 To host data, Simmate must first know what kind of data you are going to host. We do this by adding a new file to the simmate.database.third_party module. You can view this folder on github here . Start by defining a DatabaseTable with any custom columns / database mix-ins. You can scroll through the other providers to see how tables are made. Good examples to view are for JARVIS and Materials Project . Here is a template with useful comments to get you started: # Start by deciding which base data types you can include. Here, we include a # crystal structure and an energy, so we use the Structure and Thermodynamics # mix-ins. from simmate.database.base_data_types import ( table_column , Structure , Thermodynamics , ) class ExampleProviderData ( Structure , Thermodynamics ): # ----- Table columns + Required settings ----- # This Meta class tells Simmate where to store the table within our database. # All providers with have the exact same thing here. class Meta : app_label = \"third_parties\" # By default, the ID column is an IntegerField, but if your data uses a string # like \"mp-1234\" to denote structures, you can update this column to # accept a string instead. id = table_column . CharField ( max_length = 25 , primary_key = True ) # Write the name of your team here! source = \"The Example Provider Project\" # We have many alerts to let users know they should cite you. Add the DOI # that you'd like them to cite here. source_doi = \"https://doi.org/...\" # If you have any custom fields that you'd like to add, list them off here. # All data types supported by Django are also supported by Simmate. You can # view those options here: # https://docs.djangoproject.com/en/4.0/ref/models/fields/ custom_column_01 = table_column . FloatField ( blank = True , null = True ) custom_column_02 = table_column . BooleanField ( blank = True , null = True ) # Leave this as None for now. We will update this attribute in a later step. remote_archive_link = None # ----- Extra optionalfeatures ----- # (OPTIONAL) Define the \"raw data\" for your table. This is required if # you'd like to use the `to_archive` method. Fields from the mix-in # will automatically be added. archive_fields = [ \"custom_column_01\" , \"custom_column_02\" , ] # (OPTIONAL) Define how you would like data to be accessible in the REST # API from the website server. api_filters = { \"custom_column_01\" : [ \"range\" ], \"custom_column_02\" : [ \"range\" ], } # (OPTIONAL) if you host your data on a separate website, you can specify # how to access that structure here. This is important if you want users # to switch to your site for aquiring additional data. @property def external_link ( self ) -> str : return f \"https://www.exampleprovider.com/structure/ { self . id } \" Before moving on, make sure your table was configured properly by doing the following: # in the command line simmate database reset # in python from simmate.database import connect from simmate.third_parties import ExampleProviderData # This will show you all the columns for your table ExampleProviderData . show_columns () # this will show you exactly what the table looks like my_table = ExampleProviderData . objects . to_dataframe () Step 2: Download data into the Simmate format \u00b6 Now that Simmate knows what to expect, we can load your data into the database. This can be done in serveral ways. It is entirely up to you which method to use, but here are our recommended options: JSON or CSV file. If all of your data can be provide via a dump file, then we can use that! This is typically the easiest for a provider's server. For an example of this, see the COD implementation, which uses a download of CIF files. A custom python package. Feel free to add an optional dependency if your team has already put a lot of work into loading data using a python package. A great example of this is the MPRester class in pymatgen, which we use to pull Material Project data. (JARVIS, AFLOW, OQMD currently use this option too). REST API or GraphQL. If you have a web API, we can easily pull data using the python requests package. Note, in many cases, a REST API is an inefficient way to pull data - as it involves querying a database thousands of times (once for each page of structures) -- potentially crashing the server. In cases like that, we actually prefer a download file (option 1, shown above). OPTIMADE endpoint. This is a standardized REST API endpoint that many databases are using now. The huge upside here is that each database will have a matching API -- so once your team has an OPTIMADE endpoint, we can pull data into Simmate with ease. There's no need to build a 2nd implementation. The downside is the same as option 3: OPTIMADE doesn't have a good way to pull data in bulk. Their team is currently working on this though . Web scraping. As an absolute last resort, we can use requests to scrape webpages for data (or selenium in even more extreme cases.). This requires the most work from our team and is also the least efficient way to grab data. Therefore, scraping should always be avoided if possible. With your data in hand, you will now add a file that saves data to the local simmate database on your computer. This file can be added to the for_providers module ( here ). However, if you want your data and it's access to remain private, you can also keep this file out of Simmate's source-code. It's up to you, but we encourage providers to host their file in the Simmate repo -- so we can give feedback and so future providers can use it as an example/guide. Either way, here is a template of how that file will look like: from django.db import transaction from rich.progress import track from simmate.toolkit import Structure from simmate.database.third_parties import ExampleProviderData # If you want to use a custom package to load your data, be sure to let our team # know how to install it. try : from my_package.db import get_my_data except : raise ModuleNotFoundError ( \"You must install my_package with `conda install -c conda-forge my_package`\" ) # We make this an \"atomic transaction\", which means if any error is encountered # while saving results to the database, then the database will be reset to it's # original state. Adding this decorator is optional @transaction . atomic def load_all_structures (): # Use whichever method you chose above to load all of your data! # Here' we are pretending to use a function that loads all data into a # python dictionary, but this can vary. data = get_my_data () # Now iterate through all the data -- which is a list of dictionaries. # We use rich.progress.track to monitor progress. for entry in track ( data ): # The structure is in the atoms field as a dictionary. We pull this data # out and convert it to a toolkit Structure object. Note, this class # is currently a subclass of pymatgen.Structure, so it supports reading # from different file formats (like CIF or POSCAR) as well. structure = Structure ( lattice = entry [ \"atoms\" ][ \"lattice_mat\" ], species = entry [ \"atoms\" ][ \"elements\" ], coords = entry [ \"atoms\" ][ \"coords\" ], coords_are_cartesian = entry [ \"atoms\" ][ \"cartesian\" ], ) # Now that we have a structure object, we can feed that and all # other data to the from_toolkit() method. This will create a database # object in the Simmate format. Note the data we pass here is based on # the ExampleProviderData we defined in the other file. structure_db = ExampleProviderData . from_toolkit ( id = entry [ \"my_id\" ], structure = structure , # required by Structure mix-in energy = entry [ \"my_final_energy\" ], # required by Thermodynamics mix-in custom_column_01 = entry [ \"my_custom_column_01\" ], # The get method is useful if not all entries have a given field. custom_column_02 = entry . get ( \"my_custom_column_02\" ), ) # and save it to our database! structure_db . save () Try running this on your dataset (or a subset of data if you want to quickly test things). When it finishes, you can ensure data was loaded properly by running: # in python from simmate.database import connect from simmate.third_parties import ExampleProviderData # Check that the number of rows matches your source data. total_entries = ExampleProviderData . objects . count () # View the data! # The [:100] limits this to your first 100 results my_table = ExampleProviderData . objects . to_dataframe ()[: 100 ] And that's it for writing new code! All that's left is making your data available for others. Step 3: Compress the data to archive file \u00b6 This will be the easiest step yet. We need to make a zip file for users to download, which can be done in one line: ExampleProviderData . objects . to_archive () You'll find a file named ExampleProviderData-2022-01-25.zip (but with the current date) in your working directory. The date is for timestamp and versioning your archives. Because archives are a snapshot of databases that may be dynamically changing/going, this timestamp helps users know which version they are on. You can practice reloading this data into your database too: Make a copy of your database file in ~/simmate/ so you don't lose your work In the terminal, reset your database with simmate database reset In python, try reloading your data with ExampleProviderData.load_archive() Try viewing your data again with ExampleProviderData.objects.to_dataframe() Step 4: Make the archive available via a CDN \u00b6 Users with now need the archive file you made to access your data. So you must decide: how should this zip file be downloaded by users? If you give Simmate approval, we can host your archive file on our own servers. Otherwise you must host your own. The only requirement for your host server is that the zip file can be downloaded from a URL. While we encourage open-source databases, if you consider your dataset private or commercial, Simmate does not require any payment or involvement for how this CDN is hosted and maintained. Thus, you can manage access to this URL via a subscription or any other method. However, Simmate's CDNs are reserved for archives that are freely distributed. Note: when uploading new versions of your archive, you should keep the outdated archive either available via its previous URL or, at a minimum, available upon request from users. Step 5: Link the CDN to the Simmate table \u00b6 In Step 1, we left one attribute as None in our code: remote_archive_link . As a final step, you need to take the URL that you're host your zip file at and paste it here. For example, that line will become: remote_archive_link = \"https://archives.simmate.org/ExampleProviderData-2022-01-25.zip\" That's it! Let's test out everything again. Note, we are now using load_remote_archive in this process -- which will load your zip file from the URL. Make a copy of your database file in ~/simmate/ so you don't lose your work In the terminal, reset your database with simmate database reset In python, try reloading your data with ExampleProviderData.load_remote_archive() Try viewing your data again with ExampleProviderData.objects.to_dataframe() If you've made it this far, thank you for contributing!!! Your data is now easily accessible to all Simmate users, which we hope facilitates its use and even lessen the load on your own servers. Congrats!","title":"Contributing data"},{"location":"full_guides/database/contributing_data/#adding-your-data-to-simmate","text":"Warning This module is only for the Simmate dev team or third-party contributors that want to add their own data! Users should instead use the load_remote_archive method to access data. See the database docs . This module is for pulling data from various databases into Simmate using third-party codes. This can then be used to build archives that users may access.","title":"Adding your data to Simmate"},{"location":"full_guides/database/contributing_data/#benefits-of-adding-your-data-to-simmate","text":"When deciding whether your team should use Simmate, we can break down discussion to two key questions: Can you benefit from converting data into a Simmate format? Can you benefit from distributing an archive? (private or public) We will answer these questions in the next two sections.","title":"Benefits of adding your data to Simmate"},{"location":"full_guides/database/contributing_data/#converting-data-into-a-simmate-format","text":"Whether your data is open-source or proprietary, the answer to question 1 will be the same: Providers can benefit from using Simmate's database module because it... automatically builds an API and ORM for your data greatly reduces the file size of your archives By providing raw data (like a structure or energy), Simmate will automatically expand your data into the most useful columns, and you can then use our ORM to query data rapidly. For example, Simmate can use an energy column/field to create columns for energy_above_hull , formation_energy , decomposes_to , and more -- then you can filter through your data using these new columns. See the \"Querying Data\" section in the simmate.database module for examples of this query language. Using the concepts of \"raw data\" vs \"secondary columns\" (columns that can be rapidly remade/calculated using the raw data), Simmate can efficiently compress your data to a small format. To see just how small, check out the file sizes for archives of current providers: Provider Number of Structures Av. Sites per Structure Archive Size JARVIS 55,712 ~10 8.0 MB Materials Project 137,885 ~30 45.2 MB COD 471,664 ~248 1.16 GB OQMD 1,013,521 ~7 79.2 MB AFLOW n/a n/a n/a (Note, COD experiences poor compression because Simmate has not yet optimized storage for disordered structures.) These small file sizes will make it much easier for downloading and sharing your data. This can have major savings on your database server as well.","title":"Converting data into a Simmate format"},{"location":"full_guides/database/contributing_data/#hosting-distributing-the-archive","text":"Here is where being a private vs. open-source provider becomes important. Simmate lets you to decide how others access your data. If your data can only be accessible to among your own team members or subscribers, then you can be in charge of distruting the data (via a CDN, dropbox, etc.). Simmate does not require that you distribute your data freely -- though we do encourage open-source data. Either way, you can benefit from... lessening the load on your own web APIs Server load can be reduced because, in Simmate, users download your archive once and then have the data stored locally for as long as they'd like. New users often want to download a massive portion a database (or all of it) -- and also do so repeatedly as they learn about APIs, so using Simmate archives upfront can save your team from these large and often-repeated queries. If you are fine with making your data freely available, you can further benefit by... skipping the setup of up your own server and instead use Simmate's for free exposing your data to the Simmate user base Providers that permit redistribution are welcome to use our CDN for their archives. This only requires contacting our team and making this request. Further, once your archive is configured, all Simmate users will be able to easily access your data.","title":"Hosting &amp; distributing the archive"},{"location":"full_guides/database/contributing_data/#how-to-add-your-data-or-a-new-provider","text":"Tip If you want to avoid this guide, you can just contact our team! Open a github issue to get our attention. In most cases, we only need a CSV or JSON file of your data (in any data format you'd like), and we can handle the rest for you. If you'd like to contribute the data on your own, keep reading. The end goal for each provider is to allow a user do the following: from simmate.database import connect from simmate.third_parties import ExampleProviderData ExampleProviderData . load_remote_archive () search_results = ExampleProviderData . objects . filter ( ... ) . all () # plus all to_dataframe / to_toolkit features discussed elsewhere The key part that providers must understand is the load_remote_archive method. This method... loads an archive of available data (as a zip file from some CDN) unpacks the data into the Simmate format saves everything to the user's database (by defualt this is ~/simmate/database.sqlite3 ). This guide serves to make the first step work! Specifically, providers must make the archive that load_remote_archive will load in step 1 and make it downloadable by a CDN or API endpoint. It is up to the provider whether they personally distribute the archive or allow Simmate to distribute it for them.","title":"How to add your data or a new provider"},{"location":"full_guides/database/contributing_data/#outline-of-steps","text":"To illustrate how this is done, we will walk through the required steps: Define a Simmate table Download data into the Simmate format (i.e. populate the Simmate table) Compress the data to archive file Make the archive available via a CDN Link the CDN to the Simmate table Tip these steps involve contributing changes to Simmate's code, so we recommend opening a github issue before starting too. That way, our team can help you through this process. If you are new to Github and contributing, be sure to read our tutorial for contributors too.","title":"Outline of steps"},{"location":"full_guides/database/contributing_data/#step-1-define-a-simmate-table","text":"To host data, Simmate must first know what kind of data you are going to host. We do this by adding a new file to the simmate.database.third_party module. You can view this folder on github here . Start by defining a DatabaseTable with any custom columns / database mix-ins. You can scroll through the other providers to see how tables are made. Good examples to view are for JARVIS and Materials Project . Here is a template with useful comments to get you started: # Start by deciding which base data types you can include. Here, we include a # crystal structure and an energy, so we use the Structure and Thermodynamics # mix-ins. from simmate.database.base_data_types import ( table_column , Structure , Thermodynamics , ) class ExampleProviderData ( Structure , Thermodynamics ): # ----- Table columns + Required settings ----- # This Meta class tells Simmate where to store the table within our database. # All providers with have the exact same thing here. class Meta : app_label = \"third_parties\" # By default, the ID column is an IntegerField, but if your data uses a string # like \"mp-1234\" to denote structures, you can update this column to # accept a string instead. id = table_column . CharField ( max_length = 25 , primary_key = True ) # Write the name of your team here! source = \"The Example Provider Project\" # We have many alerts to let users know they should cite you. Add the DOI # that you'd like them to cite here. source_doi = \"https://doi.org/...\" # If you have any custom fields that you'd like to add, list them off here. # All data types supported by Django are also supported by Simmate. You can # view those options here: # https://docs.djangoproject.com/en/4.0/ref/models/fields/ custom_column_01 = table_column . FloatField ( blank = True , null = True ) custom_column_02 = table_column . BooleanField ( blank = True , null = True ) # Leave this as None for now. We will update this attribute in a later step. remote_archive_link = None # ----- Extra optionalfeatures ----- # (OPTIONAL) Define the \"raw data\" for your table. This is required if # you'd like to use the `to_archive` method. Fields from the mix-in # will automatically be added. archive_fields = [ \"custom_column_01\" , \"custom_column_02\" , ] # (OPTIONAL) Define how you would like data to be accessible in the REST # API from the website server. api_filters = { \"custom_column_01\" : [ \"range\" ], \"custom_column_02\" : [ \"range\" ], } # (OPTIONAL) if you host your data on a separate website, you can specify # how to access that structure here. This is important if you want users # to switch to your site for aquiring additional data. @property def external_link ( self ) -> str : return f \"https://www.exampleprovider.com/structure/ { self . id } \" Before moving on, make sure your table was configured properly by doing the following: # in the command line simmate database reset # in python from simmate.database import connect from simmate.third_parties import ExampleProviderData # This will show you all the columns for your table ExampleProviderData . show_columns () # this will show you exactly what the table looks like my_table = ExampleProviderData . objects . to_dataframe ()","title":"Step 1: Define a Simmate table"},{"location":"full_guides/database/contributing_data/#step-2-download-data-into-the-simmate-format","text":"Now that Simmate knows what to expect, we can load your data into the database. This can be done in serveral ways. It is entirely up to you which method to use, but here are our recommended options: JSON or CSV file. If all of your data can be provide via a dump file, then we can use that! This is typically the easiest for a provider's server. For an example of this, see the COD implementation, which uses a download of CIF files. A custom python package. Feel free to add an optional dependency if your team has already put a lot of work into loading data using a python package. A great example of this is the MPRester class in pymatgen, which we use to pull Material Project data. (JARVIS, AFLOW, OQMD currently use this option too). REST API or GraphQL. If you have a web API, we can easily pull data using the python requests package. Note, in many cases, a REST API is an inefficient way to pull data - as it involves querying a database thousands of times (once for each page of structures) -- potentially crashing the server. In cases like that, we actually prefer a download file (option 1, shown above). OPTIMADE endpoint. This is a standardized REST API endpoint that many databases are using now. The huge upside here is that each database will have a matching API -- so once your team has an OPTIMADE endpoint, we can pull data into Simmate with ease. There's no need to build a 2nd implementation. The downside is the same as option 3: OPTIMADE doesn't have a good way to pull data in bulk. Their team is currently working on this though . Web scraping. As an absolute last resort, we can use requests to scrape webpages for data (or selenium in even more extreme cases.). This requires the most work from our team and is also the least efficient way to grab data. Therefore, scraping should always be avoided if possible. With your data in hand, you will now add a file that saves data to the local simmate database on your computer. This file can be added to the for_providers module ( here ). However, if you want your data and it's access to remain private, you can also keep this file out of Simmate's source-code. It's up to you, but we encourage providers to host their file in the Simmate repo -- so we can give feedback and so future providers can use it as an example/guide. Either way, here is a template of how that file will look like: from django.db import transaction from rich.progress import track from simmate.toolkit import Structure from simmate.database.third_parties import ExampleProviderData # If you want to use a custom package to load your data, be sure to let our team # know how to install it. try : from my_package.db import get_my_data except : raise ModuleNotFoundError ( \"You must install my_package with `conda install -c conda-forge my_package`\" ) # We make this an \"atomic transaction\", which means if any error is encountered # while saving results to the database, then the database will be reset to it's # original state. Adding this decorator is optional @transaction . atomic def load_all_structures (): # Use whichever method you chose above to load all of your data! # Here' we are pretending to use a function that loads all data into a # python dictionary, but this can vary. data = get_my_data () # Now iterate through all the data -- which is a list of dictionaries. # We use rich.progress.track to monitor progress. for entry in track ( data ): # The structure is in the atoms field as a dictionary. We pull this data # out and convert it to a toolkit Structure object. Note, this class # is currently a subclass of pymatgen.Structure, so it supports reading # from different file formats (like CIF or POSCAR) as well. structure = Structure ( lattice = entry [ \"atoms\" ][ \"lattice_mat\" ], species = entry [ \"atoms\" ][ \"elements\" ], coords = entry [ \"atoms\" ][ \"coords\" ], coords_are_cartesian = entry [ \"atoms\" ][ \"cartesian\" ], ) # Now that we have a structure object, we can feed that and all # other data to the from_toolkit() method. This will create a database # object in the Simmate format. Note the data we pass here is based on # the ExampleProviderData we defined in the other file. structure_db = ExampleProviderData . from_toolkit ( id = entry [ \"my_id\" ], structure = structure , # required by Structure mix-in energy = entry [ \"my_final_energy\" ], # required by Thermodynamics mix-in custom_column_01 = entry [ \"my_custom_column_01\" ], # The get method is useful if not all entries have a given field. custom_column_02 = entry . get ( \"my_custom_column_02\" ), ) # and save it to our database! structure_db . save () Try running this on your dataset (or a subset of data if you want to quickly test things). When it finishes, you can ensure data was loaded properly by running: # in python from simmate.database import connect from simmate.third_parties import ExampleProviderData # Check that the number of rows matches your source data. total_entries = ExampleProviderData . objects . count () # View the data! # The [:100] limits this to your first 100 results my_table = ExampleProviderData . objects . to_dataframe ()[: 100 ] And that's it for writing new code! All that's left is making your data available for others.","title":"Step 2: Download data into the Simmate format"},{"location":"full_guides/database/contributing_data/#step-3-compress-the-data-to-archive-file","text":"This will be the easiest step yet. We need to make a zip file for users to download, which can be done in one line: ExampleProviderData . objects . to_archive () You'll find a file named ExampleProviderData-2022-01-25.zip (but with the current date) in your working directory. The date is for timestamp and versioning your archives. Because archives are a snapshot of databases that may be dynamically changing/going, this timestamp helps users know which version they are on. You can practice reloading this data into your database too: Make a copy of your database file in ~/simmate/ so you don't lose your work In the terminal, reset your database with simmate database reset In python, try reloading your data with ExampleProviderData.load_archive() Try viewing your data again with ExampleProviderData.objects.to_dataframe()","title":"Step 3: Compress the data to archive file"},{"location":"full_guides/database/contributing_data/#step-4-make-the-archive-available-via-a-cdn","text":"Users with now need the archive file you made to access your data. So you must decide: how should this zip file be downloaded by users? If you give Simmate approval, we can host your archive file on our own servers. Otherwise you must host your own. The only requirement for your host server is that the zip file can be downloaded from a URL. While we encourage open-source databases, if you consider your dataset private or commercial, Simmate does not require any payment or involvement for how this CDN is hosted and maintained. Thus, you can manage access to this URL via a subscription or any other method. However, Simmate's CDNs are reserved for archives that are freely distributed. Note: when uploading new versions of your archive, you should keep the outdated archive either available via its previous URL or, at a minimum, available upon request from users.","title":"Step 4: Make the archive available via a CDN"},{"location":"full_guides/database/contributing_data/#step-5-link-the-cdn-to-the-simmate-table","text":"In Step 1, we left one attribute as None in our code: remote_archive_link . As a final step, you need to take the URL that you're host your zip file at and paste it here. For example, that line will become: remote_archive_link = \"https://archives.simmate.org/ExampleProviderData-2022-01-25.zip\" That's it! Let's test out everything again. Note, we are now using load_remote_archive in this process -- which will load your zip file from the URL. Make a copy of your database file in ~/simmate/ so you don't lose your work In the terminal, reset your database with simmate database reset In python, try reloading your data with ExampleProviderData.load_remote_archive() Try viewing your data again with ExampleProviderData.objects.to_dataframe() If you've made it this far, thank you for contributing!!! Your data is now easily accessible to all Simmate users, which we hope facilitates its use and even lessen the load on your own servers. Congrats!","title":"Step 5: Link the CDN to the Simmate table"},{"location":"full_guides/database/custom_tables/","text":"Building custom database tables \u00b6 This module defines the fundamental building blocks for storing data. When building new and custom tables, you should inherit from one or more of these classes. The Base Data Types \u00b6 There are different \"levels\" of data types define here -- as some types inherit functionality from others. At the lowest level... base.DatabaseTable : all tables inherit from this one and it is where common functionality (like the show_columns method) is defined Next are a series of mixins defined in each of these modules... calculation : holds information about a flow run (corrections, timestamps, etc.) structure : holds a periodic crystal structure symmetry : NOT a mixin. Defines symmetry info for structure to reference forces : holds site forces and lattice stress information thermodynamics : holds energy and stability information density_of_states : holds results of a density of states calculation band_structure : holds results of a band structure calculation These mixins are frequently combined in for types of calculations. We define some of those common classes here too: static_energy : holds results of single point energy calculation relaxation : holds all steps of a structure geometry optimization nudged_elastic_band : holds all results from trajectory calculations dynamics : holds all steps of a molecular dynamics simmulation calculation_nested : a special type of calculation that involves running a workflow made of smaller workflows Building an example table \u00b6 Creating a custom table involves the following steps: defining your new table's inheritance and custom columns making sure your table is registerd to your database saving data to your new table All classes in this module are abstract and largely ment to be used as mix-ins. Each class will contain details on it's specific use, but when combining multiple types, you can do the following: from simmate.database.base_data_types import ( table_column , Structure , Thermodynamics , ) # Inherit from all the types you'd like to store data on. All of the columns # define in each of these types will be incorporated into your table. class MyCustomTable ( Structure , Thermodynamics ): # ----- Table columns ----- # Add any custom columns you'd like. # These follow the types supported by Django. # This custom field will be required and must be supplied at creation custom_column_01 = table_column . IntegerField () # This column we say is allowed to be empty. This is often needed if you # create an entry to start a calculation and then fill in data after a # it completes. custom_column_02 = table_column . FloatField ( null = True , blank = True ) # ----- Extra features ----- # If you are not using the `Calculation` mix-in, you'll have to specify # which app this table is associated with. To determine what you set here, # you should have completed the advanced simmate tutorials (08-09). class Meta : app_label = \"my_custom_app\" # (OPTIONAL) Define the \"raw data\" for your table. This is required if # you'd like to use the `to_archive` method. Fields from the mix-in # will automatically be added. archive_fields = [ \"custom_column_01\" , \"custom_column_02\" , ] # (OPTIONAL) Define how you would like data to be accessible in the REST # API from the website server. api_filters = { \"custom_column_01\" : [ \"range\" ], \"custom_column_02\" : [ \"range\" ], } Warning Unless you are contributing to Simmate's source code, defining a new table does NOT automatically register it to your database. To do this, you must follow along with our custom workflows guides . Loading data \u00b6 Once your table is created and registered, you can use the from_toolkit method to create and save your data to the database. Note, the information you pass to this method is entirely dependent on what you inherit from and define above. from my.example.project import MyCustomTable new_row = MyCustomTable . from_toolkit ( # Because we inherited from Structure, we must provide structure structure = new_structure , # provide a ToolkitStructure here # # All tables can optionally include a source too. source = \"made by jacksund\" , # # Because we inherited from Thermodynamics, we must provide energy energy =- 5.432 , # # Our custom fields can also be added custom_column_01 = 1234 , custom_column_02 = 3.14159 , ) new_row . save () Updating a column \u00b6 To modify a row, you can load it from your database, update the column, and then resave. Note, there are may more ways to do this, so consult the Django documentation for advanced usage. from my.example.project import MyCustomTable my_row = MyCustomTable . objects . get ( id = 1 ) my_row . custom_column_01 = 4321 my_row . save ()","title":"Creating custom tables"},{"location":"full_guides/database/custom_tables/#building-custom-database-tables","text":"This module defines the fundamental building blocks for storing data. When building new and custom tables, you should inherit from one or more of these classes.","title":"Building custom database tables"},{"location":"full_guides/database/custom_tables/#the-base-data-types","text":"There are different \"levels\" of data types define here -- as some types inherit functionality from others. At the lowest level... base.DatabaseTable : all tables inherit from this one and it is where common functionality (like the show_columns method) is defined Next are a series of mixins defined in each of these modules... calculation : holds information about a flow run (corrections, timestamps, etc.) structure : holds a periodic crystal structure symmetry : NOT a mixin. Defines symmetry info for structure to reference forces : holds site forces and lattice stress information thermodynamics : holds energy and stability information density_of_states : holds results of a density of states calculation band_structure : holds results of a band structure calculation These mixins are frequently combined in for types of calculations. We define some of those common classes here too: static_energy : holds results of single point energy calculation relaxation : holds all steps of a structure geometry optimization nudged_elastic_band : holds all results from trajectory calculations dynamics : holds all steps of a molecular dynamics simmulation calculation_nested : a special type of calculation that involves running a workflow made of smaller workflows","title":"The Base Data Types"},{"location":"full_guides/database/custom_tables/#building-an-example-table","text":"Creating a custom table involves the following steps: defining your new table's inheritance and custom columns making sure your table is registerd to your database saving data to your new table All classes in this module are abstract and largely ment to be used as mix-ins. Each class will contain details on it's specific use, but when combining multiple types, you can do the following: from simmate.database.base_data_types import ( table_column , Structure , Thermodynamics , ) # Inherit from all the types you'd like to store data on. All of the columns # define in each of these types will be incorporated into your table. class MyCustomTable ( Structure , Thermodynamics ): # ----- Table columns ----- # Add any custom columns you'd like. # These follow the types supported by Django. # This custom field will be required and must be supplied at creation custom_column_01 = table_column . IntegerField () # This column we say is allowed to be empty. This is often needed if you # create an entry to start a calculation and then fill in data after a # it completes. custom_column_02 = table_column . FloatField ( null = True , blank = True ) # ----- Extra features ----- # If you are not using the `Calculation` mix-in, you'll have to specify # which app this table is associated with. To determine what you set here, # you should have completed the advanced simmate tutorials (08-09). class Meta : app_label = \"my_custom_app\" # (OPTIONAL) Define the \"raw data\" for your table. This is required if # you'd like to use the `to_archive` method. Fields from the mix-in # will automatically be added. archive_fields = [ \"custom_column_01\" , \"custom_column_02\" , ] # (OPTIONAL) Define how you would like data to be accessible in the REST # API from the website server. api_filters = { \"custom_column_01\" : [ \"range\" ], \"custom_column_02\" : [ \"range\" ], } Warning Unless you are contributing to Simmate's source code, defining a new table does NOT automatically register it to your database. To do this, you must follow along with our custom workflows guides .","title":"Building an example table"},{"location":"full_guides/database/custom_tables/#loading-data","text":"Once your table is created and registered, you can use the from_toolkit method to create and save your data to the database. Note, the information you pass to this method is entirely dependent on what you inherit from and define above. from my.example.project import MyCustomTable new_row = MyCustomTable . from_toolkit ( # Because we inherited from Structure, we must provide structure structure = new_structure , # provide a ToolkitStructure here # # All tables can optionally include a source too. source = \"made by jacksund\" , # # Because we inherited from Thermodynamics, we must provide energy energy =- 5.432 , # # Our custom fields can also be added custom_column_01 = 1234 , custom_column_02 = 3.14159 , ) new_row . save ()","title":"Loading data"},{"location":"full_guides/database/custom_tables/#updating-a-column","text":"To modify a row, you can load it from your database, update the column, and then resave. Note, there are may more ways to do this, so consult the Django documentation for advanced usage. from my.example.project import MyCustomTable my_row = MyCustomTable . objects . get ( id = 1 ) my_row . custom_column_01 = 4321 my_row . save ()","title":"Updating a column"},{"location":"full_guides/database/notes/","text":"<< _register_calc >> from_run_context from_toolkit << _update_database_with_results >> from_run_context --> grabs from _register_calc update_database_from_results update_from_results update_from_toolkit from_toolkit(as_dict=True) update_from_directory from_directory(as_dict=True) from_vasp_directory(as_dict=True) ---> unexpected as_dict from_vasp_run(as_dict=True) update_from_toolkit() from_toolkit(as_dict=True) << load_completed_calc >> from_toolkit from_directory from_vasp_directory from_vasp_run","title":"Notes"},{"location":"full_guides/database/overview/","text":"The Simmate Database \u00b6 This module hosts everything for defining and interacting with your database. For beginners, make sure you have completed our database tutorial . Submodules include... base_data_types : fundamental mix-ins for creating new tables workflow_results : collection of result tables for simmate.workflows prototypes : tables of prototype structures third_parties : loads data from external providers (such as Materials Project) And there is one extra file in this module: connect : configures the database and installed apps (i.e. sets up Django)","title":"Overview"},{"location":"full_guides/database/overview/#the-simmate-database","text":"This module hosts everything for defining and interacting with your database. For beginners, make sure you have completed our database tutorial . Submodules include... base_data_types : fundamental mix-ins for creating new tables workflow_results : collection of result tables for simmate.workflows prototypes : tables of prototype structures third_parties : loads data from external providers (such as Materials Project) And there is one extra file in this module: connect : configures the database and installed apps (i.e. sets up Django)","title":"The Simmate Database"},{"location":"full_guides/database/third_party_data/","text":"Third-party database access \u00b6 This module downloads data from third-parties and stores it to your local database. This data is NOT from the Simmate team. These providers are independent groups, and you should cite them appropriately. All data from these providers remain under their source's terms and conditions. Currently, we support the following providers: COD (Crystallography Open Database) JARVIS (Joint Automated Repository for Various Integrated Simulations) Materials Project OQMD (Open Quantum Materials Database) These providers are configured, but our team is waiting for permission to redistribute their data: AFLOW (Automatic FLOW for Materials Discovery) Tip If your team would like to make your own data available via Simmate, please see the Contributing data module. Even if its is a single table, don't hesistate to make a contribution! We outline the benefits of contributing and how to package your data within the for_providers module. Downloading data \u00b6 Make sure you have completed our introductory tutorial for downloading data from these providers. Below we show example usage with MatprojStructure , but the same process can be done with all other tables in this module. WARNING: The first time you load archives of data, it can take a long time, so we recommend running some things overnight. Once completed, we also recommend backing up your database (by making a copy of your ~/simmate/my_env-database.sqlite3 file). This ensures you don't have to repeat this long process. To download all data into your database: simmate database load-remote-archives Or in python, you can download a specific table: from simmate.database.third_parties import MatprojStructure # This can take >1 hour for some providers. Optionally, you can # add `parallel=True` to speed up this process, but use caution when # parallelizing with SQLite (the default backend). We recommend # avoiding the use of parallel=True, and instead running # this line overnight. MatprojStructure . load_remote_archive () # If you use this providers data, be sure to cite them! MatprojStructure . source_doi Populating energy fields \u00b6 Some database providers give a calculated energy, which can be used to populate stability information: # updates ALL chemical systems. # Note, this can take over an hour for some providers. Try running # this overnight along with your call to load_remote_archive. MatprojStructure . update_all_stabilities () # updates ONE chemical system # This can be used if you quickly want to update a specific system MatprojStructure . update_chemical_system_stabilities ( \"Y-C-F\" ) Alternatives \u00b6 This module can be viewed as an alternative to and/or an extension of the following codes: MPContribs matminer.data_retrieval pymatgen.ext OPTIMADE APIs This module stores data locally and then allows rapidly loading data to memory, whereas alternatives involve querying external APIs and loading data into memory. We choose to store data locally because it allows stability (i.e. no breaking changes in your source data) and fast loading accross python sessions. This is particullary useful for high-throughput studies.","title":"Third-party data"},{"location":"full_guides/database/third_party_data/#third-party-database-access","text":"This module downloads data from third-parties and stores it to your local database. This data is NOT from the Simmate team. These providers are independent groups, and you should cite them appropriately. All data from these providers remain under their source's terms and conditions. Currently, we support the following providers: COD (Crystallography Open Database) JARVIS (Joint Automated Repository for Various Integrated Simulations) Materials Project OQMD (Open Quantum Materials Database) These providers are configured, but our team is waiting for permission to redistribute their data: AFLOW (Automatic FLOW for Materials Discovery) Tip If your team would like to make your own data available via Simmate, please see the Contributing data module. Even if its is a single table, don't hesistate to make a contribution! We outline the benefits of contributing and how to package your data within the for_providers module.","title":"Third-party database access"},{"location":"full_guides/database/third_party_data/#downloading-data","text":"Make sure you have completed our introductory tutorial for downloading data from these providers. Below we show example usage with MatprojStructure , but the same process can be done with all other tables in this module. WARNING: The first time you load archives of data, it can take a long time, so we recommend running some things overnight. Once completed, we also recommend backing up your database (by making a copy of your ~/simmate/my_env-database.sqlite3 file). This ensures you don't have to repeat this long process. To download all data into your database: simmate database load-remote-archives Or in python, you can download a specific table: from simmate.database.third_parties import MatprojStructure # This can take >1 hour for some providers. Optionally, you can # add `parallel=True` to speed up this process, but use caution when # parallelizing with SQLite (the default backend). We recommend # avoiding the use of parallel=True, and instead running # this line overnight. MatprojStructure . load_remote_archive () # If you use this providers data, be sure to cite them! MatprojStructure . source_doi","title":"Downloading data"},{"location":"full_guides/database/third_party_data/#populating-energy-fields","text":"Some database providers give a calculated energy, which can be used to populate stability information: # updates ALL chemical systems. # Note, this can take over an hour for some providers. Try running # this overnight along with your call to load_remote_archive. MatprojStructure . update_all_stabilities () # updates ONE chemical system # This can be used if you quickly want to update a specific system MatprojStructure . update_chemical_system_stabilities ( \"Y-C-F\" )","title":"Populating energy fields"},{"location":"full_guides/database/third_party_data/#alternatives","text":"This module can be viewed as an alternative to and/or an extension of the following codes: MPContribs matminer.data_retrieval pymatgen.ext OPTIMADE APIs This module stores data locally and then allows rapidly loading data to memory, whereas alternatives involve querying external APIs and loading data into memory. We choose to store data locally because it allows stability (i.e. no breaking changes in your source data) and fast loading accross python sessions. This is particullary useful for high-throughput studies.","title":"Alternatives"},{"location":"full_guides/database/workflow_data/","text":"Accessing workflow data \u00b6 Much like the simmate.workflows module, this module brings together all database tables that are linked to workflows and organizes them by application for convenience. Loading results \u00b6 The getting-started tutorials will teach you how to run workflows and access their results. But as a review: from simmate.workflows.static_energy import mit_workflow # runs the workflow and returns a status status = mit_workflow . run ( structure =... ) # gives the DatabaseTable where ALL results are stored mit_workflow . database_table You can also access a table directly with... # connects to the database from simmate.database import connect from simmate.database.workflow_results import MITStaticEnergy # NOTE: MITStaticEnergy here is the exact same as database_table in the codeblock # above this one. These are just two different ways of accessing it. MITStaticEnergy . objects . filter ( ... ) Location of Each Table's Source-code \u00b6 The code that defines these tables are located in the corresponding simmate.calculators module. We make tables accessible here because users often want to search for results by application -- not by their calculator name. For example, the results from all static energy calculations that use VASP under MIT project settings can be imported with... from simmate.database.workflow_results import MITStaticEnergy Alternatively, this same database table could have been imported with... from simmate.calculators.vasp.database.energy import MITStaticEnergy","title":"Workflow data"},{"location":"full_guides/database/workflow_data/#accessing-workflow-data","text":"Much like the simmate.workflows module, this module brings together all database tables that are linked to workflows and organizes them by application for convenience.","title":"Accessing workflow data"},{"location":"full_guides/database/workflow_data/#loading-results","text":"The getting-started tutorials will teach you how to run workflows and access their results. But as a review: from simmate.workflows.static_energy import mit_workflow # runs the workflow and returns a status status = mit_workflow . run ( structure =... ) # gives the DatabaseTable where ALL results are stored mit_workflow . database_table You can also access a table directly with... # connects to the database from simmate.database import connect from simmate.database.workflow_results import MITStaticEnergy # NOTE: MITStaticEnergy here is the exact same as database_table in the codeblock # above this one. These are just two different ways of accessing it. MITStaticEnergy . objects . filter ( ... )","title":"Loading results"},{"location":"full_guides/database/workflow_data/#location-of-each-tables-source-code","text":"The code that defines these tables are located in the corresponding simmate.calculators module. We make tables accessible here because users often want to search for results by application -- not by their calculator name. For example, the results from all static energy calculations that use VASP under MIT project settings can be imported with... from simmate.database.workflow_results import MITStaticEnergy Alternatively, this same database table could have been imported with... from simmate.calculators.vasp.database.energy import MITStaticEnergy","title":"Location of Each Table's Source-code"},{"location":"full_guides/extras/command_line/","text":"The Simmate Command-line Interface \u00b6 This module defines the simmate command and all of it's sub-commands. Note, nearly all of the commands in this module wrap a lower-level function, so little code is located here. For example, the simmate database reset command is just a wrapper for the following python code: from simmate.database import connect from simmate.database.utilities import reset_database reset_database () Our command-line is build using Click instead of Argparse. Be sure to read their documentation before contributing to this module. WARNING : for interacting with and using the command-line, we recommend using it directly, rather than exploring online documentation here. This is because we use typer , which make reading through options much cleaner than api docs shown here.","title":"Command line"},{"location":"full_guides/extras/command_line/#the-simmate-command-line-interface","text":"This module defines the simmate command and all of it's sub-commands. Note, nearly all of the commands in this module wrap a lower-level function, so little code is located here. For example, the simmate database reset command is just a wrapper for the following python code: from simmate.database import connect from simmate.database.utilities import reset_database reset_database () Our command-line is build using Click instead of Argparse. Be sure to read their documentation before contributing to this module. WARNING : for interacting with and using the command-line, we recommend using it directly, rather than exploring online documentation here. This is because we use typer , which make reading through options much cleaner than api docs shown here.","title":"The Simmate Command-line Interface"},{"location":"full_guides/extras/utilities/","text":"This module hosts common functions that are used throughout Simmate. This includes things like grabbing the name of the active conda environment, creating a new directory, and compressing a folder to a zip file. Typically, you don't need to interact with any of the functions in this module unless you are creating new functionality or contributing to our code.","title":"Utilities"},{"location":"full_guides/extras/visualization/","text":"Simmate Visualization \u00b6 This module helpd create plots and 3D models for visualizing data and structures. This ment to be an extension of the pymatgen.ext module. NOTE: Currently, this module only creates 3D models for Structure objects usings Blender , whereas other functionality is still under planning/development. Until these tools are more mature, we recommend using either VESTA or OVITO to visualize your crystal structures.","title":"Visualization"},{"location":"full_guides/extras/visualization/#simmate-visualization","text":"This module helpd create plots and 3D models for visualizing data and structures. This ment to be an extension of the pymatgen.ext module. NOTE: Currently, this module only creates 3D models for Structure objects usings Blender , whereas other functionality is still under planning/development. Until these tools are more mature, we recommend using either VESTA or OVITO to visualize your crystal structures.","title":"Simmate Visualization"},{"location":"full_guides/extras/file_converters/overview/","text":"File Converters \u00b6 This module hosts functionality for reading and writing different file types. For example, there are many ways to represent a crystal structure -- such as the POSCAR, CASTEP, and CIF file formats -- and it's therefore important to have functions to convert between the them. Beyond file types, this module also include functions for converting between object types (i.e. different python classes). NOTE: Currently, this module only hosts converters for structure , whereas the molecule and voxeldata types are still under planning/development.","title":"Overview"},{"location":"full_guides/extras/file_converters/overview/#file-converters","text":"This module hosts functionality for reading and writing different file types. For example, there are many ways to represent a crystal structure -- such as the POSCAR, CASTEP, and CIF file formats -- and it's therefore important to have functions to convert between the them. Beyond file types, this module also include functions for converting between object types (i.e. different python classes). NOTE: Currently, this module only hosts converters for structure , whereas the molecule and voxeldata types are still under planning/development.","title":"File Converters"},{"location":"full_guides/extras/file_converters/structures/","text":"Structure Converters \u00b6 This module hosts converter classes for common structure file and object formats. All converters are linked directly for conversion into the simmate.toolkit.base_data_types.structure.Structure class. Note, in the majority of cases, you can have Simmate attempt to figure out the file/object format you have. Using these converters directly is really only needed for advanced use or speed optimization: Example dynamic use: from simmate.toolkit import Structure structure1 = Structure . from_dynamic ( \"example.cif\" ) structure2 = Structure . from_dynamic ( \"POSCAR\" ) structure3 = Structure . from_dynamic ( { \"database_table\" : \"MITStaticEnergy\" , \"database_id\" : 1 } ) If you'd like to convert between formats (such as CIF --> POSCAR), you should treat this a two-step process: from simmate.toolkit import Structure # STEP 1: convert to simmate structure = Structure . from_dynamic ( \"example.cif\" ) # STEP 2: convert to desired format structure . to ( fmt = \"poscar\" , filename = \"POSCAR\" ) Other Converters \u00b6 This module does not host all file-converters that Simmate has. Others can be found in the calculators module, where they are associated with a specific program. For example, the converter for POSCAR files is directly from the VASP software -- therefore, you can find the POSCAR converter in the vasp.inputs.poscar module. Here is a list of other structure converters for reference: POSCAR ( simmate.calculators.vasp.inputs.poscar )","title":"Structures"},{"location":"full_guides/extras/file_converters/structures/#structure-converters","text":"This module hosts converter classes for common structure file and object formats. All converters are linked directly for conversion into the simmate.toolkit.base_data_types.structure.Structure class. Note, in the majority of cases, you can have Simmate attempt to figure out the file/object format you have. Using these converters directly is really only needed for advanced use or speed optimization: Example dynamic use: from simmate.toolkit import Structure structure1 = Structure . from_dynamic ( \"example.cif\" ) structure2 = Structure . from_dynamic ( \"POSCAR\" ) structure3 = Structure . from_dynamic ( { \"database_table\" : \"MITStaticEnergy\" , \"database_id\" : 1 } ) If you'd like to convert between formats (such as CIF --> POSCAR), you should treat this a two-step process: from simmate.toolkit import Structure # STEP 1: convert to simmate structure = Structure . from_dynamic ( \"example.cif\" ) # STEP 2: convert to desired format structure . to ( fmt = \"poscar\" , filename = \"POSCAR\" )","title":"Structure Converters"},{"location":"full_guides/extras/file_converters/structures/#other-converters","text":"This module does not host all file-converters that Simmate has. Others can be found in the calculators module, where they are associated with a specific program. For example, the converter for POSCAR files is directly from the VASP software -- therefore, you can find the POSCAR converter in the vasp.inputs.poscar module. Here is a list of other structure converters for reference: POSCAR ( simmate.calculators.vasp.inputs.poscar )","title":"Other Converters"},{"location":"full_guides/extras/visualization/blender/","text":"Blender setup \u00b6 I spent a significant amount of trying to get blender working as a python module, but I don't know enough about make files to package this for general use. For now, it's actually easier to just install Blender manually and have Simmate call it from the command line. It's extra work for the user who will just want to vizualize structures and never use Blender directly, but that's just how it is right now... In the future, I may want to just pay a Blender dev to begin supporting blender as bpy module -- even if its a minimal build specifically for Simmate. Past notes on making bpy from source \u00b6 Making my own blender bpy module (on Ubuntu 18.04) \u00b6 Following directions from https://wiki.blender.org/wiki/Building_Blender 1. First we need to install all dependencies used to build the final package sudo apt install git; sudo apt install build-essential; snap install cmake --classic; mkdir ~/blender-git; cd ~/blender-git; git clone https://git.blender.org/blender.git; cd blender; git submodule update --init --recursive; git submodule foreach git checkout master; git submodule foreach git pull --rebase origin master; cd ~/blender-git/blender/build_files/build_environment; ./install_deps.sh; 2. In the file bpy_module.cmake, we need to change the portable setting to 'ON': nano /home/jacksund/blender-git/blender/build_files/cmake/config/bpy_module.cmake; manually edit this line: set(WITH_INSTALL_PORTABLE ON CACHE BOOL \"\" FORCE) 3. With our settings and dependencies all set up, we can now build blender as a python module. The result will be in /blender-git/build_linux_bpy/bin/ cd ~/blender-git/blender; make bpy; 4. Lastly, let's copy the created module into our Anaconda enviornment and remove all these build files. cp -r /home/jacksund/blender-git/build_linux_bpy/bin/* /home/jacksund/anaconda3/envs/jacks_env/lib/python3.7/site-packages/; sudo rm -r /home/jacksund/blender-git; 5. You can now 'import bpy' in your python enviornment! Making my own blender bpy module (on Ubuntu 19.10) \u00b6 Following directions from https://wiki.blender.org/wiki/Building_Blender/Linux/Ubuntu and also troubleshooting with https://devtalk.blender.org/t/problem-with-running-blender-as-a-python-module/7367 1. First we need to install all dependencies used to build the final package sudo apt-get update; sudo apt-get install build-essential git subversion cmake libx11-dev libxxf86vm-dev libxcursor-dev libxi-dev libxrandr-dev libxinerama-dev; mkdir ~/blender-git; cd ~/blender-git; git clone http://git.blender.org/blender.git; cd ~/blender-git/blender; make update; cd ~/blender-git ./blender/build_files/build_environment/install_deps.sh --with-all 2. In the file bpy_module.cmake, we need to change the portable setting to 'ON': nano /home/jacksund/blender-git/blender/build_files/cmake/config/bpy_module.cmake; manually edit this line: set(WITH_INSTALL_PORTABLE ON CACHE BOOL \"\" FORCE) and add these lines: set(WITH_MEM_JEMALLOC OFF CACHE BOOL \"\" FORCE) set(WITH_MOD_OCEANSIM OFF CACHE BOOL \"\" FORCE) set(FFMPEG_LIBRARIES avformat;avcodec;avutil;avdevice;swscale;swresample;lzma;rt;theora;theoradec;theoraenc;vorbis;vorbisenc;vorbisfile;ogg;xvidcore;vpx;opus;mp3lame;x264;openjp2 CACHE STRING \"\" FORCE) set(PYTHON_VERSION 3.7 CACHE STRING \"\" FORCE) set(LLVM_VERSION 6.0 CACHE STRING \"\" FORCE) set(OPENCOLORIO_ROOT_DIR /opt/lib/ocio CACHE STRING \"\" FORCE) set(OPENIMAGEIO_ROOT_DIR /opt/lib/oiio CACHE STRING \"\" FORCE) set(OSL_ROOT_DIR /opt/lib/osl CACHE STRING \"\" FORCE) set(OPENSUBDIV_ROOT_DIR /opt/lib/osd CACHE STRING \"\" FORCE) set(OPENCOLLADA_ROOT_DIR /opt/lib/opencollada CACHE STRING \"\" FORCE) set(EMBREE_ROOT_DIR /opt/lib/embree CACHE STRING \"\" FORCE) set(OPENIMAGEDENOISE_ROOT_DIR /opt/lib/oidn CACHE STRING \"\" FORCE) set(ALEMBIC_ROOT_DIR /opt/lib/alembic CACHE STRING \"\" FORCE) 3. With our settings and dependencies all set up, we can now build blender as a python module. The result will be in /blender-git/build_linux_bpy/bin/ cd ~/blender-git/blender; make bpy; 4. Lastly, let's copy the created module into our Anaconda environment and remove all these build files. cp -r /home/jacksund/blender-git/build_linux_bpy/bin/* /home/jacksund/anaconda3/envs/jacks_env/lib/python3.7/site-packages/; sudo rm -r /home/jacksund/blender-git; 5. You can now 'import bpy' in your python environment!","title":"Structures"},{"location":"full_guides/extras/visualization/blender/#blender-setup","text":"I spent a significant amount of trying to get blender working as a python module, but I don't know enough about make files to package this for general use. For now, it's actually easier to just install Blender manually and have Simmate call it from the command line. It's extra work for the user who will just want to vizualize structures and never use Blender directly, but that's just how it is right now... In the future, I may want to just pay a Blender dev to begin supporting blender as bpy module -- even if its a minimal build specifically for Simmate.","title":"Blender setup"},{"location":"full_guides/extras/visualization/blender/#past-notes-on-making-bpy-from-source","text":"","title":"Past notes on making bpy from source"},{"location":"full_guides/extras/visualization/blender/#making-my-own-blender-bpy-module-on-ubuntu-1804","text":"Following directions from https://wiki.blender.org/wiki/Building_Blender 1. First we need to install all dependencies used to build the final package sudo apt install git; sudo apt install build-essential; snap install cmake --classic; mkdir ~/blender-git; cd ~/blender-git; git clone https://git.blender.org/blender.git; cd blender; git submodule update --init --recursive; git submodule foreach git checkout master; git submodule foreach git pull --rebase origin master; cd ~/blender-git/blender/build_files/build_environment; ./install_deps.sh; 2. In the file bpy_module.cmake, we need to change the portable setting to 'ON': nano /home/jacksund/blender-git/blender/build_files/cmake/config/bpy_module.cmake; manually edit this line: set(WITH_INSTALL_PORTABLE ON CACHE BOOL \"\" FORCE) 3. With our settings and dependencies all set up, we can now build blender as a python module. The result will be in /blender-git/build_linux_bpy/bin/ cd ~/blender-git/blender; make bpy; 4. Lastly, let's copy the created module into our Anaconda enviornment and remove all these build files. cp -r /home/jacksund/blender-git/build_linux_bpy/bin/* /home/jacksund/anaconda3/envs/jacks_env/lib/python3.7/site-packages/; sudo rm -r /home/jacksund/blender-git; 5. You can now 'import bpy' in your python enviornment!","title":"Making my own blender bpy module (on Ubuntu 18.04)"},{"location":"full_guides/extras/visualization/blender/#making-my-own-blender-bpy-module-on-ubuntu-1910","text":"Following directions from https://wiki.blender.org/wiki/Building_Blender/Linux/Ubuntu and also troubleshooting with https://devtalk.blender.org/t/problem-with-running-blender-as-a-python-module/7367 1. First we need to install all dependencies used to build the final package sudo apt-get update; sudo apt-get install build-essential git subversion cmake libx11-dev libxxf86vm-dev libxcursor-dev libxi-dev libxrandr-dev libxinerama-dev; mkdir ~/blender-git; cd ~/blender-git; git clone http://git.blender.org/blender.git; cd ~/blender-git/blender; make update; cd ~/blender-git ./blender/build_files/build_environment/install_deps.sh --with-all 2. In the file bpy_module.cmake, we need to change the portable setting to 'ON': nano /home/jacksund/blender-git/blender/build_files/cmake/config/bpy_module.cmake; manually edit this line: set(WITH_INSTALL_PORTABLE ON CACHE BOOL \"\" FORCE) and add these lines: set(WITH_MEM_JEMALLOC OFF CACHE BOOL \"\" FORCE) set(WITH_MOD_OCEANSIM OFF CACHE BOOL \"\" FORCE) set(FFMPEG_LIBRARIES avformat;avcodec;avutil;avdevice;swscale;swresample;lzma;rt;theora;theoradec;theoraenc;vorbis;vorbisenc;vorbisfile;ogg;xvidcore;vpx;opus;mp3lame;x264;openjp2 CACHE STRING \"\" FORCE) set(PYTHON_VERSION 3.7 CACHE STRING \"\" FORCE) set(LLVM_VERSION 6.0 CACHE STRING \"\" FORCE) set(OPENCOLORIO_ROOT_DIR /opt/lib/ocio CACHE STRING \"\" FORCE) set(OPENIMAGEIO_ROOT_DIR /opt/lib/oiio CACHE STRING \"\" FORCE) set(OSL_ROOT_DIR /opt/lib/osl CACHE STRING \"\" FORCE) set(OPENSUBDIV_ROOT_DIR /opt/lib/osd CACHE STRING \"\" FORCE) set(OPENCOLLADA_ROOT_DIR /opt/lib/opencollada CACHE STRING \"\" FORCE) set(EMBREE_ROOT_DIR /opt/lib/embree CACHE STRING \"\" FORCE) set(OPENIMAGEDENOISE_ROOT_DIR /opt/lib/oidn CACHE STRING \"\" FORCE) set(ALEMBIC_ROOT_DIR /opt/lib/alembic CACHE STRING \"\" FORCE) 3. With our settings and dependencies all set up, we can now build blender as a python module. The result will be in /blender-git/build_linux_bpy/bin/ cd ~/blender-git/blender; make bpy; 4. Lastly, let's copy the created module into our Anaconda environment and remove all these build files. cp -r /home/jacksund/blender-git/build_linux_bpy/bin/* /home/jacksund/anaconda3/envs/jacks_env/lib/python3.7/site-packages/; sudo rm -r /home/jacksund/blender-git; 5. You can now 'import bpy' in your python environment!","title":"Making my own blender bpy module (on Ubuntu 19.10)"},{"location":"full_guides/extras/visualization/overview/","text":"Simmate Visualization \u00b6 This module helpd create plots and 3D models for visualizing data and structures. This ment to be an extension of the pymatgen.ext module. NOTE: Currently, this module only creates 3D models for Structure objects usings Blender , whereas other functionality is still under planning/development. Until these tools are more mature, we recommend using either VESTA or OVITO to visualize your crystal structures.","title":"Overview"},{"location":"full_guides/extras/visualization/overview/#simmate-visualization","text":"This module helpd create plots and 3D models for visualizing data and structures. This ment to be an extension of the pymatgen.ext module. NOTE: Currently, this module only creates 3D models for Structure objects usings Blender , whereas other functionality is still under planning/development. Until these tools are more mature, we recommend using either VESTA or OVITO to visualize your crystal structures.","title":"Simmate Visualization"},{"location":"full_guides/website/overview/","text":"The Simmate Website \u00b6 The simmate.website module hosts everything for the website interface and API. Unlike other major projects, Simmate makes our website's source-code openly available to everyone! This means that you can host your own Simmate website on your own computer or even in production for your lab to privately use. Running a server locally \u00b6 You can quickly get Simmate up-and-running on your local computer using the command... command line # set up your database if you haven't done so already simmate database reset # now run the website locally! simmate run-server While this command is running, open up your preferred browser (Chrome, Firefox, etc.) and go to http://127.0.0.1:8000/ . While this looks just like our actaul website at simmate.org , this one is running locally on your desktop and using your personal database! Tip Beyond a local server, beginners don't need anything else from the website interface documentation. Feel free to skip to the \"Workflows\" guides. Running a production-ready server \u00b6 To have everything set up for your team, you can do one of the following... ask about collaborating with the Simmate team and join our server ask about having our team manage a server for you set up and manage your own server For options 1 and 2, just send us an email at simmate.team@gmail.com . For option 3, we have a guide for setting a server up on DigitalOcean . In addition to this guide, make sure you complete the base simmate tutorials -- and pay particular attention to the tutorials on setting up a cloud database and setting up computational resources . Third-party sign ins \u00b6 Simmate supports signing into the website via third-party accounts such as Google and Github. This functionality is thanks to the django-allauth package. By default, servers will not display these sign-in buttons, so if you wish to configure logins for third-party accounts, you must do this manually. While there are many types of accounts that can be used with django-allauth (see their full list ), Simmate only supports Github and Google at the moment. We give guides on how to set these up below. Github OAuth \u00b6 Create a new OAuth application with this link and the following information (note we are using http://127.0.0.1:8000 , which is your local test server. Replace this with the link to your cloud server if it's available.): application name = My New Simmate Server (edit if you'd like) homepage url = http://127.0.0.1:8000 authorization callback url = http://127.0.0.1:8000/accounts/github/login/callback/ On the next page, select \"Generate a new client secret\" and copy this value to your clipboard. On your local computer (or production-ready server), set the environment variables: GITHUB_SECRET = examplekey1234 (value is what you copied from step 2) GITHUB_CLIENT_ID = exampleid1234 (value is listed on github as \"Client ID\") Google OAuth \u00b6 Follow steps from django-allauth ( here ) to configure the Google API application On your local computer (or production-ready server), set the environment variables: GOOGLE_SECRET = examplekey1234 GOOGLE_CLIENT_ID = exampleid1234 CSS and JS assets \u00b6 Simmate does not distribute the majority of source CSS and JavaScript files because we use assets from a third-party vendor and redistribution is not allowed under their licensing. Specifically, we use the Hyper theme from the CoderThemes team . Using the subpages and guides available on their Modern Dashboard template , you'll be able to contribute to Simmate's website without needing to access any of the assets. Within our templates, you'll note we always load assets from a Simmate CDN: <!-- How an asset is normally loaded when distributed with source code --> < link href = \"assets/css/vendor/fullcalendar.min.css\" rel = \"stylesheet\" type = \"text/css\" /> <!-- How assets are loaded for Simmate using our CDN --> < link href = \"https://archives.simmate.org/assets/fullcalendar.min.css\" rel = \"stylesheet\" type = \"text/css\" /> If you ever need to alter the CSS or JS, please reach out to our team so we can discuss the best way to approach this.","title":"Overview"},{"location":"full_guides/website/overview/#the-simmate-website","text":"The simmate.website module hosts everything for the website interface and API. Unlike other major projects, Simmate makes our website's source-code openly available to everyone! This means that you can host your own Simmate website on your own computer or even in production for your lab to privately use.","title":"The Simmate Website"},{"location":"full_guides/website/overview/#running-a-server-locally","text":"You can quickly get Simmate up-and-running on your local computer using the command... command line # set up your database if you haven't done so already simmate database reset # now run the website locally! simmate run-server While this command is running, open up your preferred browser (Chrome, Firefox, etc.) and go to http://127.0.0.1:8000/ . While this looks just like our actaul website at simmate.org , this one is running locally on your desktop and using your personal database! Tip Beyond a local server, beginners don't need anything else from the website interface documentation. Feel free to skip to the \"Workflows\" guides.","title":"Running a server locally"},{"location":"full_guides/website/overview/#running-a-production-ready-server","text":"To have everything set up for your team, you can do one of the following... ask about collaborating with the Simmate team and join our server ask about having our team manage a server for you set up and manage your own server For options 1 and 2, just send us an email at simmate.team@gmail.com . For option 3, we have a guide for setting a server up on DigitalOcean . In addition to this guide, make sure you complete the base simmate tutorials -- and pay particular attention to the tutorials on setting up a cloud database and setting up computational resources .","title":"Running a production-ready server"},{"location":"full_guides/website/overview/#third-party-sign-ins","text":"Simmate supports signing into the website via third-party accounts such as Google and Github. This functionality is thanks to the django-allauth package. By default, servers will not display these sign-in buttons, so if you wish to configure logins for third-party accounts, you must do this manually. While there are many types of accounts that can be used with django-allauth (see their full list ), Simmate only supports Github and Google at the moment. We give guides on how to set these up below.","title":"Third-party sign ins"},{"location":"full_guides/website/overview/#github-oauth","text":"Create a new OAuth application with this link and the following information (note we are using http://127.0.0.1:8000 , which is your local test server. Replace this with the link to your cloud server if it's available.): application name = My New Simmate Server (edit if you'd like) homepage url = http://127.0.0.1:8000 authorization callback url = http://127.0.0.1:8000/accounts/github/login/callback/ On the next page, select \"Generate a new client secret\" and copy this value to your clipboard. On your local computer (or production-ready server), set the environment variables: GITHUB_SECRET = examplekey1234 (value is what you copied from step 2) GITHUB_CLIENT_ID = exampleid1234 (value is listed on github as \"Client ID\")","title":"Github OAuth"},{"location":"full_guides/website/overview/#google-oauth","text":"Follow steps from django-allauth ( here ) to configure the Google API application On your local computer (or production-ready server), set the environment variables: GOOGLE_SECRET = examplekey1234 GOOGLE_CLIENT_ID = exampleid1234","title":"Google OAuth"},{"location":"full_guides/website/overview/#css-and-js-assets","text":"Simmate does not distribute the majority of source CSS and JavaScript files because we use assets from a third-party vendor and redistribution is not allowed under their licensing. Specifically, we use the Hyper theme from the CoderThemes team . Using the subpages and guides available on their Modern Dashboard template , you'll be able to contribute to Simmate's website without needing to access any of the assets. Within our templates, you'll note we always load assets from a Simmate CDN: <!-- How an asset is normally loaded when distributed with source code --> < link href = \"assets/css/vendor/fullcalendar.min.css\" rel = \"stylesheet\" type = \"text/css\" /> <!-- How assets are loaded for Simmate using our CDN --> < link href = \"https://archives.simmate.org/assets/fullcalendar.min.css\" rel = \"stylesheet\" type = \"text/css\" /> If you ever need to alter the CSS or JS, please reach out to our team so we can discuss the best way to approach this.","title":"CSS and JS assets"},{"location":"full_guides/website/rest_api/","text":"The REST API \u00b6 Warning This section is only for experts! If you are trying to pull data from Simmate, then you should instead use our python client that is introduced in the database guides . Grabbing data directly from our REST API is really only for teams that can't use python or Simmate's code but still want to pull data. Also note that grabbing data via our REST API is heavily throttled, so this is not a good way to grab large amounts of data. While the accronym may not be super intuitive for beginners, \"REST API\" stands for \" Re presentational s tate t ransfer (REST) A pplication P rogramming I nterfaces (API)\". In simple terms, this is how we can access databases from a website url. Our example endpoint \u00b6 The easiest way to understand our API is with some examples. In the examples below, we only look at the Materials Project database (at /third-parties/MatprojStructure/ ), but in addition to this example, nearly every URL within Simmate has REST API functionality! Our API endpoints are automatically built from our database module -- so it's very easy to implement new features and add new tables. Accessing the API \u00b6 Let's start with a normal URL and webpage: http://simmate.org/third-parties/MatprojStructure/ When you open that link, you are brought to a webpage that let's you search through all Materials Project structures. Under the hood, this URL is actually a REST API too! All we have to do is add ?format=api to the end of the URL. Try opening this webpage: http://simmate.org/third-parties/MatprojStructure/?format=api Likewise, if we use ?format=json , we can get our data back as a JSON dictionary: http://simmate.org/third-parties/MatprojStructure/?format=json The same can be done for individual entries too! For example, if we wanted all the data for the structure with id mp-1 , then we can do... http://simmate.org/third-parties/MatprojStructure/mp-1/?format=api http://simmate.org/third-parties/MatprojStructure/mp-1/?format=json For these, you should see an output similar too... { \"id\" : \"mp-1\" , \"structure\" : \"...(hidden for clarity)\" , \"nsites\" : 1 , \"nelements\" : 1 , \"elements\" : [ \"Cs\" ], \"chemical_system\" : \"Cs\" , \"density\" : 1.9350390306525629 , \"density_atomic\" : 0.00876794537479071 , \"volume\" : 114.05180544066401 , \"volume_molar\" : 68.68360262958124 , \"formula_full\" : \"Cs1\" , \"formula_reduced\" : \"Cs\" , \"formula_anonymous\" : \"A\" , \"energy\" : -0.85663276 , \"energy_per_atom\" : -0.85663276 , \"energy_above_hull\" : null , \"is_stable\" : null , \"decomposes_to\" : null , \"formation_energy\" : null , \"formation_energy_per_atom\" : null , \"spacegroup\" : 229 , } This is super useful because we can grab data from any programming language we'd like -- python, javascript, c++, fortran, or even the command-line. The only requirement to use our REST API is that you have access to the internet! Once you load your desired data, what you do with the JSON output is up to you. Filtering results \u00b6 Our URLs also support complex filtering too. As an example, let's make a search where we want all structures that have the spacegroup 229 and also are in the Cr-N chemcial system. When you make this search in the normal webpage, you'll notice the URL becomes... http://simmate.org/third-parties/MatprojStructure/?chemical_system=Cr-N&spacegroup__number=229 We specify our conditions by adding a question mark ( ? ) at the end of the URL and then adding example_key=desired_value after that. As we add new conditions, we separate them with & -- which results in key1=value1&key2=value2&key3=value3 and so on. You can also add format=api at the end of this too! Note, our python client for accessing data is MUCH more powerful for filtering through results, so we recommend accessing data using the simmate.database module in complex/advanced cases. Paginating results \u00b6 To protect our servers from overuse, Simmate currently returns a maximum of 12 results at a time. Pagination is handled automitically using the page=... keyword in the URL. In the HTML, API, and JSON views, you should always have the link to the next page of results available. For example in the JSON view, the returned data includes next and previous URLs. Ordering results \u00b6 For API and JSON formats, you can manually set the ordering of returned data by adding ordering=example_column to your URL. You can also reverse the ordering with ordering=-example_column (note the \" - \" symbol before the column name). For example: http://simmate.org/third-parties/MatprojStructure/?ordering=density_atomic The API for experts \u00b6 We build Simmate's REST API using the django-rest-framework python package, and implement filtering using django-filter . Our endpoints are NOT fixed but are instead dynamically created for each request. This is thanks to our SimmateAPIViewset class in the simmate.website.core_components module, which takes a Simmate database table and automatically renders an API endpoint for us. The backend implementation of dynamic APIs is still experimental because we are exploring the pros and cons the approach -- for example, this enables quick start-up times for configuring Django, but also comes at the cost of more CPU time per web request.","title":"REST API"},{"location":"full_guides/website/rest_api/#the-rest-api","text":"Warning This section is only for experts! If you are trying to pull data from Simmate, then you should instead use our python client that is introduced in the database guides . Grabbing data directly from our REST API is really only for teams that can't use python or Simmate's code but still want to pull data. Also note that grabbing data via our REST API is heavily throttled, so this is not a good way to grab large amounts of data. While the accronym may not be super intuitive for beginners, \"REST API\" stands for \" Re presentational s tate t ransfer (REST) A pplication P rogramming I nterfaces (API)\". In simple terms, this is how we can access databases from a website url.","title":"The REST API"},{"location":"full_guides/website/rest_api/#our-example-endpoint","text":"The easiest way to understand our API is with some examples. In the examples below, we only look at the Materials Project database (at /third-parties/MatprojStructure/ ), but in addition to this example, nearly every URL within Simmate has REST API functionality! Our API endpoints are automatically built from our database module -- so it's very easy to implement new features and add new tables.","title":"Our example endpoint"},{"location":"full_guides/website/rest_api/#accessing-the-api","text":"Let's start with a normal URL and webpage: http://simmate.org/third-parties/MatprojStructure/ When you open that link, you are brought to a webpage that let's you search through all Materials Project structures. Under the hood, this URL is actually a REST API too! All we have to do is add ?format=api to the end of the URL. Try opening this webpage: http://simmate.org/third-parties/MatprojStructure/?format=api Likewise, if we use ?format=json , we can get our data back as a JSON dictionary: http://simmate.org/third-parties/MatprojStructure/?format=json The same can be done for individual entries too! For example, if we wanted all the data for the structure with id mp-1 , then we can do... http://simmate.org/third-parties/MatprojStructure/mp-1/?format=api http://simmate.org/third-parties/MatprojStructure/mp-1/?format=json For these, you should see an output similar too... { \"id\" : \"mp-1\" , \"structure\" : \"...(hidden for clarity)\" , \"nsites\" : 1 , \"nelements\" : 1 , \"elements\" : [ \"Cs\" ], \"chemical_system\" : \"Cs\" , \"density\" : 1.9350390306525629 , \"density_atomic\" : 0.00876794537479071 , \"volume\" : 114.05180544066401 , \"volume_molar\" : 68.68360262958124 , \"formula_full\" : \"Cs1\" , \"formula_reduced\" : \"Cs\" , \"formula_anonymous\" : \"A\" , \"energy\" : -0.85663276 , \"energy_per_atom\" : -0.85663276 , \"energy_above_hull\" : null , \"is_stable\" : null , \"decomposes_to\" : null , \"formation_energy\" : null , \"formation_energy_per_atom\" : null , \"spacegroup\" : 229 , } This is super useful because we can grab data from any programming language we'd like -- python, javascript, c++, fortran, or even the command-line. The only requirement to use our REST API is that you have access to the internet! Once you load your desired data, what you do with the JSON output is up to you.","title":"Accessing the API"},{"location":"full_guides/website/rest_api/#filtering-results","text":"Our URLs also support complex filtering too. As an example, let's make a search where we want all structures that have the spacegroup 229 and also are in the Cr-N chemcial system. When you make this search in the normal webpage, you'll notice the URL becomes... http://simmate.org/third-parties/MatprojStructure/?chemical_system=Cr-N&spacegroup__number=229 We specify our conditions by adding a question mark ( ? ) at the end of the URL and then adding example_key=desired_value after that. As we add new conditions, we separate them with & -- which results in key1=value1&key2=value2&key3=value3 and so on. You can also add format=api at the end of this too! Note, our python client for accessing data is MUCH more powerful for filtering through results, so we recommend accessing data using the simmate.database module in complex/advanced cases.","title":"Filtering results"},{"location":"full_guides/website/rest_api/#paginating-results","text":"To protect our servers from overuse, Simmate currently returns a maximum of 12 results at a time. Pagination is handled automitically using the page=... keyword in the URL. In the HTML, API, and JSON views, you should always have the link to the next page of results available. For example in the JSON view, the returned data includes next and previous URLs.","title":"Paginating results"},{"location":"full_guides/website/rest_api/#ordering-results","text":"For API and JSON formats, you can manually set the ordering of returned data by adding ordering=example_column to your URL. You can also reverse the ordering with ordering=-example_column (note the \" - \" symbol before the column name). For example: http://simmate.org/third-parties/MatprojStructure/?ordering=density_atomic","title":"Ordering results"},{"location":"full_guides/website/rest_api/#the-api-for-experts","text":"We build Simmate's REST API using the django-rest-framework python package, and implement filtering using django-filter . Our endpoints are NOT fixed but are instead dynamically created for each request. This is thanks to our SimmateAPIViewset class in the simmate.website.core_components module, which takes a Simmate database table and automatically renders an API endpoint for us. The backend implementation of dynamic APIs is still experimental because we are exploring the pros and cons the approach -- for example, this enables quick start-up times for configuring Django, but also comes at the cost of more CPU time per web request.","title":"The API for experts"},{"location":"full_guides/workflows/creating_new_workflows/","text":"Creating new workflows \u00b6 Create a workflow name \u00b6 Build your workflow name using the Simmate conventions and run some checks to make sure everything works as expected: from simmate.workflow_engine import Workflow class Example__Python__MyFavoriteSettings ( Workflow ): pass # we will build the rest of workflow later # These names can be long and unfriendly, so it can be nice to # link them to a variable name for easier access. my_workflow = Example__Python__MyFavoriteSettings # Now check that our naming convention works as expected assert my_workflow . name_full == \"example.python.my-favorite-settings\" assert my_workflow . name_type == \"example\" assert my_workflow . name_calculator == \"python\" assert my_workflow . name_preset == \"my-favorite-settings\" Warning Higher level features such as the website interface require that workflow names follow a certain format. If you skip this step, your workflows will fail and cause errors elsewhere. Tip make sure you have read of \"Workflow Names\" documentation. A basic workflow \u00b6 To build a Simmate workflow, you can have ANY python code you'd like. The only requirement is that you place that code inside a run_config method of a new subclass for Workflow : from simmate.workflow_engine import Workflow class Example__Python__MyFavoriteSettings ( Workflow ): use_database = False # we don't have a database table yet @staticmethod def run_config ( ** kwargs ): print ( \"This workflow doesn't do much\" ) return 12345 Note Behind the scenes, the run method is converting our run_config to a workflow and doing extra setup tasks for us. Danger Note that we added **kwargs to our function input. This is required for your workflow to run. Make sure you read the \"Default parameters\" section below to understand why. A pythonic workflow \u00b6 Now let's look at a realistic example where we build a Workflow that has input parameters and accesses class attributes/methods: class Example__Python__MyFavoriteSettings ( Workflow ): use_database = False # we don't have a database table yet example_constant = 12 @staticmethod def squared ( x ): return x ** 2 @classmethod def run_config ( cls , name , say_hello = True , ** kwargs ): # Workflows can contain ANY python code! # In other words... # \"The ceiling is the roof\" -Michael Jordan if say_hello : print ( f \"Hello and welcome, { name } !\" ) # grab class values and methods x = cls . example_constant example_calc = cls . squared ( x ) print ( f \"Our calculation gave a result of { example_calc } \" ) # grab extra arguments if you need them for key , value in kwargs . items (): print ( f \"An extra parameter for { key } was given \" \"with a value of {value} \" ) return \"Success!\" Danger The **kwargs is still important here. Make sure we are adding it at the end of our input parameters. (see the next section for why) Default parameters and using kwargs \u00b6 You'll notice in the workflows above that we used **kwargs in each of our run_config methods, and if you remove these, the workflow will fail. This is because simmate automatically passes default parameters to the run_config method -- even if you didn't define them as inputs. We do this to allow all workflows to access key information about the run. These parameters are: run_id : a unique id to help with tracking a calculation directory : a unique foldername that the calculation will take place in compress_output : whether to compress the directory to a zip file when we're done source : where the input of this calculation came from You can use any of these inputs to help with your workflow. Or alternatively, just add **kwargs to your function and ignore them. Common input parameters \u00b6 You often will use input parameters that correspond to toolkit objects, such as Structure or Composition . If you use the matching input parameter name, these will inherit all of their features -- such as loading from filename, a dictionary, or python object. For example, if you use a structure input variable, it behaves as described in the Parameters section. from simmate.toolkit import Structure from simmate.workflow_engine import Workflow class Example__Python__MyFavoriteSettings ( Workflow ): use_database = False # we don't have a database table yet @staticmethod def run_config ( structure , ** kwargs ): # Even if we give a filename as an input, Simmate will convert it # to a python object for us assert type ( structure ) == Structure # and you can interact with the structure object as usual return structure . volume Tip if you see a parameter in our documentation that has similar use to yours, make sure you use the same name. It can help with adding extra functionality. Writing output files \u00b6 Of all the default parameters (described above), you'll most like get the most from using the directory input. It is important to note that directory is given as a pathlib.Path object. Just add directory to your run_config() method and then use the object that's provided. For example, this workflow will write an output file to simmate-task-12345/my_output.txt (where the simmate-task-12345 folder is automatically set up by Simmate). from simmate.workflow_engine import Workflow class Example__Python__MyFavoriteSettings ( Workflow ): use_database = False # we don't have a database table yet @staticmethod def run_config ( directory , ** kwargs ): # We use the unique directory to write outputs! # Recall that we have a pathlib.Path object. output_file = directory / \"my_output.txt\" with output_file . open ( \"w\" ) as file : file . write ( \"Writing my output!\" ) # If you don't like/know pathlib.Path, you can # convert the directory name back to a string output_filename = str ( output_file ) return \"Done!\" Building from existing workflows \u00b6 For many calculators, there are workflow classes that you can use as a starting point. For example, VASP users can inherit from the VaspWorkflow class, which includes many features built-in: basic VASP example full-feature VASP example Custom INCAR modifier from simmate.calculators.vasp.workflows.base import VaspWorkflow class Relaxation__Vasp__MyExample1 ( VaspWorkflow ): functional = \"PBE\" potcar_mappings = { \"Y\" : \"Y_sv\" , \"C\" : \"C\" } incar = dict ( PREC = \"Normal\" , EDIFF = 1e-4 , ENCUT = 450 , NSW = 100 , KSPACING = 0.4 , ) from simmate.calculators.vasp.workflows.base import VaspWorkflow from simmate.calculators.vasp.inputs.potcar_mappings import ( PBE_ELEMENT_MAPPINGS , ) from simmate.calculators.vasp.error_handlers import ( Frozen , NonConverging , Unconverged , Walltime , ) class Relaxation__Vasp__MyExample2 ( VaspWorkflow ): functional = \"PBE\" potcar_mappings = PBE_ELEMENT_MAPPINGS # (1) incar = dict ( PREC = \"Normal\" , # (2) EDIFF__per_atom = 1e-5 , # (3) ENCUT = 450 , ISIF = 3 , NSW = 100 , IBRION = 1 , POTIM = 0.02 , LCHARG = False , LWAVE = False , KSPACING = 0.4 , multiple_keywords__smart_ismear = { # (4) \"metal\" : dict ( ISMEAR = 1 , SIGMA = 0.06 , ), \"non-metal\" : dict ( ISMEAR = 0 , SIGMA = 0.05 , ), }, # WARNING --> see \"Custom Modifier\"\" tab for this to work EXAMPLE__multiply_nsites = 8 , # (5) ) error_handlers = [ # (6) Unconverged (), NonConverging (), Frozen (), Walltime (), ] You can use pre-set mapping for all elements rather than define them yourself Settings that match the normal VASP input are the same for all structures regardless of composition. Settings can also be set based on the input structure using built-in tags like __per_atom . Note the two underscores ( __ ) signals that we are using a input modifier. The type of smearing we use depends on if we have a metal, semiconductor, or insulator. So we need to decide this using a built-in keyword modifier named smart_ismear . Because this handles the setting of multiple INCAR values, the input begins with multiple_keywords instead of a parameter name. If you want to create your own logic for an input parameter, you can do that as well. Here we are showing a new modifier named multiply_nsites . This would set the incar value of EXAMPLE=16 for structure with 2 sites (2*8=16). Note, we define how this modifer works and register it in the \"Custom INCAR modifier\" tab. Make sure you include this code as well. These are some default error handlers to use, and there are many more error handlers available than what's shown. Note, the order of the handlers matters here. Only the first error handler triggered in this list will be used before restarting the job If you need to add advanced logic for one of your INCAR tags, you can register a keyword_modifier to the INCAR class like so: # STEP 1: define the logic of your modifier as a function # Note that the function name must begin with \"keyword_modifier_\" def keyword_modifier_multiply_nsites ( structure , example_mod_input ): # add your advanced logic to determine the keyword value. return structure . num_sites * example_mod_input # STEP 2: register modifier with the Incar class from simmate.calculators.vasp.inputs import Incar Incar . add_keyword_modifier ( keyword_modifier_multiply_nsites ) # STEP 3: use your new modifier with any parameter you'd like incar = dict ( \"NSW__multiply_nsites\" : 2 , \"EXAMPLE__multiply_nsites\" : 123 , ) Danger Make sure this code is ran BEFORE you run the workflow. Registration is reset every time a new python session starts. Therefore, we recommend keeping your modifer in the same file that you define your workflow in. Further, can use python inheritance to borrow utilities and settings from an existing workflow: from simmate.workflows.utilities import get_workflow original_workflow = get_workflow ( \"static-energy.vasp.matproj\" ) class StaticEnergy__Vasp__MyCustomPreset ( original_workflow ): version = \"2022.07.04\" incar = original_workflow . incar . copy () # Make sure you copy! incar . update ( dict ( NPAR = 1 , ENCUT =- 1 , ) ) # make sure we have new settings updated # and that we didn't change the original assert original_workflow . incar != StaticEnergy__Vasp__MyCustomPreset Danger Make sure you are making copies of the original workflow settings! If you modify them without making a copy, you'll actually be changing the original workflow settings. The assert check that we make in the example above is therefore very important. Tip To gain more insight to workflows like this, you should read through both the \"Creating S3 Workflows\" and \"Third-party Software\" sections for more information. Linking a database table \u00b6 Many of workflows will want to store common types of data (such as static energy or relaxation data). If you would like to use these tables automatically, you simply to make sure you name_type matches what is available! For example, if we look at a static-energy calculation, you will see the StaticEnergy database table is automatically used because the name of our workflow starts with \"StaticEnergy\": from simmate.database import connect from simmate.database.workflow_results import StaticEnergy # no work required! This line shows everything is setup and working assert StaticEnergy__Vasp__MyCustomPreset . database_table == StaticEnergy If you would like to build or use a custom database, you must first have a registered DatabaseTable , and then you can link the database table to your workflow directly. The only other requiredment is that your database table uses the Calculation database mix-in: from my_project.models import MyCustomTable class Example__Python__MyFavoriteSettings ( Workflow ): database_table = MyCustomTable Tip See the \"Getting Started\" and \"Database\" tutorials for how to build a custom database table. Warning Make sure your table uses the Calculation mix-in so that the run information can be stored properly Workflows that call a command \u00b6 In many cases, you may have a workflow that runs a command or some external program and then reads the results from output files. An example of this would be an energy calculation using VASP. If your workflow involves calling another program, you should read about the S3Workflow which helps with writing input files, calling other programs, and handling errors. Registering your workflow \u00b6 Registering your workflow so that you can access it in the UI requires you to build a \"simmate project\". This is covered in the getting-started tutorials. Note For now, you can treat this step as optional if you do not have any custom database tables. Running our custom workflow \u00b6 Once you have your new workflow and registered it, you can run it as you would any other one. yaml python workflow_name : path/to/my/script.py:my_workflow_obj # (1) # Example parameters from our \"Basic Workflow\" above name : Jack say_hello : true If your workflow is not regiestered, you need to provide the path to your python script (e.g. my_script.py file) and then the variable name that the workflow is stored as. The normal variable would be Example__Python__MyFavoriteSettings , but in the python example, we set it to something shorter like my_workflow for convenience. # in the same file the workflow is defined in # These names can be long and unfriendly, so it can be nice to # link them to a variable name for easier access. my_workflow = Example__Python__MyFavoriteSettings # Here we use parameters from our \"Basic Workflow\" above state = my_workflow . run ( name = \"Jack\" say_hello = True , ) result = state . result () Example If you wrote your workflow in a file name learning_simmate.py , you could set the workflow_name to learning_simmate.py:Example__Python__MyFavoriteSettings . Make sure you read the \"common input parameters\" section above. These let us really take advantage of how we provide our input. For example, a structure parameter will automatically accept filenames or database entries: yaml python workflow_name : path/to/my/script.py:my_workflow_obj # Automatic features! structure : database_table : MatProjStructure database_id : mp-123 # in the same file the workflow is defined in state = my_workflow . run ( structure = { \"database_table\" : \"MatProjStructure\" , \"database_id\" : \"mp-123\" , } ) result = state . result () Warning When switching from Python to YAML, make sure you adjust the input format of your parameters. This is especially important if you use python a list or dict for one of your input parameters. Further, if you have complex input parameters (e.g. nested lists, matricies, etc.), we recommend using a TOML input file instead. lists dictionaries nested lists tuple # in python my_parameter = [ 1 , 2 , 3 ] # in yaml my_parameter : - 1 - 2 - 3 # in python my_parameter = { \"a\" : 123 , \"b\" : 456 , \"c\" : [ \"apple\" , \"orange\" , \"grape\" ]} # in yaml my_parameter : a : 123 b : 456 c : - apple - orange - grape # in toml [my_parameter] a = 123 b = 456 c = [ \"apple\" , \"orange\" , \"grape\" ] # in python my_parameter = [ [ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ], ] # in yaml (we recommend switching to TOML!) my_parameter : - - 1 - 2 - 3 - - 4 - 5 - 6 - - 7 - 8 - 9 # in toml my_parameter = [ [ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ], ] # in python my_parameter = ( 1 , 2 , 3 ) # in yaml my_parameter : - 1 - 2 - 3 # WARNING: This will return a list! Make sure you call # `tuple(my_parameter)` # at the start of your workflow's `run_config` if you need a tuple. # in toml my_parameter = [ 1 , 2 , 3 ] # WARNING: This will return a list! Make sure you call # `tuple(my_parameter)` # at the start of your workflow's `run_config` if you need a tuple.","title":"Creating new workflows"},{"location":"full_guides/workflows/creating_new_workflows/#creating-new-workflows","text":"","title":"Creating new workflows"},{"location":"full_guides/workflows/creating_new_workflows/#create-a-workflow-name","text":"Build your workflow name using the Simmate conventions and run some checks to make sure everything works as expected: from simmate.workflow_engine import Workflow class Example__Python__MyFavoriteSettings ( Workflow ): pass # we will build the rest of workflow later # These names can be long and unfriendly, so it can be nice to # link them to a variable name for easier access. my_workflow = Example__Python__MyFavoriteSettings # Now check that our naming convention works as expected assert my_workflow . name_full == \"example.python.my-favorite-settings\" assert my_workflow . name_type == \"example\" assert my_workflow . name_calculator == \"python\" assert my_workflow . name_preset == \"my-favorite-settings\" Warning Higher level features such as the website interface require that workflow names follow a certain format. If you skip this step, your workflows will fail and cause errors elsewhere. Tip make sure you have read of \"Workflow Names\" documentation.","title":"Create a workflow name"},{"location":"full_guides/workflows/creating_new_workflows/#a-basic-workflow","text":"To build a Simmate workflow, you can have ANY python code you'd like. The only requirement is that you place that code inside a run_config method of a new subclass for Workflow : from simmate.workflow_engine import Workflow class Example__Python__MyFavoriteSettings ( Workflow ): use_database = False # we don't have a database table yet @staticmethod def run_config ( ** kwargs ): print ( \"This workflow doesn't do much\" ) return 12345 Note Behind the scenes, the run method is converting our run_config to a workflow and doing extra setup tasks for us. Danger Note that we added **kwargs to our function input. This is required for your workflow to run. Make sure you read the \"Default parameters\" section below to understand why.","title":"A basic workflow"},{"location":"full_guides/workflows/creating_new_workflows/#a-pythonic-workflow","text":"Now let's look at a realistic example where we build a Workflow that has input parameters and accesses class attributes/methods: class Example__Python__MyFavoriteSettings ( Workflow ): use_database = False # we don't have a database table yet example_constant = 12 @staticmethod def squared ( x ): return x ** 2 @classmethod def run_config ( cls , name , say_hello = True , ** kwargs ): # Workflows can contain ANY python code! # In other words... # \"The ceiling is the roof\" -Michael Jordan if say_hello : print ( f \"Hello and welcome, { name } !\" ) # grab class values and methods x = cls . example_constant example_calc = cls . squared ( x ) print ( f \"Our calculation gave a result of { example_calc } \" ) # grab extra arguments if you need them for key , value in kwargs . items (): print ( f \"An extra parameter for { key } was given \" \"with a value of {value} \" ) return \"Success!\" Danger The **kwargs is still important here. Make sure we are adding it at the end of our input parameters. (see the next section for why)","title":"A pythonic workflow"},{"location":"full_guides/workflows/creating_new_workflows/#default-parameters-and-using-kwargs","text":"You'll notice in the workflows above that we used **kwargs in each of our run_config methods, and if you remove these, the workflow will fail. This is because simmate automatically passes default parameters to the run_config method -- even if you didn't define them as inputs. We do this to allow all workflows to access key information about the run. These parameters are: run_id : a unique id to help with tracking a calculation directory : a unique foldername that the calculation will take place in compress_output : whether to compress the directory to a zip file when we're done source : where the input of this calculation came from You can use any of these inputs to help with your workflow. Or alternatively, just add **kwargs to your function and ignore them.","title":"Default parameters and using kwargs"},{"location":"full_guides/workflows/creating_new_workflows/#common-input-parameters","text":"You often will use input parameters that correspond to toolkit objects, such as Structure or Composition . If you use the matching input parameter name, these will inherit all of their features -- such as loading from filename, a dictionary, or python object. For example, if you use a structure input variable, it behaves as described in the Parameters section. from simmate.toolkit import Structure from simmate.workflow_engine import Workflow class Example__Python__MyFavoriteSettings ( Workflow ): use_database = False # we don't have a database table yet @staticmethod def run_config ( structure , ** kwargs ): # Even if we give a filename as an input, Simmate will convert it # to a python object for us assert type ( structure ) == Structure # and you can interact with the structure object as usual return structure . volume Tip if you see a parameter in our documentation that has similar use to yours, make sure you use the same name. It can help with adding extra functionality.","title":"Common input parameters"},{"location":"full_guides/workflows/creating_new_workflows/#writing-output-files","text":"Of all the default parameters (described above), you'll most like get the most from using the directory input. It is important to note that directory is given as a pathlib.Path object. Just add directory to your run_config() method and then use the object that's provided. For example, this workflow will write an output file to simmate-task-12345/my_output.txt (where the simmate-task-12345 folder is automatically set up by Simmate). from simmate.workflow_engine import Workflow class Example__Python__MyFavoriteSettings ( Workflow ): use_database = False # we don't have a database table yet @staticmethod def run_config ( directory , ** kwargs ): # We use the unique directory to write outputs! # Recall that we have a pathlib.Path object. output_file = directory / \"my_output.txt\" with output_file . open ( \"w\" ) as file : file . write ( \"Writing my output!\" ) # If you don't like/know pathlib.Path, you can # convert the directory name back to a string output_filename = str ( output_file ) return \"Done!\"","title":"Writing output files"},{"location":"full_guides/workflows/creating_new_workflows/#building-from-existing-workflows","text":"For many calculators, there are workflow classes that you can use as a starting point. For example, VASP users can inherit from the VaspWorkflow class, which includes many features built-in: basic VASP example full-feature VASP example Custom INCAR modifier from simmate.calculators.vasp.workflows.base import VaspWorkflow class Relaxation__Vasp__MyExample1 ( VaspWorkflow ): functional = \"PBE\" potcar_mappings = { \"Y\" : \"Y_sv\" , \"C\" : \"C\" } incar = dict ( PREC = \"Normal\" , EDIFF = 1e-4 , ENCUT = 450 , NSW = 100 , KSPACING = 0.4 , ) from simmate.calculators.vasp.workflows.base import VaspWorkflow from simmate.calculators.vasp.inputs.potcar_mappings import ( PBE_ELEMENT_MAPPINGS , ) from simmate.calculators.vasp.error_handlers import ( Frozen , NonConverging , Unconverged , Walltime , ) class Relaxation__Vasp__MyExample2 ( VaspWorkflow ): functional = \"PBE\" potcar_mappings = PBE_ELEMENT_MAPPINGS # (1) incar = dict ( PREC = \"Normal\" , # (2) EDIFF__per_atom = 1e-5 , # (3) ENCUT = 450 , ISIF = 3 , NSW = 100 , IBRION = 1 , POTIM = 0.02 , LCHARG = False , LWAVE = False , KSPACING = 0.4 , multiple_keywords__smart_ismear = { # (4) \"metal\" : dict ( ISMEAR = 1 , SIGMA = 0.06 , ), \"non-metal\" : dict ( ISMEAR = 0 , SIGMA = 0.05 , ), }, # WARNING --> see \"Custom Modifier\"\" tab for this to work EXAMPLE__multiply_nsites = 8 , # (5) ) error_handlers = [ # (6) Unconverged (), NonConverging (), Frozen (), Walltime (), ] You can use pre-set mapping for all elements rather than define them yourself Settings that match the normal VASP input are the same for all structures regardless of composition. Settings can also be set based on the input structure using built-in tags like __per_atom . Note the two underscores ( __ ) signals that we are using a input modifier. The type of smearing we use depends on if we have a metal, semiconductor, or insulator. So we need to decide this using a built-in keyword modifier named smart_ismear . Because this handles the setting of multiple INCAR values, the input begins with multiple_keywords instead of a parameter name. If you want to create your own logic for an input parameter, you can do that as well. Here we are showing a new modifier named multiply_nsites . This would set the incar value of EXAMPLE=16 for structure with 2 sites (2*8=16). Note, we define how this modifer works and register it in the \"Custom INCAR modifier\" tab. Make sure you include this code as well. These are some default error handlers to use, and there are many more error handlers available than what's shown. Note, the order of the handlers matters here. Only the first error handler triggered in this list will be used before restarting the job If you need to add advanced logic for one of your INCAR tags, you can register a keyword_modifier to the INCAR class like so: # STEP 1: define the logic of your modifier as a function # Note that the function name must begin with \"keyword_modifier_\" def keyword_modifier_multiply_nsites ( structure , example_mod_input ): # add your advanced logic to determine the keyword value. return structure . num_sites * example_mod_input # STEP 2: register modifier with the Incar class from simmate.calculators.vasp.inputs import Incar Incar . add_keyword_modifier ( keyword_modifier_multiply_nsites ) # STEP 3: use your new modifier with any parameter you'd like incar = dict ( \"NSW__multiply_nsites\" : 2 , \"EXAMPLE__multiply_nsites\" : 123 , ) Danger Make sure this code is ran BEFORE you run the workflow. Registration is reset every time a new python session starts. Therefore, we recommend keeping your modifer in the same file that you define your workflow in. Further, can use python inheritance to borrow utilities and settings from an existing workflow: from simmate.workflows.utilities import get_workflow original_workflow = get_workflow ( \"static-energy.vasp.matproj\" ) class StaticEnergy__Vasp__MyCustomPreset ( original_workflow ): version = \"2022.07.04\" incar = original_workflow . incar . copy () # Make sure you copy! incar . update ( dict ( NPAR = 1 , ENCUT =- 1 , ) ) # make sure we have new settings updated # and that we didn't change the original assert original_workflow . incar != StaticEnergy__Vasp__MyCustomPreset Danger Make sure you are making copies of the original workflow settings! If you modify them without making a copy, you'll actually be changing the original workflow settings. The assert check that we make in the example above is therefore very important. Tip To gain more insight to workflows like this, you should read through both the \"Creating S3 Workflows\" and \"Third-party Software\" sections for more information.","title":"Building from existing workflows"},{"location":"full_guides/workflows/creating_new_workflows/#linking-a-database-table","text":"Many of workflows will want to store common types of data (such as static energy or relaxation data). If you would like to use these tables automatically, you simply to make sure you name_type matches what is available! For example, if we look at a static-energy calculation, you will see the StaticEnergy database table is automatically used because the name of our workflow starts with \"StaticEnergy\": from simmate.database import connect from simmate.database.workflow_results import StaticEnergy # no work required! This line shows everything is setup and working assert StaticEnergy__Vasp__MyCustomPreset . database_table == StaticEnergy If you would like to build or use a custom database, you must first have a registered DatabaseTable , and then you can link the database table to your workflow directly. The only other requiredment is that your database table uses the Calculation database mix-in: from my_project.models import MyCustomTable class Example__Python__MyFavoriteSettings ( Workflow ): database_table = MyCustomTable Tip See the \"Getting Started\" and \"Database\" tutorials for how to build a custom database table. Warning Make sure your table uses the Calculation mix-in so that the run information can be stored properly","title":"Linking a database table"},{"location":"full_guides/workflows/creating_new_workflows/#workflows-that-call-a-command","text":"In many cases, you may have a workflow that runs a command or some external program and then reads the results from output files. An example of this would be an energy calculation using VASP. If your workflow involves calling another program, you should read about the S3Workflow which helps with writing input files, calling other programs, and handling errors.","title":"Workflows that call a command"},{"location":"full_guides/workflows/creating_new_workflows/#registering-your-workflow","text":"Registering your workflow so that you can access it in the UI requires you to build a \"simmate project\". This is covered in the getting-started tutorials. Note For now, you can treat this step as optional if you do not have any custom database tables.","title":"Registering your workflow"},{"location":"full_guides/workflows/creating_new_workflows/#running-our-custom-workflow","text":"Once you have your new workflow and registered it, you can run it as you would any other one. yaml python workflow_name : path/to/my/script.py:my_workflow_obj # (1) # Example parameters from our \"Basic Workflow\" above name : Jack say_hello : true If your workflow is not regiestered, you need to provide the path to your python script (e.g. my_script.py file) and then the variable name that the workflow is stored as. The normal variable would be Example__Python__MyFavoriteSettings , but in the python example, we set it to something shorter like my_workflow for convenience. # in the same file the workflow is defined in # These names can be long and unfriendly, so it can be nice to # link them to a variable name for easier access. my_workflow = Example__Python__MyFavoriteSettings # Here we use parameters from our \"Basic Workflow\" above state = my_workflow . run ( name = \"Jack\" say_hello = True , ) result = state . result () Example If you wrote your workflow in a file name learning_simmate.py , you could set the workflow_name to learning_simmate.py:Example__Python__MyFavoriteSettings . Make sure you read the \"common input parameters\" section above. These let us really take advantage of how we provide our input. For example, a structure parameter will automatically accept filenames or database entries: yaml python workflow_name : path/to/my/script.py:my_workflow_obj # Automatic features! structure : database_table : MatProjStructure database_id : mp-123 # in the same file the workflow is defined in state = my_workflow . run ( structure = { \"database_table\" : \"MatProjStructure\" , \"database_id\" : \"mp-123\" , } ) result = state . result () Warning When switching from Python to YAML, make sure you adjust the input format of your parameters. This is especially important if you use python a list or dict for one of your input parameters. Further, if you have complex input parameters (e.g. nested lists, matricies, etc.), we recommend using a TOML input file instead. lists dictionaries nested lists tuple # in python my_parameter = [ 1 , 2 , 3 ] # in yaml my_parameter : - 1 - 2 - 3 # in python my_parameter = { \"a\" : 123 , \"b\" : 456 , \"c\" : [ \"apple\" , \"orange\" , \"grape\" ]} # in yaml my_parameter : a : 123 b : 456 c : - apple - orange - grape # in toml [my_parameter] a = 123 b = 456 c = [ \"apple\" , \"orange\" , \"grape\" ] # in python my_parameter = [ [ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ], ] # in yaml (we recommend switching to TOML!) my_parameter : - - 1 - 2 - 3 - - 4 - 5 - 6 - - 7 - 8 - 9 # in toml my_parameter = [ [ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ], ] # in python my_parameter = ( 1 , 2 , 3 ) # in yaml my_parameter : - 1 - 2 - 3 # WARNING: This will return a list! Make sure you call # `tuple(my_parameter)` # at the start of your workflow's `run_config` if you need a tuple. # in toml my_parameter = [ 1 , 2 , 3 ] # WARNING: This will return a list! Make sure you call # `tuple(my_parameter)` # at the start of your workflow's `run_config` if you need a tuple.","title":"Running our custom workflow"},{"location":"full_guides/workflows/nested_workflows/","text":"Overview \u00b6 Because workflows can contain any python code, they can also make calls to other workflows -- either via run or run_cloud methods. This enables calling one or serveral workflows in succession -- or even submitting them to the cluster if you have many analyses to run. Calling one workflow repeatedly \u00b6 You can use the run method of a workflow within another workflow and call it as much as you'd like. from simmate.workflows.utilities import get_workflow from simmate.workflow_engine import Workflow class Example__Python__MyFavoriteSettings ( Workflow ): use_database = False # we don't have a database table yet @staticmethod def run_config ( structure , ** kwargs ): # you can grab a workflow locally, attach one as a class # attribute, or anything else possible with python another_workflow = get_workflow ( \"static-energy.vasp.mit\" ) # And run the workflow how you would like. Here, we are # just running the workflow 10 times in row on different # perturbations or \"rattling\" of the original structure for n in range ( 10 ): structure . perturb ( 0.05 ) # modifies in-place state = another_workflow . run ( structure = structure ) result = state . result () # ... do something with the result Note Notice that we are calling state.result() just like we would a normal workflow run. Usage is exactly the same. Calling multiple workflows \u00b6 You can also call a series of workflows on an input. Again, any python will be accepted within the run_config so workflow usage does not change: from simmate.workflows.utilities import get_workflow from simmate.workflow_engine import Workflow class Example__Python__MyFavoriteSettings ( Workflow ): use_database = False @staticmethod def run_config ( structure , directory , ** kwargs ): subworkflow_1 = get_workflow ( \"static-energy.vasp.mit\" ) subworkflow_1 . run ( structure = structure ) subworkflow_2 = get_workflow ( \"population-analysis.vasp.elf-matproj\" ) subworkflow_2 . run ( structure = structure ) subworkflow_3 = get_workflow ( \"electronic-structure.vasp.matproj-full\" ) subworkflow_3 . run ( structure = structure ) Writing all runs to a shared directory \u00b6 When using run, you often want workflows to share a working directory, so that you can find the results all in one place. To do this, we simply need to set the directory manually for each subworkflow run: from simmate.workflows.utilities import get_workflow from simmate.workflow_engine import Workflow class Example__Python__MyFavoriteSettings ( Workflow ): use_database = False @staticmethod def run_config ( structure , directory , ** kwargs ): # <-- uses directory as an input another_workflow = get_workflow ( \"static-energy.vasp.mit\" ) for n in range ( 10 ): structure . perturb ( 0.05 ) # make sure the directory name is unique subdirectory = directory / f \"perturb_number_ { n } \" another_workflow . run ( structure = structure , directory = subdirectory , # <-- creates a subdirectory for this run ) Tip Also see writing output files Danger when using run_cloud you should NOT share a working directory. This causes problems when you have computational resource scattered accross different computers & file systems. See github #237 . Passing results between runs \u00b6 When you grab the result from one subworkflow, you can interact with that database object to pass the results to the next subworkflow. from simmate.workflows.utilities import get_workflow from simmate.workflow_engine import Workflow class Example__Python__MyFavoriteSettings ( Workflow ): use_database = False @staticmethod def run_config ( structure , directory , ** kwargs ): subworkflow_1 = get_workflow ( \"relaxation.vasp.mit\" ) state_1 = subworkflow_1 . run ( structure = structure ) result_1 = state_1 . result () # When passing structures, we can directly use the result. This is # because the 'structure' parameter accepts database objects as input. subworkflow_2 = get_workflow ( \"static-energy.vasp.mit\" ) state_2 = subworkflow_2 . run ( structure = result_1 , # use the final structure of the last calc ) result_2 = state_2 . result () # Alternatively, you may want to mutate or analyze the result in # some way before submitting a new calculations if result_2 . energy_per_atom > 0 : print ( \"Structure is very unstable even after relaxing!\" ) # maybe the atoms are too close, so let's increase the volume by 20% structure_new = result_2 . to_toolkit () structure_new . scale_lattice ( volume = structure . volume * 1.2 , ) # and try the workflow again state_2 = subworkflow_2 . run ( structure = structure_new , # use the modified structure ) Submitting parallel workflows \u00b6 Sometimes, we don't want to pause and wait for each workflow run to finish. There are even cases where we would submit hundreds of workflow runs that are indpendent and can run in parallel. To do this, we can use the run_cloud command instead of calling run . from simmate.workflows.utilities import get_workflow from simmate.workflow_engine import Workflow class Example__Python__MyFavoriteSettings ( Workflow ): use_database = False # we don't have a database table yet @staticmethod def run_config ( structure , ** kwargs ): another_workflow = get_workflow ( \"static-energy.vasp.mit\" ) # keep track of the runs we submit submitted_states = [] for n in range ( 10 ): structure . perturb ( 0.05 ) # modifies in-place # submit to cloud instead of running locally state = another_workflow . run_cloud ( structure = structure ) # add the state to our list submitted_states . append ( state ) # do NOT call result yet! This will block and wait for this # calculation to finish before continuing # state.result() # now wait for all the calculations to finish and grab the results results = [ state . result () for state in submitted_states ] # And workup the results as you see fit for result in results : print ( result . energy_per_atom ) Danger when using run_cloud you should NOT share a working directory. This causes problems when you have computational resource scattered accross different computers & file systems. See github #237 . Tip Using state.result() to wait for each result is optional too -- you decide when to call it (if at all). You can even have a workflow that just submits runs and then shuts down -- without ever waiting on the results.","title":"Creating Nested Workflows"},{"location":"full_guides/workflows/nested_workflows/#overview","text":"Because workflows can contain any python code, they can also make calls to other workflows -- either via run or run_cloud methods. This enables calling one or serveral workflows in succession -- or even submitting them to the cluster if you have many analyses to run.","title":"Overview"},{"location":"full_guides/workflows/nested_workflows/#calling-one-workflow-repeatedly","text":"You can use the run method of a workflow within another workflow and call it as much as you'd like. from simmate.workflows.utilities import get_workflow from simmate.workflow_engine import Workflow class Example__Python__MyFavoriteSettings ( Workflow ): use_database = False # we don't have a database table yet @staticmethod def run_config ( structure , ** kwargs ): # you can grab a workflow locally, attach one as a class # attribute, or anything else possible with python another_workflow = get_workflow ( \"static-energy.vasp.mit\" ) # And run the workflow how you would like. Here, we are # just running the workflow 10 times in row on different # perturbations or \"rattling\" of the original structure for n in range ( 10 ): structure . perturb ( 0.05 ) # modifies in-place state = another_workflow . run ( structure = structure ) result = state . result () # ... do something with the result Note Notice that we are calling state.result() just like we would a normal workflow run. Usage is exactly the same.","title":"Calling one workflow repeatedly"},{"location":"full_guides/workflows/nested_workflows/#calling-multiple-workflows","text":"You can also call a series of workflows on an input. Again, any python will be accepted within the run_config so workflow usage does not change: from simmate.workflows.utilities import get_workflow from simmate.workflow_engine import Workflow class Example__Python__MyFavoriteSettings ( Workflow ): use_database = False @staticmethod def run_config ( structure , directory , ** kwargs ): subworkflow_1 = get_workflow ( \"static-energy.vasp.mit\" ) subworkflow_1 . run ( structure = structure ) subworkflow_2 = get_workflow ( \"population-analysis.vasp.elf-matproj\" ) subworkflow_2 . run ( structure = structure ) subworkflow_3 = get_workflow ( \"electronic-structure.vasp.matproj-full\" ) subworkflow_3 . run ( structure = structure )","title":"Calling multiple workflows"},{"location":"full_guides/workflows/nested_workflows/#writing-all-runs-to-a-shared-directory","text":"When using run, you often want workflows to share a working directory, so that you can find the results all in one place. To do this, we simply need to set the directory manually for each subworkflow run: from simmate.workflows.utilities import get_workflow from simmate.workflow_engine import Workflow class Example__Python__MyFavoriteSettings ( Workflow ): use_database = False @staticmethod def run_config ( structure , directory , ** kwargs ): # <-- uses directory as an input another_workflow = get_workflow ( \"static-energy.vasp.mit\" ) for n in range ( 10 ): structure . perturb ( 0.05 ) # make sure the directory name is unique subdirectory = directory / f \"perturb_number_ { n } \" another_workflow . run ( structure = structure , directory = subdirectory , # <-- creates a subdirectory for this run ) Tip Also see writing output files Danger when using run_cloud you should NOT share a working directory. This causes problems when you have computational resource scattered accross different computers & file systems. See github #237 .","title":"Writing all runs to a shared directory"},{"location":"full_guides/workflows/nested_workflows/#passing-results-between-runs","text":"When you grab the result from one subworkflow, you can interact with that database object to pass the results to the next subworkflow. from simmate.workflows.utilities import get_workflow from simmate.workflow_engine import Workflow class Example__Python__MyFavoriteSettings ( Workflow ): use_database = False @staticmethod def run_config ( structure , directory , ** kwargs ): subworkflow_1 = get_workflow ( \"relaxation.vasp.mit\" ) state_1 = subworkflow_1 . run ( structure = structure ) result_1 = state_1 . result () # When passing structures, we can directly use the result. This is # because the 'structure' parameter accepts database objects as input. subworkflow_2 = get_workflow ( \"static-energy.vasp.mit\" ) state_2 = subworkflow_2 . run ( structure = result_1 , # use the final structure of the last calc ) result_2 = state_2 . result () # Alternatively, you may want to mutate or analyze the result in # some way before submitting a new calculations if result_2 . energy_per_atom > 0 : print ( \"Structure is very unstable even after relaxing!\" ) # maybe the atoms are too close, so let's increase the volume by 20% structure_new = result_2 . to_toolkit () structure_new . scale_lattice ( volume = structure . volume * 1.2 , ) # and try the workflow again state_2 = subworkflow_2 . run ( structure = structure_new , # use the modified structure )","title":"Passing results between runs"},{"location":"full_guides/workflows/nested_workflows/#submitting-parallel-workflows","text":"Sometimes, we don't want to pause and wait for each workflow run to finish. There are even cases where we would submit hundreds of workflow runs that are indpendent and can run in parallel. To do this, we can use the run_cloud command instead of calling run . from simmate.workflows.utilities import get_workflow from simmate.workflow_engine import Workflow class Example__Python__MyFavoriteSettings ( Workflow ): use_database = False # we don't have a database table yet @staticmethod def run_config ( structure , ** kwargs ): another_workflow = get_workflow ( \"static-energy.vasp.mit\" ) # keep track of the runs we submit submitted_states = [] for n in range ( 10 ): structure . perturb ( 0.05 ) # modifies in-place # submit to cloud instead of running locally state = another_workflow . run_cloud ( structure = structure ) # add the state to our list submitted_states . append ( state ) # do NOT call result yet! This will block and wait for this # calculation to finish before continuing # state.result() # now wait for all the calculations to finish and grab the results results = [ state . result () for state in submitted_states ] # And workup the results as you see fit for result in results : print ( result . energy_per_atom ) Danger when using run_cloud you should NOT share a working directory. This causes problems when you have computational resource scattered accross different computers & file systems. See github #237 . Tip Using state.result() to wait for each result is optional too -- you decide when to call it (if at all). You can even have a workflow that just submits runs and then shuts down -- without ever waiting on the results.","title":"Submitting parallel workflows"},{"location":"full_guides/workflows/overview/","text":"Simmate Workflows \u00b6 This module brings together all predefined workflows and organizes them by application for convenience. This module covers basic use, but more information is available in simmate.workflow_engine.workflow . Basic use \u00b6 The getting-started tutorials will teach you how to run workflows and access their results. But as a review: from simmate.workflows.static_energy import StaticEnergy__Vasp__Matproj as workflow # runs the workflow and returns a state state = workflow . run ( structure = \"my_structure.cif\" ) result = state . result () # gives the DatabaseTable where ALL results are stored workflow . database_table # --> gives all relaxation results workflow . all_results # --> gives all results for this relaxation preset df = workflow . all_results . to_dataframe () # convert to pandas dataframe Further information on interacting with workflows can be found in the simmate.workflow_engine module as well -- particularly, the simmate.workflow_engine.workflow module. Avoiding long import paths \u00b6 If you are a regular python user, you'll notice the import path above is long and unfriendly. If you'd like to avoid importing this way, you can instead use the get_workflow utility: from simmate.workflows.utilities import get_workflow workflow = get_workflow ( \"static-energy.vasp.matproj\" ) Overview of classes \u00b6 Here we try to give a birds-eye view of Simmate workflows and a commonly used subclass known as a \"s3 workflow\". This section is not meant to be an encompassing guide. Instead, beginners should refer to our tutorials and class-level API docs. What is a Workflow ? \u00b6 Recall from Simmate's getting-started tutorial , that a Workflow is made up of 4 stages: configure : chooses our desired settings for the calculation (such as VASP's INCAR settings) schedule : decides whether to run the workflow immediately or send off to a job queue (e.g. SLURM, PBS, or remote computers) execute : writes our input files, runs the calculation (e.g. VASP), and checks the results for errors save : saves the results to our database The configure step is simply how a workflow is defined. Pre-built workflows do this for you already, but you may want to create a custom workflow for more. By creating a new Workflow subclass, you've configured it. The schedule step is handled entirely by Simmate. All that you need to know is that run will carry out the workflow on your local computer, while run_cloud will schedule the workflow to run remotely. The execute step is what we typically think of when we think \"workflow\". It can be anything and everything. The example given (writing inputs, calling a program, and reading output files) is the most common type of workflow in Simmate -- known as a \"S3Workflow\". This is explained more below. The save step is simply taking the result of the execute and saving it to a SQL database. This is handled automatically for common workflows types like relaxations, dynamics, or static-energy calculations, but advanced users may want to customize their own methods. All stages of a Workflow are done through the run or run_cloud methods. That is... Workflow.run = configure + schedule + execute + save . To begin building custom workflows, make sure you have completed the getting-started tutorials and then read through the simmate.workflow_engine.workflow documentation. What is a NestedWorkflow ? \u00b6 Some workflows are \"nested\", which means it's a workflow made up multiple other workflows. An example of this is the relaxation.vasp.staged workflow, which involves a series of relaxations of increasing quality and then a final energy calculation. What is an S3Workflow ? \u00b6 Many workflows involve writing input files, calling some external program, and then reading through the output files. All workflows like this are known as \" S3Workflow \"s. S3 means the workflow is Supervised , Staged , and a Shell call. For shorthand, we call this a \"S3\" workflow. There is some history behind why it's named this way, but here is how the name breaks down: Staged : the overall calculation is made of three stages (each is a class method) setup = write input files run_command_and_monitor = the actual running & monitoring of the program workup = reading the output files Shell : the calculator is called through the command-line (the actual execution call) Supervised : once the shell command is started, Simmate runs in the background to monitor it for errors (occurs during the execution call) All stages of this S3 workflow are packed into the excute step of a Workflow , where Simmate has a lot of functionality built for you already. If you would like to build a custom S3 workflow, we suggest going through: 1. getting-started guides 2. simmate.workflow_engine.workflow documentation 3. simmate.workflow_engine.s3_workflow documentation","title":"Overview"},{"location":"full_guides/workflows/overview/#simmate-workflows","text":"This module brings together all predefined workflows and organizes them by application for convenience. This module covers basic use, but more information is available in simmate.workflow_engine.workflow .","title":"Simmate Workflows"},{"location":"full_guides/workflows/overview/#basic-use","text":"The getting-started tutorials will teach you how to run workflows and access their results. But as a review: from simmate.workflows.static_energy import StaticEnergy__Vasp__Matproj as workflow # runs the workflow and returns a state state = workflow . run ( structure = \"my_structure.cif\" ) result = state . result () # gives the DatabaseTable where ALL results are stored workflow . database_table # --> gives all relaxation results workflow . all_results # --> gives all results for this relaxation preset df = workflow . all_results . to_dataframe () # convert to pandas dataframe Further information on interacting with workflows can be found in the simmate.workflow_engine module as well -- particularly, the simmate.workflow_engine.workflow module.","title":"Basic use"},{"location":"full_guides/workflows/overview/#avoiding-long-import-paths","text":"If you are a regular python user, you'll notice the import path above is long and unfriendly. If you'd like to avoid importing this way, you can instead use the get_workflow utility: from simmate.workflows.utilities import get_workflow workflow = get_workflow ( \"static-energy.vasp.matproj\" )","title":"Avoiding long import paths"},{"location":"full_guides/workflows/overview/#overview-of-classes","text":"Here we try to give a birds-eye view of Simmate workflows and a commonly used subclass known as a \"s3 workflow\". This section is not meant to be an encompassing guide. Instead, beginners should refer to our tutorials and class-level API docs.","title":"Overview of classes"},{"location":"full_guides/workflows/overview/#what-is-a-workflow","text":"Recall from Simmate's getting-started tutorial , that a Workflow is made up of 4 stages: configure : chooses our desired settings for the calculation (such as VASP's INCAR settings) schedule : decides whether to run the workflow immediately or send off to a job queue (e.g. SLURM, PBS, or remote computers) execute : writes our input files, runs the calculation (e.g. VASP), and checks the results for errors save : saves the results to our database The configure step is simply how a workflow is defined. Pre-built workflows do this for you already, but you may want to create a custom workflow for more. By creating a new Workflow subclass, you've configured it. The schedule step is handled entirely by Simmate. All that you need to know is that run will carry out the workflow on your local computer, while run_cloud will schedule the workflow to run remotely. The execute step is what we typically think of when we think \"workflow\". It can be anything and everything. The example given (writing inputs, calling a program, and reading output files) is the most common type of workflow in Simmate -- known as a \"S3Workflow\". This is explained more below. The save step is simply taking the result of the execute and saving it to a SQL database. This is handled automatically for common workflows types like relaxations, dynamics, or static-energy calculations, but advanced users may want to customize their own methods. All stages of a Workflow are done through the run or run_cloud methods. That is... Workflow.run = configure + schedule + execute + save . To begin building custom workflows, make sure you have completed the getting-started tutorials and then read through the simmate.workflow_engine.workflow documentation.","title":"What is a Workflow?"},{"location":"full_guides/workflows/overview/#what-is-a-nestedworkflow","text":"Some workflows are \"nested\", which means it's a workflow made up multiple other workflows. An example of this is the relaxation.vasp.staged workflow, which involves a series of relaxations of increasing quality and then a final energy calculation.","title":"What is a NestedWorkflow?"},{"location":"full_guides/workflows/overview/#what-is-an-s3workflow","text":"Many workflows involve writing input files, calling some external program, and then reading through the output files. All workflows like this are known as \" S3Workflow \"s. S3 means the workflow is Supervised , Staged , and a Shell call. For shorthand, we call this a \"S3\" workflow. There is some history behind why it's named this way, but here is how the name breaks down: Staged : the overall calculation is made of three stages (each is a class method) setup = write input files run_command_and_monitor = the actual running & monitoring of the program workup = reading the output files Shell : the calculator is called through the command-line (the actual execution call) Supervised : once the shell command is started, Simmate runs in the background to monitor it for errors (occurs during the execution call) All stages of this S3 workflow are packed into the excute step of a Workflow , where Simmate has a lot of functionality built for you already. If you would like to build a custom S3 workflow, we suggest going through: 1. getting-started guides 2. simmate.workflow_engine.workflow documentation 3. simmate.workflow_engine.s3_workflow documentation","title":"What is an S3Workflow?"},{"location":"full_guides/workflows/s3_workflows/","text":"The Supervised-Staged-Shell Workflow \u00b6 S3 = Supervised + Staged + Shell \u00b6 This type of workflow helps to supervise a staged workflow involving some shell command. Let's breakdown what this means... A shell command is a single call to some external program. For example, VASP requires that we call the \"vasp_std > vasp.out\" command in order to run a calculation. We consider calling external programs a staged task made up of three steps: setup = writing any input files required for the program execute = actually calling the command and running our program workup = loading data from output files back into python And for supervising the task, this means we monitor the program while the execution stage is running. So once a program is started, Simmate can check output files for common errors/issues -- even while the other program is still running. If an error is found, we stop the program, fix the issue, and then restart it. graph LR A[Start] --> B[setup]; B --> C[execute]; C --> D[workup]; D --> E[Has errors?]; E -->|Yes| B; E -->|No| F[Done!]; Warning This diagram is slightly misleading because the \"Has Errors?\" check also happens while the execute step is still running . Therefore, you can catch errors before your program even finishes & exits! Running S3Workflows is the same as normal workflows (e.g. using the run method), and this entire process of supervising, staging, and shell execution is done for you! S3Workflows for common Calculators \u00b6 For programs that are commonly used in material science, you should also read through their guides in the \"Third-party Software\" section. If your program is listed there, then there is likely a subclass of S3Workflow already built for you. For example, VASP user can take advantage of VaspWorkflow to build workflows. Building a custom S3Workflow \u00b6 Tip Before starting a custom S3Workflow , make sure you have read the section above this (on S3Workflows for common Calculators like VASP). You should also have gone through the guides on building a custom Workflow . Simple command call \u00b6 The most basic example of a S3Workflow is just calling some command -- without doing anything else (no input files, no error handling, etc.). Unlike custom Workflows were we defined a run_config method, S3Workflows have a pre-built run_config method that carries out the different stages and monitoring of a workflow for us. So all the work is already done for us! As an example, let's just use the command echo to print something: from simmate.workflow_engine import S3Workflow class Example__Echo__SayHello ( S3Workflow ): use_database = False # we aren't using a custom table for now monitor = False # there is no error handling yet command = \"echo Hello\" # behaves like a normal workflow state = Example__Echo__SayHello . run () result = state . result () Tip Note that we used \"Echo\" in our workflow name. This helps the user see what commands or programs will be called when a workflow is ran. Custom setup and workup \u00b6 Now what if we'd like to write input files or read output files that are created? Here, we need to update our setup and workup methods: from simmate.workflow_engine import S3Workflow class Example__Echo__SayHello ( S3Workflow ): use_database = False # we aren't using a custom table for now monitor = False # there is no error handling yet command = \"echo Hello > output.txt\" # adds \"Hello\" into a new file @classmethod def setup ( cls , directory , custom_parameter , ** kwargs ): # The directory given is a pathlib.Path object for the directory # that the command will be called in print ( \"I'm setting things up!\" ) print ( f \"My new setting value is { cls . some_new_setting } \" ) print ( f \"My new parameter value is { custom_parmeter } \" ) return # no need to return anything. Nothing will be done with it. @staticmethod def workup ( directory ): # The directory given is a pathlib.Path object for the directory # that the command will be called in # Simply check that we have a new file output_file = directory / \"output.txt\" assert output_file . exists () print ( \"I'm working things up!\" ) return \"Done!\" task = Example__Echo__SayHello () result = task . run () There are a two important things to note here: It's optional to write new setup or workup methods. But if you do... Both setup and workup method should be either a staticmethod or classmethod Custom setup methods require the directory and **kwargs input parameters. Custom workup methods require the directory input paramter It's optional to set/overwrite attributes. You can also add new ones too. Note: S3Workflows for a custom calculator (such VaspWorkflow for VASP) will often have custom setup and workup methods already defined for you. You can update/override these as you see fit. For a full (and advanced) example of a subclass take a look at simmate.calculators.vasp.workflows.base.VaspWorkflow and the tasks that use it like simmate.calculators.vasp.workflows.relaxation.matproj . Custom error handling \u00b6 TODO -- Contact our team if you would like us to prioritize this guide Alternatives to the S3Workflow \u00b6 For experts, this class can be viewed as a combination of prefect's ShellTask, a custodian Job, and Custodian monitoring. When subclassing this, we can absorb functionality of pymatgen.io.vasp.sets too. By merging all of these together into one class, we make things much easier for users and creating new Tasks.","title":"Creating S3 Workflows"},{"location":"full_guides/workflows/s3_workflows/#the-supervised-staged-shell-workflow","text":"","title":"The Supervised-Staged-Shell Workflow"},{"location":"full_guides/workflows/s3_workflows/#s3-supervised-staged-shell","text":"This type of workflow helps to supervise a staged workflow involving some shell command. Let's breakdown what this means... A shell command is a single call to some external program. For example, VASP requires that we call the \"vasp_std > vasp.out\" command in order to run a calculation. We consider calling external programs a staged task made up of three steps: setup = writing any input files required for the program execute = actually calling the command and running our program workup = loading data from output files back into python And for supervising the task, this means we monitor the program while the execution stage is running. So once a program is started, Simmate can check output files for common errors/issues -- even while the other program is still running. If an error is found, we stop the program, fix the issue, and then restart it. graph LR A[Start] --> B[setup]; B --> C[execute]; C --> D[workup]; D --> E[Has errors?]; E -->|Yes| B; E -->|No| F[Done!]; Warning This diagram is slightly misleading because the \"Has Errors?\" check also happens while the execute step is still running . Therefore, you can catch errors before your program even finishes & exits! Running S3Workflows is the same as normal workflows (e.g. using the run method), and this entire process of supervising, staging, and shell execution is done for you!","title":"S3 = Supervised + Staged + Shell"},{"location":"full_guides/workflows/s3_workflows/#s3workflows-for-common-calculators","text":"For programs that are commonly used in material science, you should also read through their guides in the \"Third-party Software\" section. If your program is listed there, then there is likely a subclass of S3Workflow already built for you. For example, VASP user can take advantage of VaspWorkflow to build workflows.","title":"S3Workflows for common Calculators"},{"location":"full_guides/workflows/s3_workflows/#building-a-custom-s3workflow","text":"Tip Before starting a custom S3Workflow , make sure you have read the section above this (on S3Workflows for common Calculators like VASP). You should also have gone through the guides on building a custom Workflow .","title":"Building a custom S3Workflow"},{"location":"full_guides/workflows/s3_workflows/#simple-command-call","text":"The most basic example of a S3Workflow is just calling some command -- without doing anything else (no input files, no error handling, etc.). Unlike custom Workflows were we defined a run_config method, S3Workflows have a pre-built run_config method that carries out the different stages and monitoring of a workflow for us. So all the work is already done for us! As an example, let's just use the command echo to print something: from simmate.workflow_engine import S3Workflow class Example__Echo__SayHello ( S3Workflow ): use_database = False # we aren't using a custom table for now monitor = False # there is no error handling yet command = \"echo Hello\" # behaves like a normal workflow state = Example__Echo__SayHello . run () result = state . result () Tip Note that we used \"Echo\" in our workflow name. This helps the user see what commands or programs will be called when a workflow is ran.","title":"Simple command call"},{"location":"full_guides/workflows/s3_workflows/#custom-setup-and-workup","text":"Now what if we'd like to write input files or read output files that are created? Here, we need to update our setup and workup methods: from simmate.workflow_engine import S3Workflow class Example__Echo__SayHello ( S3Workflow ): use_database = False # we aren't using a custom table for now monitor = False # there is no error handling yet command = \"echo Hello > output.txt\" # adds \"Hello\" into a new file @classmethod def setup ( cls , directory , custom_parameter , ** kwargs ): # The directory given is a pathlib.Path object for the directory # that the command will be called in print ( \"I'm setting things up!\" ) print ( f \"My new setting value is { cls . some_new_setting } \" ) print ( f \"My new parameter value is { custom_parmeter } \" ) return # no need to return anything. Nothing will be done with it. @staticmethod def workup ( directory ): # The directory given is a pathlib.Path object for the directory # that the command will be called in # Simply check that we have a new file output_file = directory / \"output.txt\" assert output_file . exists () print ( \"I'm working things up!\" ) return \"Done!\" task = Example__Echo__SayHello () result = task . run () There are a two important things to note here: It's optional to write new setup or workup methods. But if you do... Both setup and workup method should be either a staticmethod or classmethod Custom setup methods require the directory and **kwargs input parameters. Custom workup methods require the directory input paramter It's optional to set/overwrite attributes. You can also add new ones too. Note: S3Workflows for a custom calculator (such VaspWorkflow for VASP) will often have custom setup and workup methods already defined for you. You can update/override these as you see fit. For a full (and advanced) example of a subclass take a look at simmate.calculators.vasp.workflows.base.VaspWorkflow and the tasks that use it like simmate.calculators.vasp.workflows.relaxation.matproj .","title":"Custom setup and workup"},{"location":"full_guides/workflows/s3_workflows/#custom-error-handling","text":"TODO -- Contact our team if you would like us to prioritize this guide","title":"Custom error handling"},{"location":"full_guides/workflows/s3_workflows/#alternatives-to-the-s3workflow","text":"For experts, this class can be viewed as a combination of prefect's ShellTask, a custodian Job, and Custodian monitoring. When subclassing this, we can absorb functionality of pymatgen.io.vasp.sets too. By merging all of these together into one class, we make things much easier for users and creating new Tasks.","title":"Alternatives to the S3Workflow"},{"location":"full_guides/workflows/using_existing_workflows/","text":"Using existing workflows \u00b6 Exploring available workflows \u00b6 All registered workflows can be listed off in the command-line. simmate workflows list-all You can also learn more about a workflow with the explore command: simmate workflows explore Tip Read through the \"Workflow Names\" section for a better understanding of the different workflows available. Loading a workflow \u00b6 Once you have a desired workflow name, you can load the workflow with: yaml python # in example.yaml workflow_name : static-energy.vasp.matproj from simmate.workflows.utilities import get_workflow workflow = get_workflow ( \"static-energy.vasp.matproj\" ) Viewing parameters & options \u00b6 Because parameters are super important for using Simmate, we gave them their own section in our documentation . Make sure you read through that section of our documentation to view full parameter descriptions and examples for each. Running a workflow (local) \u00b6 To run a workflow locally (i.e. directly on your current computer), you can use the run method. As a quick example: command line python # in example.yaml workflow_name : static-energy.vasp.matproj structure : NaCl.cif command : mpirun -n 4 vasp_std > vasp.out simmate workflows run example.yaml from simmate.workflows.utilities import get_workflow workflow = get_workflow ( \"static-energy.vasp.matproj\" ) state = workflow . run ( structure = \"NaCl.cif\" , command = \"mpirun -n 4 vasp_std > vasp.out\" , ) Running a workflow (cloud) \u00b6 Workflows can also be submitted to a remote cluster. It is important to understand how local and cloud runs are different: local (run) remote submission (run-cloud) graph TD A[submit with 'run' command] --> B[starts directly on your local computer & right away]; graph TD A[submit with 'run-cloud' command] --> B[adds job to scheduler queue]; B --> C[waits for a worker to pick up job]; C --> D[worker selects job from queue]; D --> E[runs the job where the worker is]; F[launch a worker with 'start-worker' command] --> D; Therefore, when you want to schedule a workflow to run elsewhere, you must first make sure you have your computational resources configured. You can then run workflows using the run_cloud method: command line python # in example.yaml workflow_name : static-energy.vasp.matproj structure : NaCl.cif command : mpirun -n 4 vasp_std > vasp.out simmate workflows run-cloud example.yaml from simmate.workflows.utilities import get_workflow workflow = get_workflow ( \"static-energy.vasp.matproj\" ) state = workflow . run_cloud ( structure = \"NaCl.cif\" , command = \"mpirun -n 4 vasp_std > vasp.out\" , ) Warning The run-cloud command/method only schedules the workflow. It won't run until you add computational resources (or Workers ). To do this, you must read through the \"Computational Resources\" documentation. Accessing results \u00b6 There are several ways to view the results of a workflow run, and some approaches are better than others. Tip If you'd like to run many workflows and get the results in an excel spreedsheet, then go with option 3! Option 1: output files \u00b6 Simply go to the directory that the calculation ran in, and you may notice a few extra files in your output. One of them is simmate_summary.yaml , which contains some quick information for you. Simmate can also catch errors, correct them, and retry a calculation. If this occurred during your workflow run, you'll see a file named simmate_corrections.csv with all the errors that were incountered and how they were fixed. Other workflows will also write out plots for you. For example, electronic-structure workflows will calculate a band structure using Materials Project settings, and write an image of your final band structure to band_structure.png . These extra files and plots vary for each workflow, but they make checking your results nice and quick. Tip While the plots and summary files are nice for quick viewing, there is much more information available in the database. Furthermore, you can also use python toolkit objects to run a custom analysis. These are covered in the next two sections. Option 2: python objects \u00b6 When running a workflow in python, a State object is returned. From this, you can access the results as toolkit objects. States allows you to check if the run completed successfully or not. Then final output of your workflow run can be accessed using state.result() . The State is based off of Prefect's state object, which you can read more about here . We use State s because the status of a run becomes important when we start scheduling runs to run remotely, and more importantly, it allows use to building in compatibility with other workflow engines like Prefect. python (local) python (cloud) state = workflow . run ( ... ) result = state . result () state = workflow . run_cloud ( ... ) # This will block and wait for the job to finish result = state . result () Tip This approach is best for users comfortable with python. If you want to use these features, we recommend reading through the Toolkit guides. Option 3: the database \u00b6 You can also view results database through the database_table attribute (if one is available). This returns a Simmate database object for results of ALL runs of this workflow. But as an example: python table = workflow . database_table # pandas dataframe that you can view in Spyder df = table . objects . to_dataframe () # or grab a specific run result and convert to a toolkit object entry = table . objects . get ( run_id = \"example-123456\" ) structure = entry . to_toolkit () You'll notice the table gives results for all runs of this type (e.g. all static-energies). To limit your results to just this specific workflow, you can use the all_results property: python results = workflow . all_results # the line above is a shortcut for... table = workflow . database_table results = table . objects . filter ( workflow_name = workflow . name_full ) Tip Guides for filtering and manulipating the data in this table is covered in the Database guides. Option 4: the website server \u00b6 In the simmate_summary.yaml output file, there is the _WEBSITE_URL_ . You can copy/paste this URL into your browser and view your results in an interactive format. Just make sure you are running your local server first: simmate run-server Then open the link given by _WEBSITE_URL_ : http://127.0.0.1:8000/workflows/static-energy/vasp/mit/1 Note Remember that the server and your database are limited to your local computer. Trying to access a URL on a computer that doesn't share the same database file will not work -- so you may need to copy your database file from the cluster to your local computer. Or even better -- if you would like to access results through the internet, then you have to switch to a cloud database. Massively parallel workflows \u00b6 Some workflows submit many subworkflows. For example, evolutionary structure prediction does this by submitting hundreds of individual structure relaxations, analyzing the results, and submitting new structures based on the results. This is achieved by the workflow manually calling run-cloud on others ( see section above on how run-cloud works ). If you start multiple workers elsewhere, you can calculate these subworkflows in parallel: graph TD A[main workflow]; A --> B[subworkflow]; B --> C[schedule run 1] --> G[scheduler]; B --> D[schedule run 2] --> G; B --> E[schedule run 3] --> G; B --> F[schedule run 4] --> G; G --> H[worker 1]; G --> I[worker 2]; G --> J[worker 3]; So in order to run these types of workflows, you must... Start the main workflow with the run command Start at least one worker that will run the submitted the calculations Tip Make sure you read through the \"Computational Resources\" documentation. There is also a full walk-through example of a massively-parallel workflow in the getting started guides. Note The number of workers will be how many jobs are ran in parallel -- and this is only limited by the number of jobs queued. For example, if I submit 500 workflows with run-cloud but only start 100 workers, then only 100 workflows will be ran at a time. Further, if I submit 25 workflows but have 100 workers, then that means 75 of our workflows will be sitting idle without any job to run.","title":"Using existing workflows"},{"location":"full_guides/workflows/using_existing_workflows/#using-existing-workflows","text":"","title":"Using existing workflows"},{"location":"full_guides/workflows/using_existing_workflows/#exploring-available-workflows","text":"All registered workflows can be listed off in the command-line. simmate workflows list-all You can also learn more about a workflow with the explore command: simmate workflows explore Tip Read through the \"Workflow Names\" section for a better understanding of the different workflows available.","title":"Exploring available workflows"},{"location":"full_guides/workflows/using_existing_workflows/#loading-a-workflow","text":"Once you have a desired workflow name, you can load the workflow with: yaml python # in example.yaml workflow_name : static-energy.vasp.matproj from simmate.workflows.utilities import get_workflow workflow = get_workflow ( \"static-energy.vasp.matproj\" )","title":"Loading a workflow"},{"location":"full_guides/workflows/using_existing_workflows/#viewing-parameters-options","text":"Because parameters are super important for using Simmate, we gave them their own section in our documentation . Make sure you read through that section of our documentation to view full parameter descriptions and examples for each.","title":"Viewing parameters &amp; options"},{"location":"full_guides/workflows/using_existing_workflows/#running-a-workflow-local","text":"To run a workflow locally (i.e. directly on your current computer), you can use the run method. As a quick example: command line python # in example.yaml workflow_name : static-energy.vasp.matproj structure : NaCl.cif command : mpirun -n 4 vasp_std > vasp.out simmate workflows run example.yaml from simmate.workflows.utilities import get_workflow workflow = get_workflow ( \"static-energy.vasp.matproj\" ) state = workflow . run ( structure = \"NaCl.cif\" , command = \"mpirun -n 4 vasp_std > vasp.out\" , )","title":"Running a workflow (local)"},{"location":"full_guides/workflows/using_existing_workflows/#running-a-workflow-cloud","text":"Workflows can also be submitted to a remote cluster. It is important to understand how local and cloud runs are different: local (run) remote submission (run-cloud) graph TD A[submit with 'run' command] --> B[starts directly on your local computer & right away]; graph TD A[submit with 'run-cloud' command] --> B[adds job to scheduler queue]; B --> C[waits for a worker to pick up job]; C --> D[worker selects job from queue]; D --> E[runs the job where the worker is]; F[launch a worker with 'start-worker' command] --> D; Therefore, when you want to schedule a workflow to run elsewhere, you must first make sure you have your computational resources configured. You can then run workflows using the run_cloud method: command line python # in example.yaml workflow_name : static-energy.vasp.matproj structure : NaCl.cif command : mpirun -n 4 vasp_std > vasp.out simmate workflows run-cloud example.yaml from simmate.workflows.utilities import get_workflow workflow = get_workflow ( \"static-energy.vasp.matproj\" ) state = workflow . run_cloud ( structure = \"NaCl.cif\" , command = \"mpirun -n 4 vasp_std > vasp.out\" , ) Warning The run-cloud command/method only schedules the workflow. It won't run until you add computational resources (or Workers ). To do this, you must read through the \"Computational Resources\" documentation.","title":"Running a workflow (cloud)"},{"location":"full_guides/workflows/using_existing_workflows/#accessing-results","text":"There are several ways to view the results of a workflow run, and some approaches are better than others. Tip If you'd like to run many workflows and get the results in an excel spreedsheet, then go with option 3!","title":"Accessing results"},{"location":"full_guides/workflows/using_existing_workflows/#option-1-output-files","text":"Simply go to the directory that the calculation ran in, and you may notice a few extra files in your output. One of them is simmate_summary.yaml , which contains some quick information for you. Simmate can also catch errors, correct them, and retry a calculation. If this occurred during your workflow run, you'll see a file named simmate_corrections.csv with all the errors that were incountered and how they were fixed. Other workflows will also write out plots for you. For example, electronic-structure workflows will calculate a band structure using Materials Project settings, and write an image of your final band structure to band_structure.png . These extra files and plots vary for each workflow, but they make checking your results nice and quick. Tip While the plots and summary files are nice for quick viewing, there is much more information available in the database. Furthermore, you can also use python toolkit objects to run a custom analysis. These are covered in the next two sections.","title":"Option 1: output files"},{"location":"full_guides/workflows/using_existing_workflows/#option-2-python-objects","text":"When running a workflow in python, a State object is returned. From this, you can access the results as toolkit objects. States allows you to check if the run completed successfully or not. Then final output of your workflow run can be accessed using state.result() . The State is based off of Prefect's state object, which you can read more about here . We use State s because the status of a run becomes important when we start scheduling runs to run remotely, and more importantly, it allows use to building in compatibility with other workflow engines like Prefect. python (local) python (cloud) state = workflow . run ( ... ) result = state . result () state = workflow . run_cloud ( ... ) # This will block and wait for the job to finish result = state . result () Tip This approach is best for users comfortable with python. If you want to use these features, we recommend reading through the Toolkit guides.","title":"Option 2: python objects"},{"location":"full_guides/workflows/using_existing_workflows/#option-3-the-database","text":"You can also view results database through the database_table attribute (if one is available). This returns a Simmate database object for results of ALL runs of this workflow. But as an example: python table = workflow . database_table # pandas dataframe that you can view in Spyder df = table . objects . to_dataframe () # or grab a specific run result and convert to a toolkit object entry = table . objects . get ( run_id = \"example-123456\" ) structure = entry . to_toolkit () You'll notice the table gives results for all runs of this type (e.g. all static-energies). To limit your results to just this specific workflow, you can use the all_results property: python results = workflow . all_results # the line above is a shortcut for... table = workflow . database_table results = table . objects . filter ( workflow_name = workflow . name_full ) Tip Guides for filtering and manulipating the data in this table is covered in the Database guides.","title":"Option 3: the database"},{"location":"full_guides/workflows/using_existing_workflows/#option-4-the-website-server","text":"In the simmate_summary.yaml output file, there is the _WEBSITE_URL_ . You can copy/paste this URL into your browser and view your results in an interactive format. Just make sure you are running your local server first: simmate run-server Then open the link given by _WEBSITE_URL_ : http://127.0.0.1:8000/workflows/static-energy/vasp/mit/1 Note Remember that the server and your database are limited to your local computer. Trying to access a URL on a computer that doesn't share the same database file will not work -- so you may need to copy your database file from the cluster to your local computer. Or even better -- if you would like to access results through the internet, then you have to switch to a cloud database.","title":"Option 4: the website server"},{"location":"full_guides/workflows/using_existing_workflows/#massively-parallel-workflows","text":"Some workflows submit many subworkflows. For example, evolutionary structure prediction does this by submitting hundreds of individual structure relaxations, analyzing the results, and submitting new structures based on the results. This is achieved by the workflow manually calling run-cloud on others ( see section above on how run-cloud works ). If you start multiple workers elsewhere, you can calculate these subworkflows in parallel: graph TD A[main workflow]; A --> B[subworkflow]; B --> C[schedule run 1] --> G[scheduler]; B --> D[schedule run 2] --> G; B --> E[schedule run 3] --> G; B --> F[schedule run 4] --> G; G --> H[worker 1]; G --> I[worker 2]; G --> J[worker 3]; So in order to run these types of workflows, you must... Start the main workflow with the run command Start at least one worker that will run the submitted the calculations Tip Make sure you read through the \"Computational Resources\" documentation. There is also a full walk-through example of a massively-parallel workflow in the getting started guides. Note The number of workers will be how many jobs are ran in parallel -- and this is only limited by the number of jobs queued. For example, if I submit 500 workflows with run-cloud but only start 100 workers, then only 100 workflows will be ran at a time. Further, if I submit 25 workflows but have 100 workers, then that means 75 of our workflows will be sitting idle without any job to run.","title":"Massively parallel workflows"},{"location":"full_guides/workflows/using_prefect/","text":"Prefect backend (experimental) \u00b6 When you enable Prefect as your workflow executor, Workflows are converted into Prefect Flows under the hood, so having knowledge of Prefect can be useful in advanced cases. For advanced use or when building new features, we recommend going through the prefect tutorials located here . Minimal example (Prefect vs. Simmate) \u00b6 It's useful to know how Prefect workflows compare to Simmate workflows. For simple cases where you have python code, you'd define a Prefect workflow like so: from prefect import flow @flow def my_favorite_workflow (): print ( \"This workflow doesn't do much\" ) return 42 # and then run your workflow state = my_favorite_workflow () result = state . result () To convert this to a Simmate workflow, we just need to change the format a little. Instead of a @flow decorator, we use the run_config method of a new subclass: # NOTE: this example does not follow Simmate's naming convention, so # some higher level features will be broken. We will fix in a later step. from simmate.workflow_engine import Workflow class Example__Python__MyFavoriteSettings ( Workflow ): @staticmethod def run_config ( ** kwargs ): print ( \"This workflow doesn't do much\" ) return 42 # and then run your workflow state = MyFavoriteWorkflow . run () result = state . result () Behind the scenes, the run method is converting our run_config to a Prefect workflow for us. Methods like run_cloud will automatically use Prefect now too.","title":"Prefect backend (experimental)"},{"location":"full_guides/workflows/using_prefect/#prefect-backend-experimental","text":"When you enable Prefect as your workflow executor, Workflows are converted into Prefect Flows under the hood, so having knowledge of Prefect can be useful in advanced cases. For advanced use or when building new features, we recommend going through the prefect tutorials located here .","title":"Prefect backend (experimental)"},{"location":"full_guides/workflows/using_prefect/#minimal-example-prefect-vs-simmate","text":"It's useful to know how Prefect workflows compare to Simmate workflows. For simple cases where you have python code, you'd define a Prefect workflow like so: from prefect import flow @flow def my_favorite_workflow (): print ( \"This workflow doesn't do much\" ) return 42 # and then run your workflow state = my_favorite_workflow () result = state . result () To convert this to a Simmate workflow, we just need to change the format a little. Instead of a @flow decorator, we use the run_config method of a new subclass: # NOTE: this example does not follow Simmate's naming convention, so # some higher level features will be broken. We will fix in a later step. from simmate.workflow_engine import Workflow class Example__Python__MyFavoriteSettings ( Workflow ): @staticmethod def run_config ( ** kwargs ): print ( \"This workflow doesn't do much\" ) return 42 # and then run your workflow state = MyFavoriteWorkflow . run () result = state . result () Behind the scenes, the run method is converting our run_config to a Prefect workflow for us. Methods like run_cloud will automatically use Prefect now too.","title":"Minimal example (Prefect vs. Simmate)"},{"location":"full_guides/workflows/workflow_names/","text":"Workflow naming conventions \u00b6 How we name workflows \u00b6 All workflow names follow a specific format of type.calculator.preset : type : the type of analysis the workflow is doing (relaxation, static-energy, dynamics, ...) calculator : the third party software that the workflow uses to run (vasp, abinit, qe, deepmd, ...) preset : a unique name to identify the settings used (matproj, quality00, my-test-settings,...) Example Using the workflow like static-energy.vasp.matproj this means that... type = static energy (runs a single point energy) calculator = vasp (uses VASP to calculate the energy) preset = matproj (uses \"Materials Project\" settings) Class name vs. mini name \u00b6 type.calculator.preset is what we see in most cases, but in python, the workflow class name translates to Type__Calculator__Preset . All workflows follow this format. Example static-energy.vasp.matproj --> StaticEnergy__VASP__MatProj Note, when converting a workflow name in python, we need to replace periods with 2 underscores each ( __ ) and convert our phrases to pascal case . Hyphen ( - ) placement is based off of capital letters. Location of a workflow in the website interface \u00b6 You can follow the naming conventions (described above) to find a workflow in the website interface: Template URL Example https://simmate.org/workflows/{TYPE}/{CALCULATOR}/{PRESET} https://simmate.org/workflows/static-energy/vasp/matproj Location of a workflow's source code \u00b6 The code that defines these workflows and configures their settings are located in the corresponding simmate.calculators module. We make workflows accessible here because users often want to search for workflows by application -- not by their calculator name. The source code of a workflow can be found using the naming convention for workflows described above: Template Python Import Example from simmate.calculators. { CALCULATOR } . workflows . { TYPE } . { PRESET } import { FULL_NAME } from simmate.calculators.vasp.workflows.static_energy.matproj import StaticEnergy__Vasp__Matproj This is a really long import, but it gives the same workflow as the get_workflow utility. We recommend sticking to get_workflow because it is the most convienent and easiest to remember. The only reason you'll need to interact with this longer import is to either: Find the source code for a workflow Optimize the speed of your imports (advanced users only)","title":"Workflow Names"},{"location":"full_guides/workflows/workflow_names/#workflow-naming-conventions","text":"","title":"Workflow naming conventions"},{"location":"full_guides/workflows/workflow_names/#how-we-name-workflows","text":"All workflow names follow a specific format of type.calculator.preset : type : the type of analysis the workflow is doing (relaxation, static-energy, dynamics, ...) calculator : the third party software that the workflow uses to run (vasp, abinit, qe, deepmd, ...) preset : a unique name to identify the settings used (matproj, quality00, my-test-settings,...) Example Using the workflow like static-energy.vasp.matproj this means that... type = static energy (runs a single point energy) calculator = vasp (uses VASP to calculate the energy) preset = matproj (uses \"Materials Project\" settings)","title":"How we name workflows"},{"location":"full_guides/workflows/workflow_names/#class-name-vs-mini-name","text":"type.calculator.preset is what we see in most cases, but in python, the workflow class name translates to Type__Calculator__Preset . All workflows follow this format. Example static-energy.vasp.matproj --> StaticEnergy__VASP__MatProj Note, when converting a workflow name in python, we need to replace periods with 2 underscores each ( __ ) and convert our phrases to pascal case . Hyphen ( - ) placement is based off of capital letters.","title":"Class name vs. mini name"},{"location":"full_guides/workflows/workflow_names/#location-of-a-workflow-in-the-website-interface","text":"You can follow the naming conventions (described above) to find a workflow in the website interface: Template URL Example https://simmate.org/workflows/{TYPE}/{CALCULATOR}/{PRESET} https://simmate.org/workflows/static-energy/vasp/matproj","title":"Location of a workflow in the website interface"},{"location":"full_guides/workflows/workflow_names/#location-of-a-workflows-source-code","text":"The code that defines these workflows and configures their settings are located in the corresponding simmate.calculators module. We make workflows accessible here because users often want to search for workflows by application -- not by their calculator name. The source code of a workflow can be found using the naming convention for workflows described above: Template Python Import Example from simmate.calculators. { CALCULATOR } . workflows . { TYPE } . { PRESET } import { FULL_NAME } from simmate.calculators.vasp.workflows.static_energy.matproj import StaticEnergy__Vasp__Matproj This is a really long import, but it gives the same workflow as the get_workflow utility. We recommend sticking to get_workflow because it is the most convienent and easiest to remember. The only reason you'll need to interact with this longer import is to either: Find the source code for a workflow Optimize the speed of your imports (advanced users only)","title":"Location of a workflow's source code"},{"location":"full_guides/workflows/adding_computational_resources/overview/","text":"This module represents an alternative to Prefect. It is meant to be a stable quick-start alternative, but lacks the scaling and numerous features that Prefect offers. This is an SQL executor that intends to be a stripped down version of FireWorks and Prefect. The scheduler is directly built into the django database, which makes it so you don't have to deal with firewalls or complex setups -- any worker that can connect to the database will work just fine. The downside is that the executor is slower (bc each task requires multiple database calls and also writing to the database). It's a trade off of speed for stability, but this is okay because many workflows in Simmate are >1min and the speed penality is well below 1 second. Example usage: from simmate.workflow_engine.execution.executor import SimmateExecutor # EXAMPLE 1 future = SimmateExecutor . submit ( sum , [ 4 , 3 , 2 , 1 ]) assert future . result () == 10 # EXAMPLE 2 import time def test (): futures = [ executor . submit ( time . sleep , 5 ) for n in range ( 10 )] return executor . wait ( futures ) test () # ---------------------------------------------------------------------------- from simmate.workflow_engine.execution.worker import SimmateWorker worker = SimmateWorker ( waittime_on_empty_queue = 1 , tags = []) # nitems_max=1 worker . start () # ----------------------------------------------------------------------------","title":"Overview"},{"location":"full_guides/workflows/third_party_software/deepmd/","text":"The DeePMD Calculator \u00b6 The Deep Potential for Molecular Dynamics (DeePMD) is a package that builds machine-learned interatomic potentials from energy and force-field data. These machine-learned potentials are particularlly powerful when trained on DFT data and then subsequently used to run molecular dynamics simulations. The code is free and open-source. Helpful links \u00b6 github documentation paper","title":"DeepMD"},{"location":"full_guides/workflows/third_party_software/deepmd/#the-deepmd-calculator","text":"The Deep Potential for Molecular Dynamics (DeePMD) is a package that builds machine-learned interatomic potentials from energy and force-field data. These machine-learned potentials are particularlly powerful when trained on DFT data and then subsequently used to run molecular dynamics simulations. The code is free and open-source.","title":"The DeePMD Calculator"},{"location":"full_guides/workflows/third_party_software/deepmd/#helpful-links","text":"github documentation paper","title":"Helpful links"},{"location":"full_guides/workflows/third_party_software/what_is_a_calculator/","text":"What is a calculator? \u00b6 Calculators are external codes/programs that perform some analysis for us. For example, VASP is a program that can run a variety of density functional theory (DFT) calculations. But because it isn't written in Python, we need some \"helper\" code here to call VASP commands, make input files, and pull data from the outputs. Organization of code \u00b6 All calculators have the same folder structure: \u251c\u2500\u2500 example_calculator \u2502 \u251c\u2500\u2500 database \u2502 \u251c\u2500\u2500 error_handlers \u2502 \u251c\u2500\u2500 inputs \u2502 \u251c\u2500\u2500 outputs \u2502 \u251c\u2500\u2500 configuration \u2502 \u251c\u2500\u2500 tasks \u2502 \u251c\u2500\u2500 website \u2502 \u2514\u2500\u2500 workflows In a more logical order (rather than alphabetical like above), here is what each module contains: configuration = helps to install the program and setup common settings for it inputs & outputs = automatically generate files as well as load their data into python error_handlers = help correct common errors in calculations that cause the program to fail tasks = how the program is actually setup, executed, and worked-up. It ties together all the inputs , outputs , and error-handler functions into one. A single task can be viewed as a single call to the program (i.e. a single calculation). database = holds all of the datatables for storing our results workflows = brings together tasks and database , so these setup individual tasks and handle saving the results to our database website = lets us submit workflows and view results with our website interface Note Beginners should start by looking at the workflows module as this ties all other modules together. Advanced users can start with each calculator's tasks to build your own custom workflow.","title":"What is a calculator?"},{"location":"full_guides/workflows/third_party_software/what_is_a_calculator/#what-is-a-calculator","text":"Calculators are external codes/programs that perform some analysis for us. For example, VASP is a program that can run a variety of density functional theory (DFT) calculations. But because it isn't written in Python, we need some \"helper\" code here to call VASP commands, make input files, and pull data from the outputs.","title":"What is a calculator?"},{"location":"full_guides/workflows/third_party_software/what_is_a_calculator/#organization-of-code","text":"All calculators have the same folder structure: \u251c\u2500\u2500 example_calculator \u2502 \u251c\u2500\u2500 database \u2502 \u251c\u2500\u2500 error_handlers \u2502 \u251c\u2500\u2500 inputs \u2502 \u251c\u2500\u2500 outputs \u2502 \u251c\u2500\u2500 configuration \u2502 \u251c\u2500\u2500 tasks \u2502 \u251c\u2500\u2500 website \u2502 \u2514\u2500\u2500 workflows In a more logical order (rather than alphabetical like above), here is what each module contains: configuration = helps to install the program and setup common settings for it inputs & outputs = automatically generate files as well as load their data into python error_handlers = help correct common errors in calculations that cause the program to fail tasks = how the program is actually setup, executed, and worked-up. It ties together all the inputs , outputs , and error-handler functions into one. A single task can be viewed as a single call to the program (i.e. a single calculation). database = holds all of the datatables for storing our results workflows = brings together tasks and database , so these setup individual tasks and handle saving the results to our database website = lets us submit workflows and view results with our website interface Note Beginners should start by looking at the workflows module as this ties all other modules together. Advanced users can start with each calculator's tasks to build your own custom workflow.","title":"Organization of code"},{"location":"full_guides/workflows/third_party_software/bader_henkelman/installation/","text":"Installation (Ubuntu 22.04) \u00b6 Both sections are required for use of Simmate workflows. For the bader command... 1. Download Linux x86-64 from the Henkelman website here 2. Unpack the compressed file. There should only be one \"file\" in it named bader. This is the executable. 3. Move the bader executable to a folder of your choosing. For example, ~/jacksund/bader/bader (within a folder named bader in my home directory) 4. run nano ~./bashrc to edit your bash and add this line to the bottom: export PATH = /home/jacksund/bader/: $PATH 5. restart your terminal and try the command bader --help For the chgsum.pl and extra scripts 1. Download the scripts from VTST-tools 2. Unpack the folder ( vtstscripts-1021 ) and move it into the folder with your bader executable 3. run nano ~./bashrc to edit your bash and add this line to the bottom: export PATH = /home/jacksund/bader/vtstscripts-1021: $PATH 4. restart your terminal and you're ready to try Bader analyses with Simmate!","title":"Installation"},{"location":"full_guides/workflows/third_party_software/bader_henkelman/installation/#installation-ubuntu-2204","text":"Both sections are required for use of Simmate workflows. For the bader command... 1. Download Linux x86-64 from the Henkelman website here 2. Unpack the compressed file. There should only be one \"file\" in it named bader. This is the executable. 3. Move the bader executable to a folder of your choosing. For example, ~/jacksund/bader/bader (within a folder named bader in my home directory) 4. run nano ~./bashrc to edit your bash and add this line to the bottom: export PATH = /home/jacksund/bader/: $PATH 5. restart your terminal and try the command bader --help For the chgsum.pl and extra scripts 1. Download the scripts from VTST-tools 2. Unpack the folder ( vtstscripts-1021 ) and move it into the folder with your bader executable 3. run nano ~./bashrc to edit your bash and add this line to the bottom: export PATH = /home/jacksund/bader/vtstscripts-1021: $PATH 4. restart your terminal and you're ready to try Bader analyses with Simmate!","title":"Installation (Ubuntu 22.04)"},{"location":"full_guides/workflows/third_party_software/bader_henkelman/overview/","text":"The Bader Calculator \u00b6 Bader Chager Analysis (or \"Bader\" for short) is a process of partitioning charge density in order to predict oxidation states. This module is specifically for the Henkelman Group 's code that implements this analysis. The code is free to everyone and can be downloaded here . Helpful links \u00b6 website (includes docs + guide)","title":"Overview"},{"location":"full_guides/workflows/third_party_software/bader_henkelman/overview/#the-bader-calculator","text":"Bader Chager Analysis (or \"Bader\" for short) is a process of partitioning charge density in order to predict oxidation states. This module is specifically for the Henkelman Group 's code that implements this analysis. The code is free to everyone and can be downloaded here .","title":"The Bader Calculator"},{"location":"full_guides/workflows/third_party_software/bader_henkelman/overview/#helpful-links","text":"website (includes docs + guide)","title":"Helpful links"},{"location":"full_guides/workflows/third_party_software/vasp/error_handlers/","text":"VASP Error Hanlders \u00b6 This module defines error handlers to help address issues during VASP workflow runs. This module is a fork and refactor of error handlers used by Custodian . Specifically, this is a direct alternative to the custodian.vasp.handlers module. One key difference is that we break up our error handlers into smaller handlers because this allows us to more-easily see the error/fix identified.","title":"Error handlers"},{"location":"full_guides/workflows/third_party_software/vasp/error_handlers/#vasp-error-hanlders","text":"This module defines error handlers to help address issues during VASP workflow runs. This module is a fork and refactor of error handlers used by Custodian . Specifically, this is a direct alternative to the custodian.vasp.handlers module. One key difference is that we break up our error handlers into smaller handlers because this allows us to more-easily see the error/fix identified.","title":"VASP Error Hanlders"},{"location":"full_guides/workflows/third_party_software/vasp/inputs/","text":"VASP Input Files \u00b6 This module helps with the creation and reading of VASP input files. This module is a fork and refactor of classes used by PyMatGen . Specifically, this is a direct alternative to the pymatgen.io.vasp.inputs module.","title":"Inputs"},{"location":"full_guides/workflows/third_party_software/vasp/inputs/#vasp-input-files","text":"This module helps with the creation and reading of VASP input files. This module is a fork and refactor of classes used by PyMatGen . Specifically, this is a direct alternative to the pymatgen.io.vasp.inputs module.","title":"VASP Input Files"},{"location":"full_guides/workflows/third_party_software/vasp/installation/","text":"VASP Installation \u00b6 Official guides can be found on the VASP wiki here . For example, there are clear guides for installing VASP onto your personal computer, such as VASP 6 on Ubuntu 22.04. VASP 5.4.4 on Ubuntu 22.04 \u00b6 This guide is specifically for the Warren Lab as it requires build files that we share within the team. To guarantee compatibility, we need to build all vasp dependencies by hand (for example, Ubuntu uses gcc v11 but vasp requires v9). For the Warren Lab, we have packaged everything in one zip file to make setup as simple as possible. Copy /media/synology/software/vasp/vasp.zip from WarWulf to your computer, such as your home directory (e.g. /home/jacksund/ ). This file is only 172.1MB, but will be over 9GB once we are finished installing vasp. Within the vasp directory, there are folders from (1) to (9) for each build step. These are the 9 programs that need to be built from source. Source files ( .tar.gz) are provided in each directory. Each folder also contains an \"install\" directory where the program will be installed. You'll also find another folder (usually the program name) that contains the unzipped contents of the .tar.gz. Make sure the necessary build tools are installed and up to date: sudo apt update sudo apt upgrade sudo apt install gcc make m4 g++ gfortran install gmp # NOTE: steps are effectively the same with other programs but # we only include comments on this first install # open the folder for the step we are on cd ~/vasp/01_gmp # unzip our build files tar xvzf *.tar.gz # switch into the directory before performing the rest of install steps cd gmp-6.2.1 # update this command with the proper path + username ./configure --prefix = /home/jacksund/vasp/01_gmp/install # PAUSE AND READ OUTPUT OF THIS COMMAND # Look for \"Thread(s) per core and use\" this in the next command lscpu # \u201c2\u201d here is based on our output from the previous command. This value will # be used in building our other packages too make -j 2 make install make check # (optional) to confirm successful install install mpfr cd ~/vasp/02_mpfr tar xvzf *.tar.gz cd mpfr-4.1.0 ./configure --prefix = /home/jacksund/vasp/02_mpfr/install --with-gmp = /home/jacksund/vasp/01_gmp/install make -j 2 make install make check install mpc cd ~/vasp/03_mpc tar xvzf *.tar.gz cd mpc-1.2.1 ./configure --prefix = /home/jacksund/vasp/03_mpc/install --with-gmp = /home/jacksund/vasp/01_gmp/install --with-mpfr = /home/jacksund/vasp/02_mpfr/install make -j 2 make install make check install gcc cd ~/vasp/04_gcc tar xvzf *.tar.gz cd gcc-9.5.0 mkdir build cd build ../configure --prefix = /home/jacksund/vasp/04_gcc/install --with-gmp = /home/jacksund/vasp/01_gmp/install --with-mpfr = /home/jacksund/vasp/02_mpfr/install --with-mpc = /home/jacksund/vasp/03_mpc/install --disable-multilib make -j 2 # this command takes roughly 1hr make install nano ~/.bashrc # ADD TO BOTTOM OF FILE # # export PATH=/home/jacksund/vasp/04_gcc/install/bin:$PATH # export LD_LIBRARY_PATH=/home/jacksund/vasp/04_gcc/install/lib64:$LD_LIBRARY_PATH source ~/.bashrc install openmpi cd ~/vasp/05_openmpi tar xvzf *.tar.gz cd openmpi-4.1.4 mkdir build cd build ../configure --prefix = /home/jacksund/vasp/05_openmpi/install make -j 2 make install nano ~/.bashrc # ADD TO BOTTOM OF FILE # # export PATH=/home/jacksund/vasp/05_openmpi/install/bin:$PATH # export LD_LIBRARY_PATH=/vasp/05_openmpi/install/lib:$LD_LIBRARY_PATH source ~/.bashrc mpirun --help install fftw cd ~/vasp/06_fftw tar xvzf *.tar.gz cd fftw-3.3.10 mkdir build cd build ../configure --prefix = /home/jacksund/vasp/06_fftw/install make -j 2 make install ### Scott didn\u2019t include this command. Typo? nano ~/.bashrc # ADD TO BOTTOM OF FILE # # export PATH=/home/jacksund/vasp/06_fftw/install/bin:$PATH source ~/.bashrc install lapack (blas, cblas, lapacke, lapack) cd ~/vasp/07_lapack tar xvzf *.tar.gz cd lapack-3.10.1 mv make.inc.example make.inc make all mkdir /home/jacksund/vasp/07_lapack/install cp *.a /home/jacksund/vasp/07_lapack/install cd ../install cp librefblas.a libblas.a install scalapack cd ~/vasp/08_scalapack tar xvzf *.tar.gz cd scalapack-2.2.0 mv SLmake.inc.example SLmake.inc nano SLmake.inc # Edit these lines: (leave then uncommented) # # BLASLIB = -L/home/jacksund/vasp/07_lapack/install -lblas # LAPACKLIB = -L/home/jacksund/vasp/07_lapack/install -llapack # LIBS = $(LAPACKLIB) $(BLASLIB) make all cp libscalapack.a ../../07_lapack/install install vasp cd ~/vasp/09_vasp nano makefile.include # Edit these lines: (leave then uncommented) # # LIBDIR = /home/jacksund/vasp/07_lapack/install/ # FFTW ?= /home/jacksund/vasp/06_fftw/install make std nano ~/.bashrc # ADD TO BOTTOM OF FILE # # export PATH=/home/jacksund/vasp/09_vasp/bin/:$PATH source ~/.bashrc You can now use commands like mpirun -n 4 vasp_std !!! If you try this right away, you\u2019ll see the \u201cerror\u201d (vasp fails because no input files are present).. Error reading item 'VCAIMAGES' from file INCAR. Error reading item 'VCAIMAGES' from file INCAR. Error reading item 'VCAIMAGES' from file INCAR. Error reading item 'VCAIMAGES' from file INCAR.","title":"Installation"},{"location":"full_guides/workflows/third_party_software/vasp/installation/#vasp-installation","text":"Official guides can be found on the VASP wiki here . For example, there are clear guides for installing VASP onto your personal computer, such as VASP 6 on Ubuntu 22.04.","title":"VASP Installation"},{"location":"full_guides/workflows/third_party_software/vasp/installation/#vasp-544-on-ubuntu-2204","text":"This guide is specifically for the Warren Lab as it requires build files that we share within the team. To guarantee compatibility, we need to build all vasp dependencies by hand (for example, Ubuntu uses gcc v11 but vasp requires v9). For the Warren Lab, we have packaged everything in one zip file to make setup as simple as possible. Copy /media/synology/software/vasp/vasp.zip from WarWulf to your computer, such as your home directory (e.g. /home/jacksund/ ). This file is only 172.1MB, but will be over 9GB once we are finished installing vasp. Within the vasp directory, there are folders from (1) to (9) for each build step. These are the 9 programs that need to be built from source. Source files ( .tar.gz) are provided in each directory. Each folder also contains an \"install\" directory where the program will be installed. You'll also find another folder (usually the program name) that contains the unzipped contents of the .tar.gz. Make sure the necessary build tools are installed and up to date: sudo apt update sudo apt upgrade sudo apt install gcc make m4 g++ gfortran install gmp # NOTE: steps are effectively the same with other programs but # we only include comments on this first install # open the folder for the step we are on cd ~/vasp/01_gmp # unzip our build files tar xvzf *.tar.gz # switch into the directory before performing the rest of install steps cd gmp-6.2.1 # update this command with the proper path + username ./configure --prefix = /home/jacksund/vasp/01_gmp/install # PAUSE AND READ OUTPUT OF THIS COMMAND # Look for \"Thread(s) per core and use\" this in the next command lscpu # \u201c2\u201d here is based on our output from the previous command. This value will # be used in building our other packages too make -j 2 make install make check # (optional) to confirm successful install install mpfr cd ~/vasp/02_mpfr tar xvzf *.tar.gz cd mpfr-4.1.0 ./configure --prefix = /home/jacksund/vasp/02_mpfr/install --with-gmp = /home/jacksund/vasp/01_gmp/install make -j 2 make install make check install mpc cd ~/vasp/03_mpc tar xvzf *.tar.gz cd mpc-1.2.1 ./configure --prefix = /home/jacksund/vasp/03_mpc/install --with-gmp = /home/jacksund/vasp/01_gmp/install --with-mpfr = /home/jacksund/vasp/02_mpfr/install make -j 2 make install make check install gcc cd ~/vasp/04_gcc tar xvzf *.tar.gz cd gcc-9.5.0 mkdir build cd build ../configure --prefix = /home/jacksund/vasp/04_gcc/install --with-gmp = /home/jacksund/vasp/01_gmp/install --with-mpfr = /home/jacksund/vasp/02_mpfr/install --with-mpc = /home/jacksund/vasp/03_mpc/install --disable-multilib make -j 2 # this command takes roughly 1hr make install nano ~/.bashrc # ADD TO BOTTOM OF FILE # # export PATH=/home/jacksund/vasp/04_gcc/install/bin:$PATH # export LD_LIBRARY_PATH=/home/jacksund/vasp/04_gcc/install/lib64:$LD_LIBRARY_PATH source ~/.bashrc install openmpi cd ~/vasp/05_openmpi tar xvzf *.tar.gz cd openmpi-4.1.4 mkdir build cd build ../configure --prefix = /home/jacksund/vasp/05_openmpi/install make -j 2 make install nano ~/.bashrc # ADD TO BOTTOM OF FILE # # export PATH=/home/jacksund/vasp/05_openmpi/install/bin:$PATH # export LD_LIBRARY_PATH=/vasp/05_openmpi/install/lib:$LD_LIBRARY_PATH source ~/.bashrc mpirun --help install fftw cd ~/vasp/06_fftw tar xvzf *.tar.gz cd fftw-3.3.10 mkdir build cd build ../configure --prefix = /home/jacksund/vasp/06_fftw/install make -j 2 make install ### Scott didn\u2019t include this command. Typo? nano ~/.bashrc # ADD TO BOTTOM OF FILE # # export PATH=/home/jacksund/vasp/06_fftw/install/bin:$PATH source ~/.bashrc install lapack (blas, cblas, lapacke, lapack) cd ~/vasp/07_lapack tar xvzf *.tar.gz cd lapack-3.10.1 mv make.inc.example make.inc make all mkdir /home/jacksund/vasp/07_lapack/install cp *.a /home/jacksund/vasp/07_lapack/install cd ../install cp librefblas.a libblas.a install scalapack cd ~/vasp/08_scalapack tar xvzf *.tar.gz cd scalapack-2.2.0 mv SLmake.inc.example SLmake.inc nano SLmake.inc # Edit these lines: (leave then uncommented) # # BLASLIB = -L/home/jacksund/vasp/07_lapack/install -lblas # LAPACKLIB = -L/home/jacksund/vasp/07_lapack/install -llapack # LIBS = $(LAPACKLIB) $(BLASLIB) make all cp libscalapack.a ../../07_lapack/install install vasp cd ~/vasp/09_vasp nano makefile.include # Edit these lines: (leave then uncommented) # # LIBDIR = /home/jacksund/vasp/07_lapack/install/ # FFTW ?= /home/jacksund/vasp/06_fftw/install make std nano ~/.bashrc # ADD TO BOTTOM OF FILE # # export PATH=/home/jacksund/vasp/09_vasp/bin/:$PATH source ~/.bashrc You can now use commands like mpirun -n 4 vasp_std !!! If you try this right away, you\u2019ll see the \u201cerror\u201d (vasp fails because no input files are present).. Error reading item 'VCAIMAGES' from file INCAR. Error reading item 'VCAIMAGES' from file INCAR. Error reading item 'VCAIMAGES' from file INCAR. Error reading item 'VCAIMAGES' from file INCAR.","title":"VASP 5.4.4 on Ubuntu 22.04"},{"location":"full_guides/workflows/third_party_software/vasp/outputs/","text":"VASP Output Files \u00b6 This module helps with the creation and reading of VASP output files. This module is a fork and refactor of classes used by PyMatGen . Specifically, this is a direct alternative to the pymatgen.io.vasp.outputs module.","title":"Outputs"},{"location":"full_guides/workflows/third_party_software/vasp/outputs/#vasp-output-files","text":"This module helps with the creation and reading of VASP output files. This module is a fork and refactor of classes used by PyMatGen . Specifically, this is a direct alternative to the pymatgen.io.vasp.outputs module.","title":"VASP Output Files"},{"location":"full_guides/workflows/third_party_software/vasp/overview/","text":"The VASP Calculator \u00b6 The Vienna Ab initio Simulation Package (VASP) is a program for atomic scale materials modelling from first principles. The software is proprietary and therefore requires a license for use. Helpful links \u00b6 website documentation tutorials","title":"Overview"},{"location":"full_guides/workflows/third_party_software/vasp/overview/#the-vasp-calculator","text":"The Vienna Ab initio Simulation Package (VASP) is a program for atomic scale materials modelling from first principles. The software is proprietary and therefore requires a license for use.","title":"The VASP Calculator"},{"location":"full_guides/workflows/third_party_software/vasp/overview/#helpful-links","text":"website documentation tutorials","title":"Helpful links"},{"location":"full_guides/workflows/third_party_software/vasp/workflows/","text":"VASP Workflow Library \u00b6 This module defines settings for common VASP workflows. Many workflows can be considered a fork and refactor of classes used by PyMatGen and Atomate . Specifically, this is a direct alternative to the pymatgen.io.vasp.sets module as well as the atomate.vasp.workflows module. Rather than build these tasks from many lower-level functions, we condense these classes down into a single VaspWorkflow class that is easier to interact with.","title":"Workflows"},{"location":"full_guides/workflows/third_party_software/vasp/workflows/#vasp-workflow-library","text":"This module defines settings for common VASP workflows. Many workflows can be considered a fork and refactor of classes used by PyMatGen and Atomate . Specifically, this is a direct alternative to the pymatgen.io.vasp.sets module as well as the atomate.vasp.workflows module. Rather than build these tasks from many lower-level functions, we condense these classes down into a single VaspWorkflow class that is easier to interact with.","title":"VASP Workflow Library"},{"location":"getting_started/_Integrate_with_Prefect_and_Dask/","text":"Integrate with Prefect and Dask \u00b6 We do not recommend this tutorial for users at the moment. This tutorial is for Prefect v1, but much of Simmate now depends on Prefect v2. As we adjust to the new backend, parts of this tutorial may be broken and recommended procedures are subject to change. In this tutorial, you will learn how to run workflows on distributed computational resources -- with full scheduling and monitoring. The quick tutorial The full tutorial A review of concepts Should I set up my own cluster? Setting up your scheduler with Prefect Setting up your cluster with Dask Connecting others to your scheduler For beginners, this will be the most difficult part of setting up Simmate -- but it is entirely optional. Be sure to read the section on Should I set up my own cluster? . There are many ways to set up your resources and caviats to each (especially if you are using university or federal supercomputers). While python experts should be able to learn Prefect and Dask quickly, we strongly urge beginners to get advice from our team. If you struggle to follow along with this tutorial, post a question or email us directly (simmate.team@gmail.com). The quick tutorial \u00b6 prefect , dask , and dask_jobqueue will be already installed for you because they are dependencies of Simmate Be aware that you can share a cloud database without sharing computational resources. This flexibility is very important for many collaborations. Just like with your cloud database, designate a point-person to manage your private computational resources. Everyone else can skip to step 9. Either sign in to Prefect Cloud (recommended) or setup a Prefect Server . Connect to your server with the following steps (this is from Prefect's tutorial ): On Prefect Cloud's homepage, go to User -> Account Settings -> API Keys -> Create An API Key Copy the created key set your Prefect backend with the command prefect backend cloud tell Prefect your key with the command prefect auth login --key example_key_h123j2jfk Register all Simmate workflows with Prefect using the command simmate workflow-engine setup-cloud Test that Prefect is configured properly with the following steps (this will run the workflow locally): run the command prefect agent local start (note that this will run endlessly and submit all workflows in parallel. use crtl+C to stop) in a separate terminal, rerun our workflow from tutorial 2 with run-cloud instead of run (so simmate workflows run-cloud relaxation_mit POSCAR ) Set up your computational resources using Dask (and if needed, Dask JobQueue). There are MANY options for this, which are covered in the simmate.workflow_engine module. Take the time to read the documentation here. But as an example, we'll set up a SLURM cluster and then link it to a Prefect agent. Note, if you want to run workflows with commands like mpirun -n 18 vasp_std > vasp.out , then limit the Dask worker to one core while having the SLURM job request more cores. The resulting python script will look something like this: # -------------------------------------------------------------------------------------- # STEP 1: Configure our Dask Cluster. from dask_jobqueue import SLURMCluster cluster = SLURMCluster ( # # General options scheduler_options = { \"port\" : 8786 }, local_directory = \"~\" , # # # Dask Worker Options cores = 1 , processes = 1 , memory = \"4GB\" , # REQUIRED: Make sure you preload this script! extra = [ \"--preload simmate.configuration.dask.connect_to_database\" ], # # # SLURM Job Options job_cpu = 18 , job_mem = \"50GB\" , job_extra = [ \"--output=slurm-%j.out\" , \"-N 1\" , ], walltime = \"300-00:00:00\" , queue = \"p1\" , # this is the name of the SLURM queue/partition env_extra = [ \"module load vasp;\" ], # our workflow requires the vasp module to be loaded ) # Scale the cluster to the number of SLURM jobs that you'd like cluster . scale ( 10 ) # -------------------------------------------------------------------------------------- # STEP 2: Configure our Prefect Agent and start submitting workflows! from prefect.agent.local import LocalAgent from simmate.configuration.prefect.connect_to_dask import set_default_executor # We want Prefect to use our Dask Cluster to run all of the workflow tasks. To # tell Prefect to do this, we wrote a helper function that ships with Simmate. set_default_executor ( cluster . scheduler . address ) # Start our cluster! Our cluster's name is Warwulf, so we use that here. agent = LocalAgent ( name = \"WarWulf\" , labels = [ \"WarWulf\" ], ) # Now we can start the Prefect Agent which will run and search for jobs and then # submit them to our Dask cluster. agent . start () # NOTE: this line will run endlessly unless you set a timelimit in the LocalAgent above # -------------------------------------------------------------------------------------- Test out your cluster by running simmate workflows run-cloud relaxation_mit POSCAR in a separate terminal (submit this a bunch if you'd like to). If you'd like to limit how many workflows of a given tag (e.g. \"WarWulf\" above) run in parallel, set the concurrency limit in Prefect cloud here . To let others use your cluster, simply add them to your Prefect Cloud and give them an API key. They just need to do the following: set your Prefect backend with the command prefect backend cloud tell Prefect your key with the command prefect auth login --key example_key_h123j2jfk try submitting a workflow with simmate workflows run-cloud relaxation_mit POSCAR The full tutorial \u00b6 A review of concepts \u00b6 Recall from tutorial 2, there are 4 steps to a workflow: - configure : chooses our desired settings for the calculation (such as VASP's INCAR settings) - schedule : decides whether to run the workflow immediately or send off to a job queue (e.g. SLURM, PBS, or remote computers) - execute : writes our input files, runs the calculation (e.g. calling VASP), and checks the results for errors - save : saves the results to our database This tuturial will give an overview of how to modify the schedule and determine which computer execute is called on. Up until now, we have been using the default behavior for these two steps. But now we want to instead do the following: - schedule : submits the workflow to a scheduler queue of many other workflows - execute : run the calculation on a remote cluster A scheduler is something we submit workflows to and controls when to run them. As a bunch of workflows are submitted, our scheduler forms a queue and keeps track of which ones to run next. We will use Prefect as our scheduler. A cluster is a group of computational resources that actually run the workflows. So our scheduler will find whichever workflow should be ran next, and send it to our cluster to run. Clusters are often made up of \"workers\" -- where a worker is just a single resource and it works through one job at a time. For example, if we had a cluster made up of 10 desktop computers, each computer would run a workflow and once finished ask the scheduler for the next workflow to run. At any given time, 10 workflows will be running. We'll use Dask to set up our cluster, whether your resources are on the cloud, a supercomputer, or just simple desktops. Should I set up my own cluster? \u00b6 Before we start... We understand that resource restrictions can be very stingent between labs and companies. So sharing resources is not possible, even in close collaborations. Simmate addresses this issue by making it easy to share a cloud database without sharing computational resources. In other words, you can contribute to a shared database without letting others see/access your computational resources. With that said, each team will likely need to handle their own computational resources, which can be any number of things: - a university or federal HPC cluster with SLURM, PBS, or some other queue system - a single node or even a Kubernetes cluster provided by a commercial service like DigitalOcean, GoogleCloud, etc. - a series of desktop computers that your lab shares - any combination of these resources The easiest way to use these resources is to sign on and run simmate directly on it. When this is done, the workflow runs directly on your resource and it will run there immediately. We've already see this with Tutorial 2 when we called simmate workflows run ... . Just use that on your new resource to start! (for help signing on to remote supercomputers and installing anaconda, be sure to ask the cluster's IT team). As another example, you can submit a workflow to a SLURM cluster with a submit.sh file like this: #!/bin/bash #SBATCH --output=slurm.out #SBATCH --nodes=1 #SBATCH --ntasks=2 # make sure you have you activated your conda enviornment # and required modules before submitting simmate workflows run-cloud relaxation_mit POSCAR > simmate.out If you are only running a few workflows per day (<10), we recommend you stick to running workflows in this way. That is, just calling simmate workflows run . Don't overcomplicate things. Alternatively, if your team is submitting hundreds or thousands of workflows at a time, then it would be extremely useful to monitor and orchestrate these workflows. To do this, we will use Prefect and Dask. Just like with our cloud database in the previous tutorial, you only need ONE person to manage ALL of your computational resources. Once the resources have been set up, the other users can connect using API key (which is essentially a username+password). Setting up your scheduler with Prefect \u00b6 The first half of this section is just a replica of Prefect's tutorial, but rewritten in a condensed format. If you prefer, you can use their tutorial instead: Introduction to Prefect Orchestration With Prefect as our scheduler, we have two options on how to set this up: Prefect Cloud or Prefect Server. They give a guide on which to choose here , but to summarize: - Prefect Server is free and open-source, but it requires you set up a server and manage it independently - Prefect Cloud is free for your first 20,000 workflow tasks each month (here's their pricing page ) and all set up for you Our team uses Prefect Cloud and recommends that beginners do the same. Unless you are running dozens of [evolutionary searches] per month, you won't come close to the 20,000 workflow task-run limit. This tutorial will continue with Prefect Cloud, but if you decide on Prefect Server , you'll have to set that up before continuing. Go ahead and sign in to Prefect Cloud (use your github account if you have one!). Everything will be empty to start, so we want to add all of Simmate's workflows to our interface here. To do this, we need an API key, which acts like a username and password. It's how we tell python that we now have a Prefect account. To get your API key, go to Prefect Cloud's homepage and navigate to User -> Account Settings -> API Keys -> Create An API Key . Copy the created API key. On your computer, open a terminal and make sure you have your conda enviornment active. Prefect is already installed because the installation of Simmate did it for us. So run the command: prefect backend cloud . This just tells prefect that we decided to use their scheduler. Next, run the command prefect auth login --key example_key_h123j2jfk where you replace the end text with your copied API KEY. We can now use Simmate to access your cloud and submit workflows for us! To get started, we need to add all of our workflows to Prefect. This is done with simmate workflow-engine setup-cloud . After running this, you should see all of the workflow in your Prefect Cloud now! If you were to use the simmate workflows run command that we've been using, you'll notice it still runs directly on your computer. To instead submit it to Prefect Cloud, use the command simmate workflows run-cloud instead. So for an example workflow, the full command would be... simmate workflows run-cloud relaxation_mit POSCAR . When you use run-cloud , the workflow run shows up in your Prefect Cloud, but it's not running. That is because we need a Prefect \"Agent\" -- an Agent simply checks for workflows that need to be ran and submits them to a cluster of computational resources. Let's start a Prefect Agent with all default settings (no cluster is attached yet), which means it will simply run the workflow on whichever computer the agent is on. To do this, run the command prefect agent local start . You'll see it pick up the workflow we submitted with run-cloud and run it. Even after the job completes, the agent will continue to run. So if you submit a new workflow, it will run it as well. We skimmed over a lot of the fundamentals for Prefect here, so we highly recommend going through Prefect's guides and tutorials . Spending a day or two on this will save you a lot of headache down the road. Setting up your cluster with Dask \u00b6 Because there is such a diverse set of computational resources that teams can have, we can't cover all setup scenarios in this tutorial. Instead, we will go through some examples of submitting Simmate workflows to a Dask cluster. This will all be on your local computer. For switching to remote resources (and job queue clusters like SLURM), we can only point you to key tutorials and documentation on how to set up your cluster. You can ask our team which setup is the best fit for your team, and we'll try to guide you through the process. Setting up your cluster shouldn't take longer than a hour, so post a question if you're struggling! Here are some useful resources for setting up a cluster with Dask. We recommend going through these before trying to use Dask with Simmate: - Introduction to Dask Futures - This is the best tutorial to start with! Then go through their example . Dask can do a lot, but Simmate only really uses this feature. If you understand how to use the client.submit , then you understand how Simmate is using Dask - Introduction to Dask Jobqueue - If you use a queue system like PBS, Slurm, MOAB, SGE, LSF, and HTCondor, then this will show you how to set up a cluster on your resource. Here's a simple example of using Dask to run a function that \"sleeps\" for 1 second import time # This loop will take 60 seconds to complete for n in range ( 60 ): # run this 60 times time . sleep ( 1 ) # sleeps 1 second # Now we switch to using Dask from dask.distributed import Client client = Client () # Futures are basically our \"job_id\". They let us check the status and result futures = [] for n in range ( 60 ): # run this 60 times # submits time.sleep(1) to Dask. pure=False tells Dask to rerun each instead # of loading past results from each time.sleep(1). future = client . submit ( time . sleep , 1 , pure = False ) futures . append ( future ) # now wait for all the jobs to finish # This will take much less than 60 seconds! results = [ future . result () for future in futures ] We'll now try using Dask to run our Simmate workflows. In serial (one item at time) \u00b6 Let's start with how we've been submitting workflows: using workflow.run . This runs the workflow immediately and on your local computer. If we were to run a workflow of many structures, only one workflow would run at a time. Once a workflow completes, it moves on to the next one: from simmate.workflows import example_workflow for structure in structures : result = example_workflow . run ( structure = structure ) In parallel (many workflows at once) \u00b6 To use your entire computer and all of its CPUs, we can set up a Dask cluster. There's one added thing you need to do though -- and that's make sure all of Dask workers are able to connect to the Simmate database. This example let's you submit a bunch of workflows and multiple workflows can run at the same time. You can do this with... # first setup Dask so that it can connect to Simmate from dask.distributed import Client client = Client ( preload = \"simmate.configuration.dask.connect_to_database\" ) # now submit your workflows and futures = [] for structure in structures : future = client . submit ( example_workflow . run , structure = structure ) result = future . result () # wait for all workflows to finish # you can monitor progress at http://localhost:8787/status In parallel (many tasks from a single workflow at once) \u00b6 But what if you want to run a single workflow with all of it's tasks in parallel? To do that, we use... # first setup Dask so that it can connect to Simmate from dask.distributed import Client client = Client ( preload = \"simmate.configuration.dask.connect_to_database\" ) # Tell Prefect that we should submit each task to Dask from prefect.executors import DaskExecutor example_workflow . executor = DaskExecutor ( address = client . scheduler . address ) # now run your workflow and wait for it to finish result = example_workflow . run () Connecting your Dask cluster to your Prefect Agent \u00b6 Once you learned how to set up your Dask cluster, the next step is tell Prefect where it is. You can always write your python script as two steps: Configure and then start your Dask Cluster. Configure and then start your Prefect Agent, which will start submitting workflows! In the future, we hope to have more convenient methods through the command-line, but these features are not complete yet. Connecting others to your scheduler \u00b6 Once you've set up your scheduler with Prefect and cluster with Dask, your team members simply need to connect to Prefect Cloud to submit their workflows. Generate an API key for them. Then they can complete step 9 of the quick tutorial (above). Just remember... workflow.run() in python and simmate workflows run in the command-line will still run the workflow locally and right away. workflow.run_cloud() in python and simmate workflows run-cloud in the command-line will submit your workflow to Prefect Cloud.","title":"Integrate with Prefect and Dask"},{"location":"getting_started/_Integrate_with_Prefect_and_Dask/#integrate-with-prefect-and-dask","text":"We do not recommend this tutorial for users at the moment. This tutorial is for Prefect v1, but much of Simmate now depends on Prefect v2. As we adjust to the new backend, parts of this tutorial may be broken and recommended procedures are subject to change. In this tutorial, you will learn how to run workflows on distributed computational resources -- with full scheduling and monitoring. The quick tutorial The full tutorial A review of concepts Should I set up my own cluster? Setting up your scheduler with Prefect Setting up your cluster with Dask Connecting others to your scheduler For beginners, this will be the most difficult part of setting up Simmate -- but it is entirely optional. Be sure to read the section on Should I set up my own cluster? . There are many ways to set up your resources and caviats to each (especially if you are using university or federal supercomputers). While python experts should be able to learn Prefect and Dask quickly, we strongly urge beginners to get advice from our team. If you struggle to follow along with this tutorial, post a question or email us directly (simmate.team@gmail.com).","title":"Integrate with Prefect and Dask"},{"location":"getting_started/_Integrate_with_Prefect_and_Dask/#the-quick-tutorial","text":"prefect , dask , and dask_jobqueue will be already installed for you because they are dependencies of Simmate Be aware that you can share a cloud database without sharing computational resources. This flexibility is very important for many collaborations. Just like with your cloud database, designate a point-person to manage your private computational resources. Everyone else can skip to step 9. Either sign in to Prefect Cloud (recommended) or setup a Prefect Server . Connect to your server with the following steps (this is from Prefect's tutorial ): On Prefect Cloud's homepage, go to User -> Account Settings -> API Keys -> Create An API Key Copy the created key set your Prefect backend with the command prefect backend cloud tell Prefect your key with the command prefect auth login --key example_key_h123j2jfk Register all Simmate workflows with Prefect using the command simmate workflow-engine setup-cloud Test that Prefect is configured properly with the following steps (this will run the workflow locally): run the command prefect agent local start (note that this will run endlessly and submit all workflows in parallel. use crtl+C to stop) in a separate terminal, rerun our workflow from tutorial 2 with run-cloud instead of run (so simmate workflows run-cloud relaxation_mit POSCAR ) Set up your computational resources using Dask (and if needed, Dask JobQueue). There are MANY options for this, which are covered in the simmate.workflow_engine module. Take the time to read the documentation here. But as an example, we'll set up a SLURM cluster and then link it to a Prefect agent. Note, if you want to run workflows with commands like mpirun -n 18 vasp_std > vasp.out , then limit the Dask worker to one core while having the SLURM job request more cores. The resulting python script will look something like this: # -------------------------------------------------------------------------------------- # STEP 1: Configure our Dask Cluster. from dask_jobqueue import SLURMCluster cluster = SLURMCluster ( # # General options scheduler_options = { \"port\" : 8786 }, local_directory = \"~\" , # # # Dask Worker Options cores = 1 , processes = 1 , memory = \"4GB\" , # REQUIRED: Make sure you preload this script! extra = [ \"--preload simmate.configuration.dask.connect_to_database\" ], # # # SLURM Job Options job_cpu = 18 , job_mem = \"50GB\" , job_extra = [ \"--output=slurm-%j.out\" , \"-N 1\" , ], walltime = \"300-00:00:00\" , queue = \"p1\" , # this is the name of the SLURM queue/partition env_extra = [ \"module load vasp;\" ], # our workflow requires the vasp module to be loaded ) # Scale the cluster to the number of SLURM jobs that you'd like cluster . scale ( 10 ) # -------------------------------------------------------------------------------------- # STEP 2: Configure our Prefect Agent and start submitting workflows! from prefect.agent.local import LocalAgent from simmate.configuration.prefect.connect_to_dask import set_default_executor # We want Prefect to use our Dask Cluster to run all of the workflow tasks. To # tell Prefect to do this, we wrote a helper function that ships with Simmate. set_default_executor ( cluster . scheduler . address ) # Start our cluster! Our cluster's name is Warwulf, so we use that here. agent = LocalAgent ( name = \"WarWulf\" , labels = [ \"WarWulf\" ], ) # Now we can start the Prefect Agent which will run and search for jobs and then # submit them to our Dask cluster. agent . start () # NOTE: this line will run endlessly unless you set a timelimit in the LocalAgent above # -------------------------------------------------------------------------------------- Test out your cluster by running simmate workflows run-cloud relaxation_mit POSCAR in a separate terminal (submit this a bunch if you'd like to). If you'd like to limit how many workflows of a given tag (e.g. \"WarWulf\" above) run in parallel, set the concurrency limit in Prefect cloud here . To let others use your cluster, simply add them to your Prefect Cloud and give them an API key. They just need to do the following: set your Prefect backend with the command prefect backend cloud tell Prefect your key with the command prefect auth login --key example_key_h123j2jfk try submitting a workflow with simmate workflows run-cloud relaxation_mit POSCAR","title":"The quick tutorial"},{"location":"getting_started/_Integrate_with_Prefect_and_Dask/#the-full-tutorial","text":"","title":"The full tutorial"},{"location":"getting_started/_Integrate_with_Prefect_and_Dask/#a-review-of-concepts","text":"Recall from tutorial 2, there are 4 steps to a workflow: - configure : chooses our desired settings for the calculation (such as VASP's INCAR settings) - schedule : decides whether to run the workflow immediately or send off to a job queue (e.g. SLURM, PBS, or remote computers) - execute : writes our input files, runs the calculation (e.g. calling VASP), and checks the results for errors - save : saves the results to our database This tuturial will give an overview of how to modify the schedule and determine which computer execute is called on. Up until now, we have been using the default behavior for these two steps. But now we want to instead do the following: - schedule : submits the workflow to a scheduler queue of many other workflows - execute : run the calculation on a remote cluster A scheduler is something we submit workflows to and controls when to run them. As a bunch of workflows are submitted, our scheduler forms a queue and keeps track of which ones to run next. We will use Prefect as our scheduler. A cluster is a group of computational resources that actually run the workflows. So our scheduler will find whichever workflow should be ran next, and send it to our cluster to run. Clusters are often made up of \"workers\" -- where a worker is just a single resource and it works through one job at a time. For example, if we had a cluster made up of 10 desktop computers, each computer would run a workflow and once finished ask the scheduler for the next workflow to run. At any given time, 10 workflows will be running. We'll use Dask to set up our cluster, whether your resources are on the cloud, a supercomputer, or just simple desktops.","title":"A review of concepts"},{"location":"getting_started/_Integrate_with_Prefect_and_Dask/#should-i-set-up-my-own-cluster","text":"Before we start... We understand that resource restrictions can be very stingent between labs and companies. So sharing resources is not possible, even in close collaborations. Simmate addresses this issue by making it easy to share a cloud database without sharing computational resources. In other words, you can contribute to a shared database without letting others see/access your computational resources. With that said, each team will likely need to handle their own computational resources, which can be any number of things: - a university or federal HPC cluster with SLURM, PBS, or some other queue system - a single node or even a Kubernetes cluster provided by a commercial service like DigitalOcean, GoogleCloud, etc. - a series of desktop computers that your lab shares - any combination of these resources The easiest way to use these resources is to sign on and run simmate directly on it. When this is done, the workflow runs directly on your resource and it will run there immediately. We've already see this with Tutorial 2 when we called simmate workflows run ... . Just use that on your new resource to start! (for help signing on to remote supercomputers and installing anaconda, be sure to ask the cluster's IT team). As another example, you can submit a workflow to a SLURM cluster with a submit.sh file like this: #!/bin/bash #SBATCH --output=slurm.out #SBATCH --nodes=1 #SBATCH --ntasks=2 # make sure you have you activated your conda enviornment # and required modules before submitting simmate workflows run-cloud relaxation_mit POSCAR > simmate.out If you are only running a few workflows per day (<10), we recommend you stick to running workflows in this way. That is, just calling simmate workflows run . Don't overcomplicate things. Alternatively, if your team is submitting hundreds or thousands of workflows at a time, then it would be extremely useful to monitor and orchestrate these workflows. To do this, we will use Prefect and Dask. Just like with our cloud database in the previous tutorial, you only need ONE person to manage ALL of your computational resources. Once the resources have been set up, the other users can connect using API key (which is essentially a username+password).","title":"Should I set up my own cluster?"},{"location":"getting_started/_Integrate_with_Prefect_and_Dask/#setting-up-your-scheduler-with-prefect","text":"The first half of this section is just a replica of Prefect's tutorial, but rewritten in a condensed format. If you prefer, you can use their tutorial instead: Introduction to Prefect Orchestration With Prefect as our scheduler, we have two options on how to set this up: Prefect Cloud or Prefect Server. They give a guide on which to choose here , but to summarize: - Prefect Server is free and open-source, but it requires you set up a server and manage it independently - Prefect Cloud is free for your first 20,000 workflow tasks each month (here's their pricing page ) and all set up for you Our team uses Prefect Cloud and recommends that beginners do the same. Unless you are running dozens of [evolutionary searches] per month, you won't come close to the 20,000 workflow task-run limit. This tutorial will continue with Prefect Cloud, but if you decide on Prefect Server , you'll have to set that up before continuing. Go ahead and sign in to Prefect Cloud (use your github account if you have one!). Everything will be empty to start, so we want to add all of Simmate's workflows to our interface here. To do this, we need an API key, which acts like a username and password. It's how we tell python that we now have a Prefect account. To get your API key, go to Prefect Cloud's homepage and navigate to User -> Account Settings -> API Keys -> Create An API Key . Copy the created API key. On your computer, open a terminal and make sure you have your conda enviornment active. Prefect is already installed because the installation of Simmate did it for us. So run the command: prefect backend cloud . This just tells prefect that we decided to use their scheduler. Next, run the command prefect auth login --key example_key_h123j2jfk where you replace the end text with your copied API KEY. We can now use Simmate to access your cloud and submit workflows for us! To get started, we need to add all of our workflows to Prefect. This is done with simmate workflow-engine setup-cloud . After running this, you should see all of the workflow in your Prefect Cloud now! If you were to use the simmate workflows run command that we've been using, you'll notice it still runs directly on your computer. To instead submit it to Prefect Cloud, use the command simmate workflows run-cloud instead. So for an example workflow, the full command would be... simmate workflows run-cloud relaxation_mit POSCAR . When you use run-cloud , the workflow run shows up in your Prefect Cloud, but it's not running. That is because we need a Prefect \"Agent\" -- an Agent simply checks for workflows that need to be ran and submits them to a cluster of computational resources. Let's start a Prefect Agent with all default settings (no cluster is attached yet), which means it will simply run the workflow on whichever computer the agent is on. To do this, run the command prefect agent local start . You'll see it pick up the workflow we submitted with run-cloud and run it. Even after the job completes, the agent will continue to run. So if you submit a new workflow, it will run it as well. We skimmed over a lot of the fundamentals for Prefect here, so we highly recommend going through Prefect's guides and tutorials . Spending a day or two on this will save you a lot of headache down the road.","title":"Setting up your scheduler with Prefect"},{"location":"getting_started/_Integrate_with_Prefect_and_Dask/#setting-up-your-cluster-with-dask","text":"Because there is such a diverse set of computational resources that teams can have, we can't cover all setup scenarios in this tutorial. Instead, we will go through some examples of submitting Simmate workflows to a Dask cluster. This will all be on your local computer. For switching to remote resources (and job queue clusters like SLURM), we can only point you to key tutorials and documentation on how to set up your cluster. You can ask our team which setup is the best fit for your team, and we'll try to guide you through the process. Setting up your cluster shouldn't take longer than a hour, so post a question if you're struggling! Here are some useful resources for setting up a cluster with Dask. We recommend going through these before trying to use Dask with Simmate: - Introduction to Dask Futures - This is the best tutorial to start with! Then go through their example . Dask can do a lot, but Simmate only really uses this feature. If you understand how to use the client.submit , then you understand how Simmate is using Dask - Introduction to Dask Jobqueue - If you use a queue system like PBS, Slurm, MOAB, SGE, LSF, and HTCondor, then this will show you how to set up a cluster on your resource. Here's a simple example of using Dask to run a function that \"sleeps\" for 1 second import time # This loop will take 60 seconds to complete for n in range ( 60 ): # run this 60 times time . sleep ( 1 ) # sleeps 1 second # Now we switch to using Dask from dask.distributed import Client client = Client () # Futures are basically our \"job_id\". They let us check the status and result futures = [] for n in range ( 60 ): # run this 60 times # submits time.sleep(1) to Dask. pure=False tells Dask to rerun each instead # of loading past results from each time.sleep(1). future = client . submit ( time . sleep , 1 , pure = False ) futures . append ( future ) # now wait for all the jobs to finish # This will take much less than 60 seconds! results = [ future . result () for future in futures ] We'll now try using Dask to run our Simmate workflows.","title":"Setting up your cluster with Dask"},{"location":"getting_started/_Integrate_with_Prefect_and_Dask/#in-serial-one-item-at-time","text":"Let's start with how we've been submitting workflows: using workflow.run . This runs the workflow immediately and on your local computer. If we were to run a workflow of many structures, only one workflow would run at a time. Once a workflow completes, it moves on to the next one: from simmate.workflows import example_workflow for structure in structures : result = example_workflow . run ( structure = structure )","title":"In serial (one item at time)"},{"location":"getting_started/_Integrate_with_Prefect_and_Dask/#in-parallel-many-workflows-at-once","text":"To use your entire computer and all of its CPUs, we can set up a Dask cluster. There's one added thing you need to do though -- and that's make sure all of Dask workers are able to connect to the Simmate database. This example let's you submit a bunch of workflows and multiple workflows can run at the same time. You can do this with... # first setup Dask so that it can connect to Simmate from dask.distributed import Client client = Client ( preload = \"simmate.configuration.dask.connect_to_database\" ) # now submit your workflows and futures = [] for structure in structures : future = client . submit ( example_workflow . run , structure = structure ) result = future . result () # wait for all workflows to finish # you can monitor progress at http://localhost:8787/status","title":"In parallel (many workflows at once)"},{"location":"getting_started/_Integrate_with_Prefect_and_Dask/#in-parallel-many-tasks-from-a-single-workflow-at-once","text":"But what if you want to run a single workflow with all of it's tasks in parallel? To do that, we use... # first setup Dask so that it can connect to Simmate from dask.distributed import Client client = Client ( preload = \"simmate.configuration.dask.connect_to_database\" ) # Tell Prefect that we should submit each task to Dask from prefect.executors import DaskExecutor example_workflow . executor = DaskExecutor ( address = client . scheduler . address ) # now run your workflow and wait for it to finish result = example_workflow . run ()","title":"In parallel (many tasks from a single workflow at once)"},{"location":"getting_started/_Integrate_with_Prefect_and_Dask/#connecting-your-dask-cluster-to-your-prefect-agent","text":"Once you learned how to set up your Dask cluster, the next step is tell Prefect where it is. You can always write your python script as two steps: Configure and then start your Dask Cluster. Configure and then start your Prefect Agent, which will start submitting workflows! In the future, we hope to have more convenient methods through the command-line, but these features are not complete yet.","title":"Connecting your Dask cluster to your Prefect Agent"},{"location":"getting_started/_Integrate_with_Prefect_and_Dask/#connecting-others-to-your-scheduler","text":"Once you've set up your scheduler with Prefect and cluster with Dask, your team members simply need to connect to Prefect Cloud to submit their workflows. Generate an API key for them. Then they can complete step 9 of the quick tutorial (above). Just remember... workflow.run() in python and simmate workflows run in the command-line will still run the workflow locally and right away. workflow.run_cloud() in python and simmate workflows run-cloud in the command-line will submit your workflow to Prefect Cloud.","title":"Connecting others to your scheduler"},{"location":"getting_started/overview/","text":"Getting Started Tutorials \u00b6 Welcome! These tutorials should help you get familiar with Simmate -- whether you're new to coding or a python expert. If anything is unclear, don't hesitate to post a question on our forum . Expected duration \u00b6 If you are comfortable with python and the command-line, tutorials 01-05 should take under 1 hour in total . Tutorials 06-09 will take 2-3 hours in total but are optional. If you are new to coding, we recommend you complete one tutorial per day (1-2 hrs) to fully understand each. This includes exploring extra resources in each tutorial. Keep in mind that tutorials 06-09 are advanced topics but also completely optional. Tip Before starting any tutorials, we recommend that you make a GitHub account if you don't have one already. An account is optional but will allow you to post questions on our forum and sign-in to our website. A github account can also be used for optional services like cloud database hosting (covered in a later tutorial). Use the buttons below to begin!","title":"Overview"},{"location":"getting_started/overview/#getting-started-tutorials","text":"Welcome! These tutorials should help you get familiar with Simmate -- whether you're new to coding or a python expert. If anything is unclear, don't hesitate to post a question on our forum .","title":"Getting Started Tutorials"},{"location":"getting_started/overview/#expected-duration","text":"If you are comfortable with python and the command-line, tutorials 01-05 should take under 1 hour in total . Tutorials 06-09 will take 2-3 hours in total but are optional. If you are new to coding, we recommend you complete one tutorial per day (1-2 hrs) to fully understand each. This includes exploring extra resources in each tutorial. Keep in mind that tutorials 06-09 are advanced topics but also completely optional. Tip Before starting any tutorials, we recommend that you make a GitHub account if you don't have one already. An account is optional but will allow you to post questions on our forum and sign-in to our website. A github account can also be used for optional services like cloud database hosting (covered in a later tutorial). Use the buttons below to begin!","title":"Expected duration"},{"location":"getting_started/wrap_up/","text":"You did it! \u00b6 If you made it to this point, you're pretty much a Simmate expert! Any other guides and tutorials will be in the full-guides section . We hope you see the potential Simmate has to offer the larger materials community! With a powerful framework like Simmate in hand, anything is possible. In other words... \"The ceiling is the roof\" -Michael Jordan Have fun coding and always be sure to ask for help/feedback when you need it.","title":"Wrap up"},{"location":"getting_started/wrap_up/#you-did-it","text":"If you made it to this point, you're pretty much a Simmate expert! Any other guides and tutorials will be in the full-guides section . We hope you see the potential Simmate has to offer the larger materials community! With a powerful framework like Simmate in hand, anything is possible. In other words... \"The ceiling is the roof\" -Michael Jordan Have fun coding and always be sure to ask for help/feedback when you need it.","title":"You did it!"},{"location":"getting_started/access_the_database/access_thirdparty_data/","text":"Accessing third-party data \u00b6 When running our own calculations with Simmate, it is also important to know what other researchers have already calculated for a given material. Many research teams around the world have built databases made of 100,000+ structures -- and many of these teams even ran calculations on all of them. Here, we will use Simmate to explore their data. Loading a table \u00b6 Let's start with one of the smaller databases out there: JARVIS . It may be smaller than the others, but their dataset still includes ~56,000 structures! Simmate makes the download for all of these under 0.01 GB. In the previous section, we loaded our DatabaseTable from the workflow. But now we don't have a workflow... We just want to grab the table directly. To do this we run the following: # This line MUST be ran before any tables can be loaded from simmate.database import connect # this connects to our database # This gives the database_table we were using in the previous section from simmate.database.workflow_results import StaticEnergy # This loads the table where we store all of the JARVIS data. from simmate.database.third_parties import JarvisStructure table from the previous section (on accessing workflow data) and the StaticEnergy class here are the exact same class/table. These are just different ways of loading it. While loading a workflow sets up a database connection for us, we have the do that step manually here (with from simmate.database import connect ). Warning When loading database tables directly from the simmate.database module, the most common error is forgetting to connect to your database. So don't forget to include from simmate.database import connect ! Filling data \u00b6 Now that we have our datatable class ( JarvisStructure ) loaded, let's check if there's any data in it: JarvisStructure . objects . count () Note If you accepted the download during the simmate database reset command, then you should see that there are thousands of structures already in this database table! If the count gives 0, then that means you still need to load data. We can quickly load all of the data using the load_remote_archive method. Behind the scenes, this is downloading the JARVIS data from simmate.org and moving it into your database. This can take ~10 minutes because we are actually saving all these structures to your computer -- that way, you can rapidly load these structures in under 1 second in the future. # NOTE: This line is only needed if you did NOT accept the download # when running `simmate database reset`. JarvisStructure . load_remote_archive () # This may take ~10min to complete Warning It is very important that you read the warnings printed by load_remote_archive . This data was NOT made by Simmate. We are just helping to distribute it on behalf of these other teams. Be sure to cite them for their work! Calling load_remote_archive loads ALL data to your computer and saves it. This data will not be updated unless you call load_remote_archive again. This should only be done every time we release a new archive version (typically once per year). To protect our servers from misuse, you can only call load_remote_archive() a few times per month -- no matter what. Don't overuse this feature. Start exploring the data \u00b6 Now that we ensured our database if filled with data, we can start looking through: # We use [:150] to just show the first 150 rows data = JarvisStructure . objects . to_dataframe ()[: 150 ] Now let's really test out our filtering ability with this new data: from simmate.database import connect # this connects to our database from simmate.database.third_parties import JarvisStructure # EXAMPLE 1: all structures that have less than 6 sites in their unitcell structures_1 = JarvisStructure . objects . filter ( nsites__lt = 6 ) . all () # EXAMPLE 2: all MoS2 structures that are less than 5/A^3 and have a spacegroup # symbol of R3mH structures_2 = JarvisStructure . objects . filter ( formula_full = \"Mo1 S2\" , density__lt = 5 , spacegroup__symbol = \"R3mH\" , ) . all () # You can use to_dataframe() to convert these to a pandas Dataframe object and # then view them in Spyder's variable explorer df_1 = structures_1 . to_dataframe () df_2 = structures_2 . to_dataframe () Advanced data manipulation \u00b6 There are many ways to search through your tables, and we only covered the basics here. Advanced users will benefit from knowing that we use Django's query api under the hood. It can take a long time to master, so we only recommend going through Django's full tutorial if you plan on joining our team or are a fully computational student. Beginners can just ask for help. Figuring out the correct filter can take new users hours while it will only take our team a minute or two. Save your time and post questions here .","title":"Access third-party data"},{"location":"getting_started/access_the_database/access_thirdparty_data/#accessing-third-party-data","text":"When running our own calculations with Simmate, it is also important to know what other researchers have already calculated for a given material. Many research teams around the world have built databases made of 100,000+ structures -- and many of these teams even ran calculations on all of them. Here, we will use Simmate to explore their data.","title":"Accessing third-party data"},{"location":"getting_started/access_the_database/access_thirdparty_data/#loading-a-table","text":"Let's start with one of the smaller databases out there: JARVIS . It may be smaller than the others, but their dataset still includes ~56,000 structures! Simmate makes the download for all of these under 0.01 GB. In the previous section, we loaded our DatabaseTable from the workflow. But now we don't have a workflow... We just want to grab the table directly. To do this we run the following: # This line MUST be ran before any tables can be loaded from simmate.database import connect # this connects to our database # This gives the database_table we were using in the previous section from simmate.database.workflow_results import StaticEnergy # This loads the table where we store all of the JARVIS data. from simmate.database.third_parties import JarvisStructure table from the previous section (on accessing workflow data) and the StaticEnergy class here are the exact same class/table. These are just different ways of loading it. While loading a workflow sets up a database connection for us, we have the do that step manually here (with from simmate.database import connect ). Warning When loading database tables directly from the simmate.database module, the most common error is forgetting to connect to your database. So don't forget to include from simmate.database import connect !","title":"Loading a table"},{"location":"getting_started/access_the_database/access_thirdparty_data/#filling-data","text":"Now that we have our datatable class ( JarvisStructure ) loaded, let's check if there's any data in it: JarvisStructure . objects . count () Note If you accepted the download during the simmate database reset command, then you should see that there are thousands of structures already in this database table! If the count gives 0, then that means you still need to load data. We can quickly load all of the data using the load_remote_archive method. Behind the scenes, this is downloading the JARVIS data from simmate.org and moving it into your database. This can take ~10 minutes because we are actually saving all these structures to your computer -- that way, you can rapidly load these structures in under 1 second in the future. # NOTE: This line is only needed if you did NOT accept the download # when running `simmate database reset`. JarvisStructure . load_remote_archive () # This may take ~10min to complete Warning It is very important that you read the warnings printed by load_remote_archive . This data was NOT made by Simmate. We are just helping to distribute it on behalf of these other teams. Be sure to cite them for their work! Calling load_remote_archive loads ALL data to your computer and saves it. This data will not be updated unless you call load_remote_archive again. This should only be done every time we release a new archive version (typically once per year). To protect our servers from misuse, you can only call load_remote_archive() a few times per month -- no matter what. Don't overuse this feature.","title":"Filling data"},{"location":"getting_started/access_the_database/access_thirdparty_data/#start-exploring-the-data","text":"Now that we ensured our database if filled with data, we can start looking through: # We use [:150] to just show the first 150 rows data = JarvisStructure . objects . to_dataframe ()[: 150 ] Now let's really test out our filtering ability with this new data: from simmate.database import connect # this connects to our database from simmate.database.third_parties import JarvisStructure # EXAMPLE 1: all structures that have less than 6 sites in their unitcell structures_1 = JarvisStructure . objects . filter ( nsites__lt = 6 ) . all () # EXAMPLE 2: all MoS2 structures that are less than 5/A^3 and have a spacegroup # symbol of R3mH structures_2 = JarvisStructure . objects . filter ( formula_full = \"Mo1 S2\" , density__lt = 5 , spacegroup__symbol = \"R3mH\" , ) . all () # You can use to_dataframe() to convert these to a pandas Dataframe object and # then view them in Spyder's variable explorer df_1 = structures_1 . to_dataframe () df_2 = structures_2 . to_dataframe ()","title":"Start exploring the data"},{"location":"getting_started/access_the_database/access_thirdparty_data/#advanced-data-manipulation","text":"There are many ways to search through your tables, and we only covered the basics here. Advanced users will benefit from knowing that we use Django's query api under the hood. It can take a long time to master, so we only recommend going through Django's full tutorial if you plan on joining our team or are a fully computational student. Beginners can just ask for help. Figuring out the correct filter can take new users hours while it will only take our team a minute or two. Save your time and post questions here .","title":"Advanced data manipulation"},{"location":"getting_started/access_the_database/access_workflow_data/","text":"Accessing results from local calculations \u00b6 Loading a table \u00b6 In the \"run a workflow\" tutorial, we ran a calculation and then added results to our database table. Here, we will now go through the results. The database table for results is always attached to the workflow as the database_table attribute. You can load it like this: from simmate.workflows.utilities import get_workflow workflow = get_workflow ( \"static-energy.vasp.mit\" ) table = workflow . database_table Seeing the available columns \u00b6 To see all of the data this table stores, we can use it's show_columns() method. Here, we'll see a bunch of columns printed for us... table . show_columns () ... which will output ... - id - created_at - updated_at - source - structure - nsites - nelements - elements - chemical_system - density - density_atomic - volume - volume_molar - formula_full - formula_reduced - formula_anonymous - spacegroup (relation to Spacegroup) - workflow_name - location - directory - run_id - corrections - site_forces - lattice_stress - site_force_norm_max - site_forces_norm - site_forces_norm_per_atom - lattice_stress_norm - lattice_stress_norm_per_atom - energy - energy_per_atom - energy_above_hull - is_stable - decomposes_to - formation_energy - formation_energy_per_atom - band_gap - is_gap_direct - energy_fermi - conduction_band_minimum - valence_band_maximum These are a lot of columns... and you may not need all of them. But Simmate still builds all of these for you right away because they don't take up very much storage space. Convert to an excel-like table \u00b6 Next we'd want to see the table with all of its data. To access the table rows, we use the objects attribute, and then to get this into a table, we convert to a \"dataframe\". A dataframe is a filtered portion of a database table -- and because we didn't filter any of our results yet, our dataframe is just the whole table. data = table . objects . to_dataframe () Open up this variable by double-clicking data in Spyder's variable explorer (top right window) and you can view the table. Here's what a typical dataframe looks like in Spyder: Filtering results from the table \u00b6 Next, we can use our table columns to start filtering through our results. Your search results will be given back as a list of rows that met the filtering criteria. In the example above, we converted that list of results to into a dataframe for easy viewing. You can also convert each row into our ToolkitStructure from tutorial 3! There are a bunch of things to try, so play around with each: # We can filter off rows in the table. Any column can be used! search_results = table . objects . filter ( formula_reduced = \"NaCl\" , # check an exact match for any column nelements = 2 , # filter a column based on a greater or equal to (gte) condition ) . all () # If we look at this closely, you notice this is just a list of database objects (1 object = 1 row) print ( search_results ) # We can convert this list of objects to a dataframe like we did above data = search_results . to_dataframe () # Or we can convert to a list of structure objects (ToolkitStructure) structures = search_results . to_toolkit () This isn't very exciting now because we just have one row/structure in our table , but we'll do some more exciting filtering in the next section.","title":"Access workflow data"},{"location":"getting_started/access_the_database/access_workflow_data/#accessing-results-from-local-calculations","text":"","title":"Accessing results from local calculations"},{"location":"getting_started/access_the_database/access_workflow_data/#loading-a-table","text":"In the \"run a workflow\" tutorial, we ran a calculation and then added results to our database table. Here, we will now go through the results. The database table for results is always attached to the workflow as the database_table attribute. You can load it like this: from simmate.workflows.utilities import get_workflow workflow = get_workflow ( \"static-energy.vasp.mit\" ) table = workflow . database_table","title":"Loading a table"},{"location":"getting_started/access_the_database/access_workflow_data/#seeing-the-available-columns","text":"To see all of the data this table stores, we can use it's show_columns() method. Here, we'll see a bunch of columns printed for us... table . show_columns () ... which will output ... - id - created_at - updated_at - source - structure - nsites - nelements - elements - chemical_system - density - density_atomic - volume - volume_molar - formula_full - formula_reduced - formula_anonymous - spacegroup (relation to Spacegroup) - workflow_name - location - directory - run_id - corrections - site_forces - lattice_stress - site_force_norm_max - site_forces_norm - site_forces_norm_per_atom - lattice_stress_norm - lattice_stress_norm_per_atom - energy - energy_per_atom - energy_above_hull - is_stable - decomposes_to - formation_energy - formation_energy_per_atom - band_gap - is_gap_direct - energy_fermi - conduction_band_minimum - valence_band_maximum These are a lot of columns... and you may not need all of them. But Simmate still builds all of these for you right away because they don't take up very much storage space.","title":"Seeing the available columns"},{"location":"getting_started/access_the_database/access_workflow_data/#convert-to-an-excel-like-table","text":"Next we'd want to see the table with all of its data. To access the table rows, we use the objects attribute, and then to get this into a table, we convert to a \"dataframe\". A dataframe is a filtered portion of a database table -- and because we didn't filter any of our results yet, our dataframe is just the whole table. data = table . objects . to_dataframe () Open up this variable by double-clicking data in Spyder's variable explorer (top right window) and you can view the table. Here's what a typical dataframe looks like in Spyder:","title":"Convert to an excel-like table"},{"location":"getting_started/access_the_database/access_workflow_data/#filtering-results-from-the-table","text":"Next, we can use our table columns to start filtering through our results. Your search results will be given back as a list of rows that met the filtering criteria. In the example above, we converted that list of results to into a dataframe for easy viewing. You can also convert each row into our ToolkitStructure from tutorial 3! There are a bunch of things to try, so play around with each: # We can filter off rows in the table. Any column can be used! search_results = table . objects . filter ( formula_reduced = \"NaCl\" , # check an exact match for any column nelements = 2 , # filter a column based on a greater or equal to (gte) condition ) . all () # If we look at this closely, you notice this is just a list of database objects (1 object = 1 row) print ( search_results ) # We can convert this list of objects to a dataframe like we did above data = search_results . to_dataframe () # Or we can convert to a list of structure objects (ToolkitStructure) structures = search_results . to_toolkit () This isn't very exciting now because we just have one row/structure in our table , but we'll do some more exciting filtering in the next section.","title":"Filtering results from the table"},{"location":"getting_started/access_the_database/intro_to_python_inheritance/","text":"Python Inheritance with Datatables \u00b6 Review of concepts \u00b6 To review key concepts up until this point... we setup our database and added calcution results to it. we learned about python classes, and in particular, the importance of the Structure class. we learned how to explore the documentation and use new classes. Now, we want to bring these ideas together in order to expolore our database. An example table in python \u00b6 Let's start simple... All datatables are represented by a class, where the general format looks like this: from simmate.database.base_data_types import DatabaseTable , table_column class MyExampleTable ( DatabaseTable ): column_01 = table_columns . CharField () # CharField --> means we store text column_02 = table_columns . BoolField () # BoolField --> means we store True/False column_03 = table_columns . FloatField () # FloatField --> means we store a number/decimal And the corresponding table (with random data added) would look like... column_01 column_02 column_03 jack True 3.1456 lauren False 299792458 siona True 1.6180 scott False 1.602e-19 ... ... ... That's how all tables are made! We just make a class, say it is a DatabaseTable and then list off our desired columns. Building tables with inheritance \u00b6 However, this could get really repetitive if we have a bunch of tables that contain similar information. For example, we may want to store structures in many different tables -- each one with columns like density, number of sites, number of elements, etc.. To save time, we use what is known as python \"inheritance\". Here's how it works: First, we define a table with common information (let's say a Person ). from simmate.database.base_data_types import DatabaseTable , table_column class Person ( DatabaseTable ): name = table_columns . CharField () age = table_columns . IntField () height = table_columns . FloatField () Next, we want a separate table to contain this type of information and more: class Student ( Person ): # <--- note we have Person here instead of DatabaseTable year = table_columns . IntField () # e.g. class of 2020 gpa = table_columns . FloatField () The Student datatable now looks like this: name age height year gpa jack 15 6.1 2020 3.6 lauren 16 5.8 2019 4.0 siona 15 5.6 2020 3.7 scott 14 6.2 2021 3.2 ... ... ... ... ... Simmate uses this idea with common materials science data -- such as structures, thermodynamic data, site forces, and more. You'll find our fundamental building blocks for tables in the simmate.database.base_data_types module ( covered here ). All of our datatables start from these classes and build up. Up next, we'll look at an actual database table and learn how to use it to view data.","title":"Intro to Python inheritance"},{"location":"getting_started/access_the_database/intro_to_python_inheritance/#python-inheritance-with-datatables","text":"","title":"Python Inheritance with Datatables"},{"location":"getting_started/access_the_database/intro_to_python_inheritance/#review-of-concepts","text":"To review key concepts up until this point... we setup our database and added calcution results to it. we learned about python classes, and in particular, the importance of the Structure class. we learned how to explore the documentation and use new classes. Now, we want to bring these ideas together in order to expolore our database.","title":"Review of concepts"},{"location":"getting_started/access_the_database/intro_to_python_inheritance/#an-example-table-in-python","text":"Let's start simple... All datatables are represented by a class, where the general format looks like this: from simmate.database.base_data_types import DatabaseTable , table_column class MyExampleTable ( DatabaseTable ): column_01 = table_columns . CharField () # CharField --> means we store text column_02 = table_columns . BoolField () # BoolField --> means we store True/False column_03 = table_columns . FloatField () # FloatField --> means we store a number/decimal And the corresponding table (with random data added) would look like... column_01 column_02 column_03 jack True 3.1456 lauren False 299792458 siona True 1.6180 scott False 1.602e-19 ... ... ... That's how all tables are made! We just make a class, say it is a DatabaseTable and then list off our desired columns.","title":"An example table in python"},{"location":"getting_started/access_the_database/intro_to_python_inheritance/#building-tables-with-inheritance","text":"However, this could get really repetitive if we have a bunch of tables that contain similar information. For example, we may want to store structures in many different tables -- each one with columns like density, number of sites, number of elements, etc.. To save time, we use what is known as python \"inheritance\". Here's how it works: First, we define a table with common information (let's say a Person ). from simmate.database.base_data_types import DatabaseTable , table_column class Person ( DatabaseTable ): name = table_columns . CharField () age = table_columns . IntField () height = table_columns . FloatField () Next, we want a separate table to contain this type of information and more: class Student ( Person ): # <--- note we have Person here instead of DatabaseTable year = table_columns . IntField () # e.g. class of 2020 gpa = table_columns . FloatField () The Student datatable now looks like this: name age height year gpa jack 15 6.1 2020 3.6 lauren 16 5.8 2019 4.0 siona 15 5.6 2020 3.7 scott 14 6.2 2021 3.2 ... ... ... ... ... Simmate uses this idea with common materials science data -- such as structures, thermodynamic data, site forces, and more. You'll find our fundamental building blocks for tables in the simmate.database.base_data_types module ( covered here ). All of our datatables start from these classes and build up. Up next, we'll look at an actual database table and learn how to use it to view data.","title":"Building tables with inheritance"},{"location":"getting_started/access_the_database/quick_start/","text":"Search the database \u00b6 In this tutorial, you will learn how to explore your database as well as load data from third-party providers. Beginners will also be introduced to python inheritance. Tip This entire tutorial should be ran on your local computer because we will be using Spyder. To use results from your remote calculations, you can copy/paste your database file from your remote computer to where our local database one should be (~/simmate/my_env-database.sqlite3). Note, copy/pasting will no longer be necessary when we switch to a cloud database in the next tutorial. The quick tutorial \u00b6 Make sure you've initialized your database. This was done in tutorial 2 with the command simmate database reset . Do NOT rerun this command as it will empty your database and delete your results. Go to the simmate.database module to view all available tables. The table for Tutorial 2's results are located in the StaticEnergy datatable class, which can be loaded via either of these options: # OPTION 1 from simmate.database import connect # this connects to our database from simmate.database.workflow_results import StaticEnergy # OPTION 2 (recommended for convenience) from simmate.workflows.utilities import get_workflow workflow = get_workflow ( \"static-energy.vasp.mit\" ) table = workflow . database_table # results here is the same thing as StaticEnergy above View all the possible table columns with StaticEnergy.show_columns() View the full table as pandas dataframe with StaticEnergy.objects.to_dataframe() Filter results using django-based queries . For example: filtered_results = StaticEnergy . objects . filter ( formula_reduced = \"NaCl\" , nsites__lte = 2 , ) . all () Convert the final structure from a database object (aka DatabaseStructure ) to a structure object (aka ToolkitStructure ). single_relaxation = StaticEnergy . objects . filter ( formula_reduced = \"NaCl\" , nsites__lte = 2 , ) . first () nacl_structure = single_relaxation . to_toolkit () For third-party data (like Material Project , AFLOW , COD , etc.) load the database table and then request to download all the available data: from simmate.database import connect # this connects to our database from simmate.database.third_parties import JarvisStructure # NOTE: This line is only needed if you did NOT accept the download # when running `simmate database reset`. # This only needs to ran once -- then data is stored locally. JarvisStructure . load_remote_archive () # now explore the data (only the first 150 structures here) first_150_rows = JarvisStructure . objects . all ()[: 150 ] dataframe = first_150_rows . to_dataframe () Warning To protect our servers, you can only call load_remote_archive() a few times per month. Don't overuse this feature.","title":"Quickstart"},{"location":"getting_started/access_the_database/quick_start/#search-the-database","text":"In this tutorial, you will learn how to explore your database as well as load data from third-party providers. Beginners will also be introduced to python inheritance. Tip This entire tutorial should be ran on your local computer because we will be using Spyder. To use results from your remote calculations, you can copy/paste your database file from your remote computer to where our local database one should be (~/simmate/my_env-database.sqlite3). Note, copy/pasting will no longer be necessary when we switch to a cloud database in the next tutorial.","title":"Search the database"},{"location":"getting_started/access_the_database/quick_start/#the-quick-tutorial","text":"Make sure you've initialized your database. This was done in tutorial 2 with the command simmate database reset . Do NOT rerun this command as it will empty your database and delete your results. Go to the simmate.database module to view all available tables. The table for Tutorial 2's results are located in the StaticEnergy datatable class, which can be loaded via either of these options: # OPTION 1 from simmate.database import connect # this connects to our database from simmate.database.workflow_results import StaticEnergy # OPTION 2 (recommended for convenience) from simmate.workflows.utilities import get_workflow workflow = get_workflow ( \"static-energy.vasp.mit\" ) table = workflow . database_table # results here is the same thing as StaticEnergy above View all the possible table columns with StaticEnergy.show_columns() View the full table as pandas dataframe with StaticEnergy.objects.to_dataframe() Filter results using django-based queries . For example: filtered_results = StaticEnergy . objects . filter ( formula_reduced = \"NaCl\" , nsites__lte = 2 , ) . all () Convert the final structure from a database object (aka DatabaseStructure ) to a structure object (aka ToolkitStructure ). single_relaxation = StaticEnergy . objects . filter ( formula_reduced = \"NaCl\" , nsites__lte = 2 , ) . first () nacl_structure = single_relaxation . to_toolkit () For third-party data (like Material Project , AFLOW , COD , etc.) load the database table and then request to download all the available data: from simmate.database import connect # this connects to our database from simmate.database.third_parties import JarvisStructure # NOTE: This line is only needed if you did NOT accept the download # when running `simmate database reset`. # This only needs to ran once -- then data is stored locally. JarvisStructure . load_remote_archive () # now explore the data (only the first 150 structures here) first_150_rows = JarvisStructure . objects . all ()[: 150 ] dataframe = first_150_rows . to_dataframe () Warning To protect our servers, you can only call load_remote_archive() a few times per month. Don't overuse this feature.","title":"The quick tutorial"},{"location":"getting_started/add_computational_resources/adding_clusters_and_workers/","text":"Setting up your cluster and workers \u00b6 Run vs. Run-cloud \u00b6 Scheduling a workflow is straight-forward. Simply change all your scripts and commands from the run method to the run_cloud method. For example, if you are using the command line: command line python simmate workflows run-cloud my_settings.yaml workflow . run_cloud ( ... ) This schedules your workflow, but it won't run yet. It is simply sitting in the queue and waiting for a worker to pick it up. Once we start a worker, then it will actually run. Start a \"single-flow\" worker \u00b6 Whereever you'd like to run the workflow, start a worker with: simmate workflow-engine start-singleflow-worker Danger If you are on a cluster, start-worker should be called within your submit script (e.g. inside submit.sh for SLURM). Don't run workers on the head node! Start a \"many-flow\" worker \u00b6 When you run this \"singleflow\" worker, you'll notice that the Worker will start, run 1 workflow, then shutdown. This is the recommended approach for HPC clusters because it follow best practices for sharing resources. You don't want a worker hogging computational resources if there aren't any workflows scheduled to run! However, if you would like more control over how many workflows are ran or even run a worker endlessly, you can use the command: simmate workflow-engine start-worker If your team runs many mini workflows that are under 5 minutes, starting/stopping workers could be a pain (sometimes it can take simmate up to 10 seconds to set everything up). That's a significant overhead and wasted computation time. To overcome this, we would run a worker that shuts down after 10 workflows or if the queue is empty: simmate workflow-engine start-worker --nitems-max 10 --close-on-empty-queue Starting many workers at once \u00b6 If you need to start many workers at once, you can use the start-cluster command as well. # starts 5 local workers simmate workflow-engine start-cluster 5 Controlling what workflows are ran by each worker \u00b6 Warning The full guide for custom workers is still being written. See workflow \"tags\" in the full guides for more information. Connecting others to your scheduler \u00b6 If they are connected to your database, then they're good to go! Other schedulers like Prefect or Dask require extra setup.","title":"Adding workers and clusters"},{"location":"getting_started/add_computational_resources/adding_clusters_and_workers/#setting-up-your-cluster-and-workers","text":"","title":"Setting up your cluster and workers"},{"location":"getting_started/add_computational_resources/adding_clusters_and_workers/#run-vs-run-cloud","text":"Scheduling a workflow is straight-forward. Simply change all your scripts and commands from the run method to the run_cloud method. For example, if you are using the command line: command line python simmate workflows run-cloud my_settings.yaml workflow . run_cloud ( ... ) This schedules your workflow, but it won't run yet. It is simply sitting in the queue and waiting for a worker to pick it up. Once we start a worker, then it will actually run.","title":"Run vs. Run-cloud"},{"location":"getting_started/add_computational_resources/adding_clusters_and_workers/#start-a-single-flow-worker","text":"Whereever you'd like to run the workflow, start a worker with: simmate workflow-engine start-singleflow-worker Danger If you are on a cluster, start-worker should be called within your submit script (e.g. inside submit.sh for SLURM). Don't run workers on the head node!","title":"Start a \"single-flow\" worker"},{"location":"getting_started/add_computational_resources/adding_clusters_and_workers/#start-a-many-flow-worker","text":"When you run this \"singleflow\" worker, you'll notice that the Worker will start, run 1 workflow, then shutdown. This is the recommended approach for HPC clusters because it follow best practices for sharing resources. You don't want a worker hogging computational resources if there aren't any workflows scheduled to run! However, if you would like more control over how many workflows are ran or even run a worker endlessly, you can use the command: simmate workflow-engine start-worker If your team runs many mini workflows that are under 5 minutes, starting/stopping workers could be a pain (sometimes it can take simmate up to 10 seconds to set everything up). That's a significant overhead and wasted computation time. To overcome this, we would run a worker that shuts down after 10 workflows or if the queue is empty: simmate workflow-engine start-worker --nitems-max 10 --close-on-empty-queue","title":"Start a \"many-flow\" worker"},{"location":"getting_started/add_computational_resources/adding_clusters_and_workers/#starting-many-workers-at-once","text":"If you need to start many workers at once, you can use the start-cluster command as well. # starts 5 local workers simmate workflow-engine start-cluster 5","title":"Starting many workers at once"},{"location":"getting_started/add_computational_resources/adding_clusters_and_workers/#controlling-what-workflows-are-ran-by-each-worker","text":"Warning The full guide for custom workers is still being written. See workflow \"tags\" in the full guides for more information.","title":"Controlling what workflows are ran by each worker"},{"location":"getting_started/add_computational_resources/adding_clusters_and_workers/#connecting-others-to-your-scheduler","text":"If they are connected to your database, then they're good to go! Other schedulers like Prefect or Dask require extra setup.","title":"Connecting others to your scheduler"},{"location":"getting_started/add_computational_resources/checklist_for_workers/","text":"A check-list for your workers \u00b6 Now that we know the terms scheduler , cluster , and worker (and also know whether we need these), we can start going through a check list to set everything up: configure your scheduler connect a cloud database connect to the scheduler register all custom projects/apps 1. The Scheduler \u00b6 If you stick to the \"SimmateExecutor\", then you're already all good to go! Nothing needs to be done. This is because the queue of job submissions is really just a database table inside the simmate database. Workers will queue this table and grab the first result. 2. Connecting to a Cloud Database \u00b6 We learned from previous tutorials that simmate (by default) writes results to a local file named ~/simmate/my_env-database.sqlite3 . We also learned that cloud databases let many different computers share data and access the database through an internet connection. Because SQLite3 (the default database engine) is not build for hundreds of connections and we often use separate computers to run workflows, you should build a cloud database. Therefore, don't skip tutorial 08 where we set up a cloud database! 3. Connecting to the Scheduler \u00b6 Because we are using the \"SimmateExecutor\" all we need is a connection to the cloud database. All you need to do make sure ALL of your computational resources are connected to the cloud database you configured. If your workers aren't picking up any of your workflow submissions, it's probably because you didn't connect them properly. 4. Connecting custom projects \u00b6 If you have custom database tables or code, it's important that (a) the cloud database knows about these tables and (b) your remote resources can access your custom code. Therefore, your custom project/app should be installed and accessible by all of your computation resources. Be sure to pip install your-cool-project for all computers. Tip Because SimmateExecutor uses cloudpickle when submitting tasks, many custom workflows will work just fine without this step. Our team is still working out how to guide users and make this as easy as possible. For now, we suggest just trying out your workflow when you skip this step -- as most times it will work. If not, then the text above explains why.","title":"Checklist for each worker"},{"location":"getting_started/add_computational_resources/checklist_for_workers/#a-check-list-for-your-workers","text":"Now that we know the terms scheduler , cluster , and worker (and also know whether we need these), we can start going through a check list to set everything up: configure your scheduler connect a cloud database connect to the scheduler register all custom projects/apps","title":"A check-list for your workers"},{"location":"getting_started/add_computational_resources/checklist_for_workers/#1-the-scheduler","text":"If you stick to the \"SimmateExecutor\", then you're already all good to go! Nothing needs to be done. This is because the queue of job submissions is really just a database table inside the simmate database. Workers will queue this table and grab the first result.","title":"1. The Scheduler"},{"location":"getting_started/add_computational_resources/checklist_for_workers/#2-connecting-to-a-cloud-database","text":"We learned from previous tutorials that simmate (by default) writes results to a local file named ~/simmate/my_env-database.sqlite3 . We also learned that cloud databases let many different computers share data and access the database through an internet connection. Because SQLite3 (the default database engine) is not build for hundreds of connections and we often use separate computers to run workflows, you should build a cloud database. Therefore, don't skip tutorial 08 where we set up a cloud database!","title":"2. Connecting to a Cloud Database"},{"location":"getting_started/add_computational_resources/checklist_for_workers/#3-connecting-to-the-scheduler","text":"Because we are using the \"SimmateExecutor\" all we need is a connection to the cloud database. All you need to do make sure ALL of your computational resources are connected to the cloud database you configured. If your workers aren't picking up any of your workflow submissions, it's probably because you didn't connect them properly.","title":"3. Connecting to the Scheduler"},{"location":"getting_started/add_computational_resources/checklist_for_workers/#4-connecting-custom-projects","text":"If you have custom database tables or code, it's important that (a) the cloud database knows about these tables and (b) your remote resources can access your custom code. Therefore, your custom project/app should be installed and accessible by all of your computation resources. Be sure to pip install your-cool-project for all computers. Tip Because SimmateExecutor uses cloudpickle when submitting tasks, many custom workflows will work just fine without this step. Our team is still working out how to guide users and make this as easy as possible. For now, we suggest just trying out your workflow when you skip this step -- as most times it will work. If not, then the text above explains why.","title":"4. Connecting custom projects"},{"location":"getting_started/add_computational_resources/intro_to_clusters/","text":"Intro to engine concepts \u00b6 Overview \u00b6 Recall from our earlier tutorial, there are 4 steps to a workflow ( configure , schedule , execute , and save ). This tutorial will give an overview of how to modify the schedule and determine which computer execute is called on. Up until now, we have been using the default behavior for these two steps. But now we want to instead do the following: schedule : submit the workflow to a queue of many other workflows execute : run the calculation on a remote cluster Visualizing our setup \u00b6 The following schematic will help with understanding the concepts described below. Take a moment to understand the organization our resources and use this as a reference when reading below. general setup example (the Warren lab) graph TD; A[user 1]-->E[scheduler]; B[user 2]-->E; C[user 3]-->E; D[user 4]-->E; E-->F[cluster 1]; E-->G[cluster 2]; E-->H[cluster 3]; F-->I[worker 1]; F-->J[worker 2]; F-->K[worker 3]; G-->L[worker 4]; G-->M[worker 5]; G-->N[worker 6]; H-->O[worker 7]; H-->P[worker 8]; H-->Q[worker 9]; graph TD; A[Jack's submissions]-->E[cloud database]; B[Scott's submissions]-->E; C[Lauren's submissions]-->E; D[Siona's submissions]-->E; E-->F[WarWulf]; E-->G[LongLeaf]; E-->H[DogWood]; F-->I[slurm job 1]; F-->J[slurm job 2]; F-->K[slurm job 3]; G-->L[slurm job 4]; G-->M[slurm job 5]; G-->N[slurm job 6]; H-->O[slurm job 7]; H-->P[slurm job 8]; H-->Q[slurm job 9]; What is a scheduler? \u00b6 A scheduler is something we submit workflows to and it is what controls when & where to run workflows. The terms \"scheduler\" and \"executor\" are sometimes used interchangeably. As a bunch of workflows are submitted, our scheduler forms a queue and keeps track of which ones to run next. To do this, we can use the built-in SimmateExecutor , Dask , or Prefect as our scheduler. For this tutorial, we will use the SimmateExecutor because it is the default one and it's already set up for us. What is a cluster? \u00b6 A cluster is a group of computational resources that actually run the workflows. So our scheduler will find whichever workflow should be ran next, and send it to our cluster to run. Clusters are often made up of workers -- where a worker is just a single resource and it works through one job at a time. For example, say we have 10 computers (or slurm jobs) that each run one workflow at a time. All computers together are our cluster. Each computer is a worker. At any given time, 10 workflows will be running because each worker will have one it is in charge of. Because we are using the SimmateExectuor , we will be using SimmateWorker s to set up each worker and therefore our cluster. Set-up for each worker is the same -- whether your resources are on a cloud service, a supercomputer, or just simple desktops.","title":"Intro to clusters"},{"location":"getting_started/add_computational_resources/intro_to_clusters/#intro-to-engine-concepts","text":"","title":"Intro to engine concepts"},{"location":"getting_started/add_computational_resources/intro_to_clusters/#overview","text":"Recall from our earlier tutorial, there are 4 steps to a workflow ( configure , schedule , execute , and save ). This tutorial will give an overview of how to modify the schedule and determine which computer execute is called on. Up until now, we have been using the default behavior for these two steps. But now we want to instead do the following: schedule : submit the workflow to a queue of many other workflows execute : run the calculation on a remote cluster","title":"Overview"},{"location":"getting_started/add_computational_resources/intro_to_clusters/#visualizing-our-setup","text":"The following schematic will help with understanding the concepts described below. Take a moment to understand the organization our resources and use this as a reference when reading below. general setup example (the Warren lab) graph TD; A[user 1]-->E[scheduler]; B[user 2]-->E; C[user 3]-->E; D[user 4]-->E; E-->F[cluster 1]; E-->G[cluster 2]; E-->H[cluster 3]; F-->I[worker 1]; F-->J[worker 2]; F-->K[worker 3]; G-->L[worker 4]; G-->M[worker 5]; G-->N[worker 6]; H-->O[worker 7]; H-->P[worker 8]; H-->Q[worker 9]; graph TD; A[Jack's submissions]-->E[cloud database]; B[Scott's submissions]-->E; C[Lauren's submissions]-->E; D[Siona's submissions]-->E; E-->F[WarWulf]; E-->G[LongLeaf]; E-->H[DogWood]; F-->I[slurm job 1]; F-->J[slurm job 2]; F-->K[slurm job 3]; G-->L[slurm job 4]; G-->M[slurm job 5]; G-->N[slurm job 6]; H-->O[slurm job 7]; H-->P[slurm job 8]; H-->Q[slurm job 9];","title":"Visualizing our setup"},{"location":"getting_started/add_computational_resources/intro_to_clusters/#what-is-a-scheduler","text":"A scheduler is something we submit workflows to and it is what controls when & where to run workflows. The terms \"scheduler\" and \"executor\" are sometimes used interchangeably. As a bunch of workflows are submitted, our scheduler forms a queue and keeps track of which ones to run next. To do this, we can use the built-in SimmateExecutor , Dask , or Prefect as our scheduler. For this tutorial, we will use the SimmateExecutor because it is the default one and it's already set up for us.","title":"What is a scheduler?"},{"location":"getting_started/add_computational_resources/intro_to_clusters/#what-is-a-cluster","text":"A cluster is a group of computational resources that actually run the workflows. So our scheduler will find whichever workflow should be ran next, and send it to our cluster to run. Clusters are often made up of workers -- where a worker is just a single resource and it works through one job at a time. For example, say we have 10 computers (or slurm jobs) that each run one workflow at a time. All computers together are our cluster. Each computer is a worker. At any given time, 10 workflows will be running because each worker will have one it is in charge of. Because we are using the SimmateExectuor , we will be using SimmateWorker s to set up each worker and therefore our cluster. Set-up for each worker is the same -- whether your resources are on a cloud service, a supercomputer, or just simple desktops.","title":"What is a cluster?"},{"location":"getting_started/add_computational_resources/quick_start/","text":"Add computational resources \u00b6 In this tutorial, you will learn how to run workflows on distributed computational resources -- with full scheduling and monitoring. Note This tutorial will use the default scheduler/executor, \"SimmateExecutor\". However, you can also use Prefect and/or Dask to build out your cluster. This is covered elsewhere, but it is not recommended at the moment. The quick tutorial \u00b6 Be aware that you can share a cloud database without sharing computational resources. This flexibility is very important for many collaborations. Just like with your cloud database, designate a point-person to manage your private computational resources. Everyone else only needs to switch from run to run_cloud . If your computational resources are distributed on different computers, make sure you have set up a cloud database (see the previous tutorial on how to do this). If you want to schedule AND run things entirely on your local computer (or file system), then you can skip this step. If you have remote resources, make sure you have ALL simmate installations connected to the same database (i.e. your database connection file should be on all resources). If you have custom workflows, make sure you are using a simmate project and all resources have this app installed. However, if you don't have custom database tables, you can try continuing without this step -- but registering via an app is the only way to guarantee that the workflow will run properly. Schedule your simmate workflows by switching from the run method to the run_cloud method. This workflow will be scheduled but it won't run until we start a worker: simmate workflows run-cloud my_settings.yaml Wherever you'd like to run the workflow, start a worker with the following command. If you are on a cluster, start-worker should be called within your submit script (e.g. inside submit.sh for SLURM). Don't run workers on the head node. simmate workflow-engine start-singleflow-worker Note this \"singleflow\" worker will start, run 1 workflow, then shutdown. If you would like more control over how many workflows are ran or even run a worker endlessly, you can use the command: simmate workflow-engine start-worker Scale out your cluster! Start workers anywhere you'd like, and start as many as you'd like. Just make sure you follow steps 4 and 5 for every worker. If you need to start many workers at once, you can use the start-cluster command as well. # starts 5 local workers simmate workflow-engine start-cluster 5 To control which workers run which workflows, use tags. Workers will only pick up submissions that have matching tags. # when submitting simmate workflows run-cloud ... -t my_tag -t small-job # when starting the worker (typically on a different computer) simmate workflow-engine start-worker -t small-job To let others use your cluster, simply connect them to the same database.","title":"Quickstart"},{"location":"getting_started/add_computational_resources/quick_start/#add-computational-resources","text":"In this tutorial, you will learn how to run workflows on distributed computational resources -- with full scheduling and monitoring. Note This tutorial will use the default scheduler/executor, \"SimmateExecutor\". However, you can also use Prefect and/or Dask to build out your cluster. This is covered elsewhere, but it is not recommended at the moment.","title":"Add computational resources"},{"location":"getting_started/add_computational_resources/quick_start/#the-quick-tutorial","text":"Be aware that you can share a cloud database without sharing computational resources. This flexibility is very important for many collaborations. Just like with your cloud database, designate a point-person to manage your private computational resources. Everyone else only needs to switch from run to run_cloud . If your computational resources are distributed on different computers, make sure you have set up a cloud database (see the previous tutorial on how to do this). If you want to schedule AND run things entirely on your local computer (or file system), then you can skip this step. If you have remote resources, make sure you have ALL simmate installations connected to the same database (i.e. your database connection file should be on all resources). If you have custom workflows, make sure you are using a simmate project and all resources have this app installed. However, if you don't have custom database tables, you can try continuing without this step -- but registering via an app is the only way to guarantee that the workflow will run properly. Schedule your simmate workflows by switching from the run method to the run_cloud method. This workflow will be scheduled but it won't run until we start a worker: simmate workflows run-cloud my_settings.yaml Wherever you'd like to run the workflow, start a worker with the following command. If you are on a cluster, start-worker should be called within your submit script (e.g. inside submit.sh for SLURM). Don't run workers on the head node. simmate workflow-engine start-singleflow-worker Note this \"singleflow\" worker will start, run 1 workflow, then shutdown. If you would like more control over how many workflows are ran or even run a worker endlessly, you can use the command: simmate workflow-engine start-worker Scale out your cluster! Start workers anywhere you'd like, and start as many as you'd like. Just make sure you follow steps 4 and 5 for every worker. If you need to start many workers at once, you can use the start-cluster command as well. # starts 5 local workers simmate workflow-engine start-cluster 5 To control which workers run which workflows, use tags. Workers will only pick up submissions that have matching tags. # when submitting simmate workflows run-cloud ... -t my_tag -t small-job # when starting the worker (typically on a different computer) simmate workflow-engine start-worker -t small-job To let others use your cluster, simply connect them to the same database.","title":"The quick tutorial"},{"location":"getting_started/add_computational_resources/should_you_setup/","text":"Should I set up my own cluster? \u00b6 Note We understand that resource restrictions can be very stingent between labs and companies. So sharing resources is not possible, even in close collaborations. Simmate addresses this issue by making it easy to share a cloud database without sharing computational resources. In other words, you can contribute to a shared database without letting others see/access your computational resources. Basic use of resources \u00b6 Each team will likely need to handle their own computational resources, which can be any number of things: a university or federal HPC cluster with SLURM, PBS, or some other queue system a single node or even a Kubernetes cluster on a cloud provider a series of desktop computers that your lab shares any combination of these resources The easiest way to use these resources is to sign on and run a simmate workflow using the run method. When this is done, the workflow runs directly on your resource. This was done in the \"Run a workflow\" tutorial when we called simmate workflows run ... . When on a HPC SLURM cluster, we would run simmate using a submit.sh : #!/bin/bash #SBATCH --output=slurm.out #SBATCH --nodes=1 #SBATCH --ntasks=2 simmate workflows run-yaml my_settings.yaml > simmate.out Tip If you are only running a few workflows per day (<10), we recommend you stick to running workflows in this way. That is, just calling simmate workflows run . Don't overcomplicate things. Go back to tutorial 02 to review these concepts. Deciding to set up workers \u00b6 If your team is submitting hundreds or thousands of workflows at a time, then it would be extremely useful to monitor and orchestrate these workflows using a scheduler and cluster . Just like with our cloud database in the previous tutorial, you only need ONE person to manage ALL of your computational resources. Once the resources have been set up, the other users can connect using the database connection file (or an API key if you are using Prefect). If you are that one person for your team, then continue with this tutorial. If not, then you should instead talk with your point person! Using your teams resources should be as easy as switching from the run to run_cloud method.","title":"Should you set up workers?"},{"location":"getting_started/add_computational_resources/should_you_setup/#should-i-set-up-my-own-cluster","text":"Note We understand that resource restrictions can be very stingent between labs and companies. So sharing resources is not possible, even in close collaborations. Simmate addresses this issue by making it easy to share a cloud database without sharing computational resources. In other words, you can contribute to a shared database without letting others see/access your computational resources.","title":"Should I set up my own cluster?"},{"location":"getting_started/add_computational_resources/should_you_setup/#basic-use-of-resources","text":"Each team will likely need to handle their own computational resources, which can be any number of things: a university or federal HPC cluster with SLURM, PBS, or some other queue system a single node or even a Kubernetes cluster on a cloud provider a series of desktop computers that your lab shares any combination of these resources The easiest way to use these resources is to sign on and run a simmate workflow using the run method. When this is done, the workflow runs directly on your resource. This was done in the \"Run a workflow\" tutorial when we called simmate workflows run ... . When on a HPC SLURM cluster, we would run simmate using a submit.sh : #!/bin/bash #SBATCH --output=slurm.out #SBATCH --nodes=1 #SBATCH --ntasks=2 simmate workflows run-yaml my_settings.yaml > simmate.out Tip If you are only running a few workflows per day (<10), we recommend you stick to running workflows in this way. That is, just calling simmate workflows run . Don't overcomplicate things. Go back to tutorial 02 to review these concepts.","title":"Basic use of resources"},{"location":"getting_started/add_computational_resources/should_you_setup/#deciding-to-set-up-workers","text":"If your team is submitting hundreds or thousands of workflows at a time, then it would be extremely useful to monitor and orchestrate these workflows using a scheduler and cluster . Just like with our cloud database in the previous tutorial, you only need ONE person to manage ALL of your computational resources. Once the resources have been set up, the other users can connect using the database connection file (or an API key if you are using Prefect). If you are that one person for your team, then continue with this tutorial. If not, then you should instead talk with your point person! Using your teams resources should be as easy as switching from the run to run_cloud method.","title":"Deciding to set up workers"},{"location":"getting_started/analyze_and_modify_structures/advanced_classes/","text":"Advanced classes \u00b6 Beyond the Structure class \u00b6 To give you a sneak-peak of some advanced classes and functionality, we outline some examples in this section. Note that Simmate is still early in development, so there are many more features available through PyMatGen and MatMiner packages, which were installed alongside Simmate. Tip If you're trying to follow a paper and analyze a structure, odds are that someone made a class/function for that analysis! Be sure to search around the documentation for it (next tutorial teaches you how to do this) or just post a question and we'll point you in the right direction. Example 1: Structure Creation \u00b6 Simmate's toolkit is (at the moment) most useful for structure creation. This includes creating structures from random symmetry, prototype structures, and more. We only need to give these \"creator\" classes a composition object: from simmate.toolkit import Composition from simmate.toolkit.creators import RandomSymStructure composition = Composition ( \"Ca2N\" ) creator = RandomSymStructure ( composition ) structure1 = creator . create_structure ( spacegroup = 166 ) structure2 = creator . create_structure ( spacegroup = 225 ) Example 2: Fingerprint analysis \u00b6 Matminer is particularly useful for analyzing \"features\" of a structure and making machine-learning inputs. One common analysis gives the \"fingerprint\" of the structure, which helps characterize bonding in the crystal. The most basic fingerprint is the radial distribution function (rdf) -- it shows the distance of all atoms from one another. We take any structure object and can feed it to a matminer Featurizer object: from matminer.featurizers.structure.rdf import RadialDistributionFunction rdf_analyzer = RadialDistributionFunction ( bin_size = 0.1 ) rdf1 = rdf_analyzer . featurize ( structure1 ) rdf2 = rdf_analyzer . featurize ( structure2 ) We can plot an RDF using python too. Matminer doesn't have a convenient way to plot this for us yet (with Simmate there would be a show_plot() method), so we can use this opportunity to learn how to plot things ourselves: import matplotlib.pyplot as plt # The x-axis goes from 0.1 to 20 in steps 0.1 (in Angstroms). # Matminer doesn't give us a list of these values but # we can make it using this line. rdf_x = [ n / 10 + 0.1 for n in range ( 0 , 200 )] # Make a simple line plot with lists of (x,y) values plt . plot ( rdf_x , rdf1 ) # Show the plot without any fancy formatting or labels. plt . show () Example 3: Structure matching \u00b6 Pymatgen is currently the largest package and has the most toolkit-like features. As an example, it's common to compare two structures and see if are symmetrically equivalent (within a given tolerance). You give it two structures and it will return True or False on whether they are matching: from pymatgen.analysis.structure_matcher import StructureMatcher matcher = StructureMatcher () # Now compare our two random structures! # This should give False. Check in your Spyder variable explorer. is_matching = matcher . fit ( structure1 , structure2 )","title":"Advanced classes"},{"location":"getting_started/analyze_and_modify_structures/advanced_classes/#advanced-classes","text":"","title":"Advanced classes"},{"location":"getting_started/analyze_and_modify_structures/advanced_classes/#beyond-the-structure-class","text":"To give you a sneak-peak of some advanced classes and functionality, we outline some examples in this section. Note that Simmate is still early in development, so there are many more features available through PyMatGen and MatMiner packages, which were installed alongside Simmate. Tip If you're trying to follow a paper and analyze a structure, odds are that someone made a class/function for that analysis! Be sure to search around the documentation for it (next tutorial teaches you how to do this) or just post a question and we'll point you in the right direction.","title":"Beyond the Structure class"},{"location":"getting_started/analyze_and_modify_structures/advanced_classes/#example-1-structure-creation","text":"Simmate's toolkit is (at the moment) most useful for structure creation. This includes creating structures from random symmetry, prototype structures, and more. We only need to give these \"creator\" classes a composition object: from simmate.toolkit import Composition from simmate.toolkit.creators import RandomSymStructure composition = Composition ( \"Ca2N\" ) creator = RandomSymStructure ( composition ) structure1 = creator . create_structure ( spacegroup = 166 ) structure2 = creator . create_structure ( spacegroup = 225 )","title":"Example 1: Structure Creation"},{"location":"getting_started/analyze_and_modify_structures/advanced_classes/#example-2-fingerprint-analysis","text":"Matminer is particularly useful for analyzing \"features\" of a structure and making machine-learning inputs. One common analysis gives the \"fingerprint\" of the structure, which helps characterize bonding in the crystal. The most basic fingerprint is the radial distribution function (rdf) -- it shows the distance of all atoms from one another. We take any structure object and can feed it to a matminer Featurizer object: from matminer.featurizers.structure.rdf import RadialDistributionFunction rdf_analyzer = RadialDistributionFunction ( bin_size = 0.1 ) rdf1 = rdf_analyzer . featurize ( structure1 ) rdf2 = rdf_analyzer . featurize ( structure2 ) We can plot an RDF using python too. Matminer doesn't have a convenient way to plot this for us yet (with Simmate there would be a show_plot() method), so we can use this opportunity to learn how to plot things ourselves: import matplotlib.pyplot as plt # The x-axis goes from 0.1 to 20 in steps 0.1 (in Angstroms). # Matminer doesn't give us a list of these values but # we can make it using this line. rdf_x = [ n / 10 + 0.1 for n in range ( 0 , 200 )] # Make a simple line plot with lists of (x,y) values plt . plot ( rdf_x , rdf1 ) # Show the plot without any fancy formatting or labels. plt . show ()","title":"Example 2: Fingerprint analysis"},{"location":"getting_started/analyze_and_modify_structures/advanced_classes/#example-3-structure-matching","text":"Pymatgen is currently the largest package and has the most toolkit-like features. As an example, it's common to compare two structures and see if are symmetrically equivalent (within a given tolerance). You give it two structures and it will return True or False on whether they are matching: from pymatgen.analysis.structure_matcher import StructureMatcher matcher = StructureMatcher () # Now compare our two random structures! # This should give False. Check in your Spyder variable explorer. is_matching = matcher . fit ( structure1 , structure2 )","title":"Example 3: Structure matching"},{"location":"getting_started/analyze_and_modify_structures/extra_resources/","text":"Extra resources \u00b6 Before going to the next tutorial, we recommend going through the following guides. These will save you TONS of time in the long run -- so don't rush ahead. Introduction to Spyder --> watch the 3 videos (each is under 4 minutes) Codecademy's python lessons --> spend 2-3 days going through if you're new to python","title":"Extra learning resources"},{"location":"getting_started/analyze_and_modify_structures/extra_resources/#extra-resources","text":"Before going to the next tutorial, we recommend going through the following guides. These will save you TONS of time in the long run -- so don't rush ahead. Introduction to Spyder --> watch the 3 videos (each is under 4 minutes) Codecademy's python lessons --> spend 2-3 days going through if you're new to python","title":"Extra resources"},{"location":"getting_started/analyze_and_modify_structures/intro_to_spyder/","text":"An introduction to Spyder \u00b6 Prebuilt workflows can only take you so far when carrying out materials research. At some point, you'll want to make new crystal structures, modify them in some way, and perform a novel analysis. To do all of this, we turn away from the command-line and now start using python. Intro lessons for python \u00b6 If you are a brand new to coding and python, we will introduce the bare-minimum of python needed to start using simmate's toolkit in this tutorial, but we highly recommend spending 2-3 days on learning all the python fundamentals. Codecademy's python lessons are a great way to learn without installing anything extra. Tip Spending a full day with these tutorials (or even more time) will certainly save you time and headache in the long rule. We highly recommend taking the time to go through these. Selecting our IDE \u00b6 Ready or not, let's learn how to use Simmate's python code. Recall from the Installation tutorial: Anaconda gave us a bunch of programs on their home screen, such as Orange3, Jupyter Notebook, Spyder, and others. These programs are for you to write your own python code. Just like how there is Microsoft Word, Google Docs, and LibreOffice for writing papers -- all of these programs are different ways to write Python. Our team uses Spyder, so we highly recommend users pick Spyder too. Opening Spyder \u00b6 If you followed the installation tutorial exactly, you should have Spyder installed and ready to go. To open it, search for Spyder in your computer's apps (use the searchbar on the bottom-left of your screen on windows 10) and select Spyder (my_env) . Your Spyder will be empty when you first open it up, but here's a showcase of Spyder in full swing: For this tutorial, we will only be using the Python console (bottom-right of the screen). Tip If you are comfortable with python or took the Codecademy intro course, you can get up to speed with Spyder with their intro videos. There are 3 videos, and each is under 4 minutes .","title":"Intro to Spyder (IDE)"},{"location":"getting_started/analyze_and_modify_structures/intro_to_spyder/#an-introduction-to-spyder","text":"Prebuilt workflows can only take you so far when carrying out materials research. At some point, you'll want to make new crystal structures, modify them in some way, and perform a novel analysis. To do all of this, we turn away from the command-line and now start using python.","title":"An introduction to Spyder"},{"location":"getting_started/analyze_and_modify_structures/intro_to_spyder/#intro-lessons-for-python","text":"If you are a brand new to coding and python, we will introduce the bare-minimum of python needed to start using simmate's toolkit in this tutorial, but we highly recommend spending 2-3 days on learning all the python fundamentals. Codecademy's python lessons are a great way to learn without installing anything extra. Tip Spending a full day with these tutorials (or even more time) will certainly save you time and headache in the long rule. We highly recommend taking the time to go through these.","title":"Intro lessons for python"},{"location":"getting_started/analyze_and_modify_structures/intro_to_spyder/#selecting-our-ide","text":"Ready or not, let's learn how to use Simmate's python code. Recall from the Installation tutorial: Anaconda gave us a bunch of programs on their home screen, such as Orange3, Jupyter Notebook, Spyder, and others. These programs are for you to write your own python code. Just like how there is Microsoft Word, Google Docs, and LibreOffice for writing papers -- all of these programs are different ways to write Python. Our team uses Spyder, so we highly recommend users pick Spyder too.","title":"Selecting our IDE"},{"location":"getting_started/analyze_and_modify_structures/intro_to_spyder/#opening-spyder","text":"If you followed the installation tutorial exactly, you should have Spyder installed and ready to go. To open it, search for Spyder in your computer's apps (use the searchbar on the bottom-left of your screen on windows 10) and select Spyder (my_env) . Your Spyder will be empty when you first open it up, but here's a showcase of Spyder in full swing: For this tutorial, we will only be using the Python console (bottom-right of the screen). Tip If you are comfortable with python or took the Codecademy intro course, you can get up to speed with Spyder with their intro videos. There are 3 videos, and each is under 4 minutes .","title":"Opening Spyder"},{"location":"getting_started/analyze_and_modify_structures/quick_start/","text":"Analyze and modify structures \u00b6 In this tutorial, you will learn about the most important class in Simmate: the Structure class. Beginners will also be introduced to classes and the Spyder IDE. Note Simmate uses pymatgen under the hood, so this tutorial is really one for using their package. Their guides also are useful, but they are written for those already familiar with python. The quick tutorial \u00b6 In Spyder's python console, run import simmate to make sure everything is set up. Make sure you still have our POSCAR file of NaCl from the last tutorial. Now let's explore the toolkit and a few of its features! Load the structure into python and then write it to another file format: from simmate.toolkit import Structure structure = Structure . from_file ( \"POSCAR\" ) structure . to ( \"cif\" , \"NaCl.cif\" ) The Structure class (aka a ToolkitStructure ) provides many extra properties and methods, so nearly all functions in Simmate use it as an input. This includes running workflows like we did in the previous tutorial. All available workflows can be loaded from the simmate.workflows module: from simmate.workflows.relaxation import mit_workflow result = mit_workflow . run ( structure = structure ) Grab varius properties of the structure, lattice, and composition: # explore structure-based properties structure . density structure . distance_matrix structure . cart_coords structure . num_sites # grab the structure's composition and access it's properties composition = structure . composition composition . reduced_formula composition . elements # grab the structure's lattice and access it's properties lattice = structure . lattice lattice . volume lattice . matrix lattice . beta Make new structures using some tranformation or analysis: structure . add_oxidation_state_by_guess () structure . make_supercell ([ 2 , 2 , 2 ]) new_structure = structure . get_primitive_structure () What about advanced features? Simmate is slowly adding these to our toolkit module, but many more are available through PyMatGen and MatMiner (which are preinstalled for you). # Simmate is in the process of adding new features. One example # creating a random structure from a spacegroup and composition from simmate.toolkit import Composition from simmate.toolkit.creators import RandomSymStructure composition = Composition ( \"Ca2N\" ) creator = RandomSymStructure ( composition ) structure1 = creator . create_structure ( spacegroup = 166 ) structure2 = creator . create_structure ( spacegroup = 225 ) # ---------------------------------------------------------------------- # Matminer is handy for analyzing structures and making # machine-learning inputs. One common analysis is the generating # a RDF fingerprint to help analyze bonding and compare structures from matminer.featurizers.structure.rdf import RadialDistributionFunction rdf_analyzer = RadialDistributionFunction ( bin_size = 0.1 ) rdf1 = rdf_analyzer . featurize ( structure1 ) rdf2 = rdf_analyzer . featurize ( structure2 ) # ---------------------------------------------------------------------- # Pymatgen currently has the most functionality. One common # function is checking if two structures are symmetrically # equivalent (under some tolerance). from pymatgen.analysis.structure_matcher import StructureMatcher matcher = StructureMatcher () # Now compare our two random structures! # This should give False. Check in your Spyder variable explorer. is_matching = matcher . fit ( structure1 , structure2 ) There are many more features available! If you can't find what you're looking for, be sure to ask for help before trying to code something on your own. Chances are that the feature exists somewhere -- and if we don't have it, we'll direct you to a package that does.","title":"Quickstart"},{"location":"getting_started/analyze_and_modify_structures/quick_start/#analyze-and-modify-structures","text":"In this tutorial, you will learn about the most important class in Simmate: the Structure class. Beginners will also be introduced to classes and the Spyder IDE. Note Simmate uses pymatgen under the hood, so this tutorial is really one for using their package. Their guides also are useful, but they are written for those already familiar with python.","title":"Analyze and modify structures"},{"location":"getting_started/analyze_and_modify_structures/quick_start/#the-quick-tutorial","text":"In Spyder's python console, run import simmate to make sure everything is set up. Make sure you still have our POSCAR file of NaCl from the last tutorial. Now let's explore the toolkit and a few of its features! Load the structure into python and then write it to another file format: from simmate.toolkit import Structure structure = Structure . from_file ( \"POSCAR\" ) structure . to ( \"cif\" , \"NaCl.cif\" ) The Structure class (aka a ToolkitStructure ) provides many extra properties and methods, so nearly all functions in Simmate use it as an input. This includes running workflows like we did in the previous tutorial. All available workflows can be loaded from the simmate.workflows module: from simmate.workflows.relaxation import mit_workflow result = mit_workflow . run ( structure = structure ) Grab varius properties of the structure, lattice, and composition: # explore structure-based properties structure . density structure . distance_matrix structure . cart_coords structure . num_sites # grab the structure's composition and access it's properties composition = structure . composition composition . reduced_formula composition . elements # grab the structure's lattice and access it's properties lattice = structure . lattice lattice . volume lattice . matrix lattice . beta Make new structures using some tranformation or analysis: structure . add_oxidation_state_by_guess () structure . make_supercell ([ 2 , 2 , 2 ]) new_structure = structure . get_primitive_structure () What about advanced features? Simmate is slowly adding these to our toolkit module, but many more are available through PyMatGen and MatMiner (which are preinstalled for you). # Simmate is in the process of adding new features. One example # creating a random structure from a spacegroup and composition from simmate.toolkit import Composition from simmate.toolkit.creators import RandomSymStructure composition = Composition ( \"Ca2N\" ) creator = RandomSymStructure ( composition ) structure1 = creator . create_structure ( spacegroup = 166 ) structure2 = creator . create_structure ( spacegroup = 225 ) # ---------------------------------------------------------------------- # Matminer is handy for analyzing structures and making # machine-learning inputs. One common analysis is the generating # a RDF fingerprint to help analyze bonding and compare structures from matminer.featurizers.structure.rdf import RadialDistributionFunction rdf_analyzer = RadialDistributionFunction ( bin_size = 0.1 ) rdf1 = rdf_analyzer . featurize ( structure1 ) rdf2 = rdf_analyzer . featurize ( structure2 ) # ---------------------------------------------------------------------- # Pymatgen currently has the most functionality. One common # function is checking if two structures are symmetrically # equivalent (under some tolerance). from pymatgen.analysis.structure_matcher import StructureMatcher matcher = StructureMatcher () # Now compare our two random structures! # This should give False. Check in your Spyder variable explorer. is_matching = matcher . fit ( structure1 , structure2 ) There are many more features available! If you can't find what you're looking for, be sure to ask for help before trying to code something on your own. Chances are that the feature exists somewhere -- and if we don't have it, we'll direct you to a package that does.","title":"The quick tutorial"},{"location":"getting_started/analyze_and_modify_structures/structure_methods/","text":"Methods of a Structure object \u00b6 Properties vs Methods \u00b6 In addition to properties , an object can also have methods . Typically, a method will perform a calculation; it may also save the results of that calculation. For example, you might want to convert a conventional unit cell into a primitive unit cell. You can do this with nacl_structure.get_primitive_structure() . Try this in your python console: nacl_structure . get_primitive_structure () You'll see it prints out a new structure. We had the primitive structure already, so it should be identical to our output from before. Save the output to a new variable \u00b6 For this method, we save it as a new Structure object by assigning it to the name nacl_prim : nacl_prim = nacl_structure . get_primitive_structure () nacl_prim . density Giving parameters to methods \u00b6 Note that all methods end with parentheses () . This allows us to alter the method. For example, get_primitive_structure() makes use of symmetry in its calculation, so if we do: nacl_structure . get_primitive_structure ( tolerance = 0.1 ) The calculation will allow atoms that are nearly in their 'symmetrically correct' positions (within 0.1 \u00c5) to be identiified as symmetrical. Note, we didn't have to set a tolerance before because there are default values being used. Some methods don't have defaults. For example, the make_supercell method require you to specify a supercell size. There are many other methods available for structures too: nacl_structure . add_oxidation_state_by_guess () nacl_structure . make_supercell ([ 2 , 2 , 2 ]) Exploring other methods and properties \u00b6 To get a quick look at all of the properties and methods available, type nacl_structure into the terminal but don't hit enter yet. Then add a period so you have nacl_structure. , and finally, hit tab . You should see a list pop up with everything available. Select lattice, then do this tab trick again. The list should look similar to this (this image isn't for a structure object though): This can be done with any class or any object. There are many different classes in Simmate, but you'll interact with Structure the most. To fully understand all of the options for these classes, you'll need to explore the code's documentation, which we will cover in the next guide.","title":"Structure methods"},{"location":"getting_started/analyze_and_modify_structures/structure_methods/#methods-of-a-structure-object","text":"","title":"Methods of a Structure object"},{"location":"getting_started/analyze_and_modify_structures/structure_methods/#properties-vs-methods","text":"In addition to properties , an object can also have methods . Typically, a method will perform a calculation; it may also save the results of that calculation. For example, you might want to convert a conventional unit cell into a primitive unit cell. You can do this with nacl_structure.get_primitive_structure() . Try this in your python console: nacl_structure . get_primitive_structure () You'll see it prints out a new structure. We had the primitive structure already, so it should be identical to our output from before.","title":"Properties vs Methods"},{"location":"getting_started/analyze_and_modify_structures/structure_methods/#save-the-output-to-a-new-variable","text":"For this method, we save it as a new Structure object by assigning it to the name nacl_prim : nacl_prim = nacl_structure . get_primitive_structure () nacl_prim . density","title":"Save the output to a new variable"},{"location":"getting_started/analyze_and_modify_structures/structure_methods/#giving-parameters-to-methods","text":"Note that all methods end with parentheses () . This allows us to alter the method. For example, get_primitive_structure() makes use of symmetry in its calculation, so if we do: nacl_structure . get_primitive_structure ( tolerance = 0.1 ) The calculation will allow atoms that are nearly in their 'symmetrically correct' positions (within 0.1 \u00c5) to be identiified as symmetrical. Note, we didn't have to set a tolerance before because there are default values being used. Some methods don't have defaults. For example, the make_supercell method require you to specify a supercell size. There are many other methods available for structures too: nacl_structure . add_oxidation_state_by_guess () nacl_structure . make_supercell ([ 2 , 2 , 2 ])","title":"Giving parameters to methods"},{"location":"getting_started/analyze_and_modify_structures/structure_methods/#exploring-other-methods-and-properties","text":"To get a quick look at all of the properties and methods available, type nacl_structure into the terminal but don't hit enter yet. Then add a period so you have nacl_structure. , and finally, hit tab . You should see a list pop up with everything available. Select lattice, then do this tab trick again. The list should look similar to this (this image isn't for a structure object though): This can be done with any class or any object. There are many different classes in Simmate, but you'll interact with Structure the most. To fully understand all of the options for these classes, you'll need to explore the code's documentation, which we will cover in the next guide.","title":"Exploring other methods and properties"},{"location":"getting_started/analyze_and_modify_structures/structure_properties/","text":"Properties of a Structure object \u00b6 Classes give us properties \u00b6 We make classes because they allow us to automate common calculations. For example, all structures have a density , which is easily calculated once you know the lattice and atomic sites. The formula to calculate is always the same, so we can automate the calculation. You can access this and other properties through our structure object. Try typing nacl_structure.density in the python terminal and hit enter. It should tell you the density of our structure: nacl_structure . density Lattice properties \u00b6 Now what about other properties for the lattice, like volume, angles, and vectors? For the sake of organization, the Structure class has an associated class called Lattice , and within the lattice object we find properties like volume . Try out these in your python terminal (only run one line at a time): nacl_structure . lattice . volume nacl_structure . lattice . matrix nacl_structure . lattice . beta Your outputs will be... # EXPECTED OUTPUTS 46.09614820053437 array([[3.48543651, 0. , 2.01231771], [1.16181217, 3.28610106, 2.01231771], [0. , 0. , 4.02463542]]) 59.99999999999999 If you don't like to type long lines, there is a shortcut. We save the Lattice object (here, we call it as nacl_lat , but you can pick a different name) to a new variable name and then call its properties: nacl_lat = nacl_structure . lattice nacl_lat . volume nacl_lat . matrix nacl_lat . beta Note, outputs will be the same as above. Composition properties \u00b6 We can apply the same idea to other Structure sub-classes, such as Composition . This allows us to see properties related to composition: nacl_compo = nacl_structure.composition nacl_compo.reduced_formula nacl_compo.elements This will give outputs of... # EXPECTED OUTPUTS Comp: Na1 Cl1 'NaCl' [Element Na, Element Cl] # <-- these are Element objects! Keeping track of our variables \u00b6 As you create new python objects and name them different things, you'll need help keep track of them. Fortunately, Spyder's variable explorer (a tab in top right window) let's us track them! Try double-clicking some of your variables and explore what Spyder can do:","title":"Structure properties"},{"location":"getting_started/analyze_and_modify_structures/structure_properties/#properties-of-a-structure-object","text":"","title":"Properties of a Structure object"},{"location":"getting_started/analyze_and_modify_structures/structure_properties/#classes-give-us-properties","text":"We make classes because they allow us to automate common calculations. For example, all structures have a density , which is easily calculated once you know the lattice and atomic sites. The formula to calculate is always the same, so we can automate the calculation. You can access this and other properties through our structure object. Try typing nacl_structure.density in the python terminal and hit enter. It should tell you the density of our structure: nacl_structure . density","title":"Classes give us properties"},{"location":"getting_started/analyze_and_modify_structures/structure_properties/#lattice-properties","text":"Now what about other properties for the lattice, like volume, angles, and vectors? For the sake of organization, the Structure class has an associated class called Lattice , and within the lattice object we find properties like volume . Try out these in your python terminal (only run one line at a time): nacl_structure . lattice . volume nacl_structure . lattice . matrix nacl_structure . lattice . beta Your outputs will be... # EXPECTED OUTPUTS 46.09614820053437 array([[3.48543651, 0. , 2.01231771], [1.16181217, 3.28610106, 2.01231771], [0. , 0. , 4.02463542]]) 59.99999999999999 If you don't like to type long lines, there is a shortcut. We save the Lattice object (here, we call it as nacl_lat , but you can pick a different name) to a new variable name and then call its properties: nacl_lat = nacl_structure . lattice nacl_lat . volume nacl_lat . matrix nacl_lat . beta Note, outputs will be the same as above.","title":"Lattice properties"},{"location":"getting_started/analyze_and_modify_structures/structure_properties/#composition-properties","text":"We can apply the same idea to other Structure sub-classes, such as Composition . This allows us to see properties related to composition: nacl_compo = nacl_structure.composition nacl_compo.reduced_formula nacl_compo.elements This will give outputs of... # EXPECTED OUTPUTS Comp: Na1 Cl1 'NaCl' [Element Na, Element Cl] # <-- these are Element objects!","title":"Composition properties"},{"location":"getting_started/analyze_and_modify_structures/structure_properties/#keeping-track-of-our-variables","text":"As you create new python objects and name them different things, you'll need help keep track of them. Fortunately, Spyder's variable explorer (a tab in top right window) let's us track them! Try double-clicking some of your variables and explore what Spyder can do:","title":"Keeping track of our variables"},{"location":"getting_started/analyze_and_modify_structures/the_structure_class/","text":"The Structure class \u00b6 What is a class? \u00b6 Python \"classes\" and \"objects\" are central concepts in python that are important to understand. In real life, we might say that McDonald's, Burger King, and Wendy's are examples of restauraunts. In python, we could say that mcdonalds , burgerking , and wendys are objects of the class Restaurants . By organizing objects into classes , python simplifies the way we program. For example, we could design the Restaurants class to have a property called menu . Then, we could view the menu simply by typing wendys.menu . We (essentially) set a rule that -- no matter what restaurant we have -- the menu info can be accessed with example_restaurant.menu . This might seem silly, but it becomes very powerful once we want to start building out functionality and anaylses. Loading the Structure class \u00b6 In materials science, the class we use most is for crystal structures. In Simmate, we call this class Structure . A crystal structure is always made up of a lattice and a list of atomic sites. Fortunately, this is exactly what we have in our POSCAR file from tutorial 2, so let's use Simmate to create an object of the Structure . Start by entering this line into the python console and hit enter: from simmate.toolkit import Structure This line says we want to use the Structure class from Simmate's code. The Structure class is now loaded into memory and is waiting for you to do something with it. Loading our structure from a file \u00b6 Next, make sure you have the correct working directory (just like we did with the command-line). Spyder lists this in the top right -- and you can hit the folder icon to change it. We want to be in the same folder as our POSCAR file. Run this line in your python terminal: nacl_structure = Structure . from_file ( \"POSCAR\" ) Here, we told python that we have a Structure and the information for it is located in the POSCAR file. This could be in many other formats, such as a CIF file. But we now have a Structure object and it is named nacl_structure . To make sure it loaded correctly, run this line: nacl_structure It should print out... Structure Summary Lattice abc : 4.02463542 4.02463542 4.02463542 angles : 59.99999999999999 59.99999999999999 59.99999999999999 volume : 46.09614820053437 A : 3.4854365146906536 0.0 2.0123177100000005 B : 1.1618121715635514 3.2861010599106226 2.0123177100000005 C : 0.0 0.0 4.02463542 PeriodicSite: Na (0.0000, 0.0000, 0.0000) [0.0000, 0.0000, 0.0000] PeriodicSite: Cl (2.3236, 1.6431, 4.0246) [0.5000, 0.5000, 0.5000] This is the same information from our POSCAR! Manually create a structure in python \u00b6 Alternatively, we could have created this structure manually: # note we can name the object whatever we want. We use 's' here. s = Structure ( lattice = [ [ 3.48543651 , 0.0 , 2.01231771 ], [ 1.16181217 , 3.28610106 , 2.01231771 ], [ 0.0 , 0.0 , 4.02463542 ], ], species = [ \"Na\" , \"Cl\" ], coords = [ [ 0.0000 , 0.0000 , 0.0000 ], [ 0.5000 , 0.5000 , 0.5000 ], ], ) Use the structure in a workflow \u00b6 Whatever your method for creating a structure, we now have our Structure object and we can use its properties (and methods) to simplify our calculations. For example, we can use it to run a workflow. We did this with the command-line in the last tutorial but can accomplish the same thing with python: python # Using Python (in Spyder) from simmate.toolkit import Structure from simmate.workflows.utilities import get_workflow workflow = get_workflow ( \"static-energy.vasp.mit\" ) nacl_structure = Structure . from_file ( \"POSCAR\" ) # ... modify your structure in some way ... # (we will learn how in the next section) result = workflow . run ( structure = nacl_structure ) If we don't need to modify the structure, we could have just given the filename to our workflow: python yaml from simmate.workflows.utilities import get_workflow workflow = get_workflow ( \"static-energy.vasp.mit\" ) result = workflow . run ( structure = \"POSCAR\" ) workflow_name : static-energy.vasp.mit structure : POSCAR","title":"The structure class"},{"location":"getting_started/analyze_and_modify_structures/the_structure_class/#the-structure-class","text":"","title":"The Structure class"},{"location":"getting_started/analyze_and_modify_structures/the_structure_class/#what-is-a-class","text":"Python \"classes\" and \"objects\" are central concepts in python that are important to understand. In real life, we might say that McDonald's, Burger King, and Wendy's are examples of restauraunts. In python, we could say that mcdonalds , burgerking , and wendys are objects of the class Restaurants . By organizing objects into classes , python simplifies the way we program. For example, we could design the Restaurants class to have a property called menu . Then, we could view the menu simply by typing wendys.menu . We (essentially) set a rule that -- no matter what restaurant we have -- the menu info can be accessed with example_restaurant.menu . This might seem silly, but it becomes very powerful once we want to start building out functionality and anaylses.","title":"What is a class?"},{"location":"getting_started/analyze_and_modify_structures/the_structure_class/#loading-the-structure-class","text":"In materials science, the class we use most is for crystal structures. In Simmate, we call this class Structure . A crystal structure is always made up of a lattice and a list of atomic sites. Fortunately, this is exactly what we have in our POSCAR file from tutorial 2, so let's use Simmate to create an object of the Structure . Start by entering this line into the python console and hit enter: from simmate.toolkit import Structure This line says we want to use the Structure class from Simmate's code. The Structure class is now loaded into memory and is waiting for you to do something with it.","title":"Loading the Structure class"},{"location":"getting_started/analyze_and_modify_structures/the_structure_class/#loading-our-structure-from-a-file","text":"Next, make sure you have the correct working directory (just like we did with the command-line). Spyder lists this in the top right -- and you can hit the folder icon to change it. We want to be in the same folder as our POSCAR file. Run this line in your python terminal: nacl_structure = Structure . from_file ( \"POSCAR\" ) Here, we told python that we have a Structure and the information for it is located in the POSCAR file. This could be in many other formats, such as a CIF file. But we now have a Structure object and it is named nacl_structure . To make sure it loaded correctly, run this line: nacl_structure It should print out... Structure Summary Lattice abc : 4.02463542 4.02463542 4.02463542 angles : 59.99999999999999 59.99999999999999 59.99999999999999 volume : 46.09614820053437 A : 3.4854365146906536 0.0 2.0123177100000005 B : 1.1618121715635514 3.2861010599106226 2.0123177100000005 C : 0.0 0.0 4.02463542 PeriodicSite: Na (0.0000, 0.0000, 0.0000) [0.0000, 0.0000, 0.0000] PeriodicSite: Cl (2.3236, 1.6431, 4.0246) [0.5000, 0.5000, 0.5000] This is the same information from our POSCAR!","title":"Loading our structure from a file"},{"location":"getting_started/analyze_and_modify_structures/the_structure_class/#manually-create-a-structure-in-python","text":"Alternatively, we could have created this structure manually: # note we can name the object whatever we want. We use 's' here. s = Structure ( lattice = [ [ 3.48543651 , 0.0 , 2.01231771 ], [ 1.16181217 , 3.28610106 , 2.01231771 ], [ 0.0 , 0.0 , 4.02463542 ], ], species = [ \"Na\" , \"Cl\" ], coords = [ [ 0.0000 , 0.0000 , 0.0000 ], [ 0.5000 , 0.5000 , 0.5000 ], ], )","title":"Manually create a structure in python"},{"location":"getting_started/analyze_and_modify_structures/the_structure_class/#use-the-structure-in-a-workflow","text":"Whatever your method for creating a structure, we now have our Structure object and we can use its properties (and methods) to simplify our calculations. For example, we can use it to run a workflow. We did this with the command-line in the last tutorial but can accomplish the same thing with python: python # Using Python (in Spyder) from simmate.toolkit import Structure from simmate.workflows.utilities import get_workflow workflow = get_workflow ( \"static-energy.vasp.mit\" ) nacl_structure = Structure . from_file ( \"POSCAR\" ) # ... modify your structure in some way ... # (we will learn how in the next section) result = workflow . run ( structure = nacl_structure ) If we don't need to modify the structure, we could have just given the filename to our workflow: python yaml from simmate.workflows.utilities import get_workflow workflow = get_workflow ( \"static-energy.vasp.mit\" ) result = workflow . run ( structure = \"POSCAR\" ) workflow_name : static-energy.vasp.mit structure : POSCAR","title":"Use the structure in a workflow"},{"location":"getting_started/custom_tables_and_apps/create_a_custom_app/","text":"Creating new project files \u00b6 To create a new project, navigate to a folder where you'd like to store your code and run... simmate start-project Doublecheck that you see a new folder named my_new_project . Open it up and you should see a series of new files: my_new_project/ \u251c\u2500\u2500 pyproject.toml \u251c\u2500\u2500 README.md \u2514\u2500\u2500 example_app \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 apps.py \u251c\u2500\u2500 models.py \u251c\u2500\u2500 tests.py \u251c\u2500\u2500 urls.py \u251c\u2500\u2500 views.py \u2514\u2500\u2500 workflows.py Make note that we have a folder named example_app . This is where our code will go. You can have as many app folders as you'd like (and in extreme scenarios even have apps within other apps). Name our project and app \u00b6 Danger Once you choose a name, stick with it. Changing your project name or app name after it has been installed can lead your code failing with ModuleNotFound errors. Name the project Rename your folder from \"my_new_project\" to a name of your choosing. Try to follow python conventions and keep your project name all lowercase and connected with underscores. For example, warren_lab or scotts_project are good project names. Open the file new_project_project/pyproject.toml and change the name here as well. [project] name = \"my_simmate_project\" # <--- update with your new name Name the app Decide on how your code should be imported. For example, you may want your workflows to be loaded like so: from example_app.workflows import Example__Workflow__Settings Take the first part of this ( example_app ) and rename the example_app folder to match this. The python conventions (described above) also apply here. For example, simmate_abinit or simmate_clease are informative and good project names. These let the user know what's in your app and they're easy to remember. Here's how they would work: from simmate_clease.workflows import ClusterExpansion__Clease__BasicSettings Open the file example_app/apps.py and rename the class AND name property to match your app name. from django.apps import AppConfig class SimmateCleaseConfig ( AppConfig ): # don't forget the class name name = \"simmate_clease\" Note This file might seem silly, but it allows users to build complex apps that include many other apps / subapps. Beginners will likely never look at this file again. Install our app \u00b6 Open the pyproject.toml file. This file is what tells Python how to install your code. It doesn't take much to install a package . As you project gets larger and needs other programs installed, you'll make note of them here. For now, nothing needs to be changed. While inside your new project folder, we want to \"install\" the project to your conda envirnment in \"--editable\" (-e) mode. This means you'll be make changes to your code and Python should automatically use your changes. # replace \"my_new_project\" with the name of your project cd my_new_project pip install -e . Make sure this install worked by running these lines in python. You may need to restart your terminal/Spyder for this to work. example 1 example 2 import example_app from example_app.apps import ExampleAppConfig import simmate_clease from simmate_clease.apps import SimmateCleaseConfig You now have an installed app! But one issue remains: Simmate still doesn't know it exists yet. We need to tell Simmate to load it. Register your app with Simmate \u00b6 Go to your simmate configuration folder. Recall from earlier tutorials that this where your database is stored, and it is located at... # in your home directory cd ~/simmate/ Look for the file ~/simmate/my_env-apps.yaml , which is named after your conda environment. Open it up and you'll see we have apps already installed with Simmate: - simmate.workflows.base_flow_types.apps.BaseWorkflowsConfig - simmate.calculators.vasp.apps.VaspConfig - simmate.calculators.bader.apps.BaderConfig - simmate.toolkit.structure_prediction.evolution.apps.EvolutionarySearchConfig In this file, add the following line at the bottom - example_app.apps.ExampleAppConfig Make sure Simmate can find and load your app in python from simmate.configuration.django.settings import SIMMATE_APPS print ( SIMMATE_APPS ) # you should see your new app! Make sure Simmate can configure your new app and it's tables properly from simmate.database import connect Tip If you ever want to stop using this app, you can delete it from your ~/simmate/my_env-apps.yaml file.","title":"Creating a custom project"},{"location":"getting_started/custom_tables_and_apps/create_a_custom_app/#creating-new-project-files","text":"To create a new project, navigate to a folder where you'd like to store your code and run... simmate start-project Doublecheck that you see a new folder named my_new_project . Open it up and you should see a series of new files: my_new_project/ \u251c\u2500\u2500 pyproject.toml \u251c\u2500\u2500 README.md \u2514\u2500\u2500 example_app \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 apps.py \u251c\u2500\u2500 models.py \u251c\u2500\u2500 tests.py \u251c\u2500\u2500 urls.py \u251c\u2500\u2500 views.py \u2514\u2500\u2500 workflows.py Make note that we have a folder named example_app . This is where our code will go. You can have as many app folders as you'd like (and in extreme scenarios even have apps within other apps).","title":"Creating new project files"},{"location":"getting_started/custom_tables_and_apps/create_a_custom_app/#name-our-project-and-app","text":"Danger Once you choose a name, stick with it. Changing your project name or app name after it has been installed can lead your code failing with ModuleNotFound errors. Name the project Rename your folder from \"my_new_project\" to a name of your choosing. Try to follow python conventions and keep your project name all lowercase and connected with underscores. For example, warren_lab or scotts_project are good project names. Open the file new_project_project/pyproject.toml and change the name here as well. [project] name = \"my_simmate_project\" # <--- update with your new name Name the app Decide on how your code should be imported. For example, you may want your workflows to be loaded like so: from example_app.workflows import Example__Workflow__Settings Take the first part of this ( example_app ) and rename the example_app folder to match this. The python conventions (described above) also apply here. For example, simmate_abinit or simmate_clease are informative and good project names. These let the user know what's in your app and they're easy to remember. Here's how they would work: from simmate_clease.workflows import ClusterExpansion__Clease__BasicSettings Open the file example_app/apps.py and rename the class AND name property to match your app name. from django.apps import AppConfig class SimmateCleaseConfig ( AppConfig ): # don't forget the class name name = \"simmate_clease\" Note This file might seem silly, but it allows users to build complex apps that include many other apps / subapps. Beginners will likely never look at this file again.","title":"Name our project and app"},{"location":"getting_started/custom_tables_and_apps/create_a_custom_app/#install-our-app","text":"Open the pyproject.toml file. This file is what tells Python how to install your code. It doesn't take much to install a package . As you project gets larger and needs other programs installed, you'll make note of them here. For now, nothing needs to be changed. While inside your new project folder, we want to \"install\" the project to your conda envirnment in \"--editable\" (-e) mode. This means you'll be make changes to your code and Python should automatically use your changes. # replace \"my_new_project\" with the name of your project cd my_new_project pip install -e . Make sure this install worked by running these lines in python. You may need to restart your terminal/Spyder for this to work. example 1 example 2 import example_app from example_app.apps import ExampleAppConfig import simmate_clease from simmate_clease.apps import SimmateCleaseConfig You now have an installed app! But one issue remains: Simmate still doesn't know it exists yet. We need to tell Simmate to load it.","title":"Install our app"},{"location":"getting_started/custom_tables_and_apps/create_a_custom_app/#register-your-app-with-simmate","text":"Go to your simmate configuration folder. Recall from earlier tutorials that this where your database is stored, and it is located at... # in your home directory cd ~/simmate/ Look for the file ~/simmate/my_env-apps.yaml , which is named after your conda environment. Open it up and you'll see we have apps already installed with Simmate: - simmate.workflows.base_flow_types.apps.BaseWorkflowsConfig - simmate.calculators.vasp.apps.VaspConfig - simmate.calculators.bader.apps.BaderConfig - simmate.toolkit.structure_prediction.evolution.apps.EvolutionarySearchConfig In this file, add the following line at the bottom - example_app.apps.ExampleAppConfig Make sure Simmate can find and load your app in python from simmate.configuration.django.settings import SIMMATE_APPS print ( SIMMATE_APPS ) # you should see your new app! Make sure Simmate can configure your new app and it's tables properly from simmate.database import connect Tip If you ever want to stop using this app, you can delete it from your ~/simmate/my_env-apps.yaml file.","title":"Register your app with Simmate"},{"location":"getting_started/custom_tables_and_apps/create_a_custom_table/","text":"Creating custom database tables \u00b6 Build custom tables \u00b6 Inside your project, there are example database tables that show how to build simple ones. It may seem super minimal, but there's really nothing else to do! Recall the lesson from the section on inheritance from the \"access the database\" tutorial. This is why building out new tables is so easy. Some thing as simple as... # Don't run this code. Just read for understanding. from simmate.database.base_data_types import ( table_column , Structure , Calculation , # useful for tables used in workflows ) class MyCustomTable1 ( Structure , Calculation ): pass # nothing else is required unless you want to add custom columns/features ... will build out a new database table with all the following columns - created_at - updated_at - source - structure - nsites - nelements - elements - chemical_system - density - density_atomic - volume - volume_molar - formula_full - formula_reduced - formula_anonymous - spacegroup (points to an entry in the Spacegroup table) However, we won't be able to import these tables or load data just yet. We will cover this next. Tip Here we just show Structure and Calculation data, but there are many more base_data_types that you can use. All types build out features for you automatically. Be sure to read through our guides in the simmate.database module for more info. Advanced database tables may require reading more on the base data types too. Add tables to your database \u00b6 Open up the example_app/models.py file and read through the example tables. We can add these table to our database by Simmate updating our database: simmate database update Check the output of your update command. You should see that your new app was detected and that your tables were added. Migrations for 'example_app' : example_app/migrations/0001_initial.py - Create model MyCustomTable2 - Create model MyCustomTable1 Make sure you can view the new tables in your database. Just remember that we need to connect to our database first. Note we are starting with MyCustomTable2 here: from simmate.database import connect from example_app.models import MyCustomTable2 MyCustomTable2 . objects . count () # should output 0 bc we haven't added data yet Danger Whenever you change the models.py file, be sure to either (1) reset your database or (2) run simmate database update in order for your changes to be applied to your database. Info In Django (which Simmate uses under the hood), a DatabaseTable is known as a Model . So a model and table can be viewed as the same thing. Because we are using Django, the file name models.py must stay this way. That's where Django looks for your custom database tables. Tip simmate database reset will also load your changes to the database. Just make sure you say no to the prebuilt database. Add data to your new table \u00b6 You can automatically fill this table using the from_toolkit method too: from simmate.database import connect from simmate.toolkit import Structure from example_app.models import MyCustomTable1 nacl = Structure . from_file ( \"NaCl.cif\" ) new_entry = MyCustomTable1 . from_toolkit ( structure = nacl ) new_entry . save () Tip Manually filling your table with data is often not necessary. You can instead attach your database to a workflow, which will fill it with data automatically. We will learn how to do this in a later step. Search your new data \u00b6 You can filter off results just like you would any other table. Be sure to go back through the earlier database tutorial if you need a refresher. df = MyCustomTable1 . objects . filter ( nsites__lte = 10 ) . to_dataframe ()","title":"Creating a custom table"},{"location":"getting_started/custom_tables_and_apps/create_a_custom_table/#creating-custom-database-tables","text":"","title":"Creating custom database tables"},{"location":"getting_started/custom_tables_and_apps/create_a_custom_table/#build-custom-tables","text":"Inside your project, there are example database tables that show how to build simple ones. It may seem super minimal, but there's really nothing else to do! Recall the lesson from the section on inheritance from the \"access the database\" tutorial. This is why building out new tables is so easy. Some thing as simple as... # Don't run this code. Just read for understanding. from simmate.database.base_data_types import ( table_column , Structure , Calculation , # useful for tables used in workflows ) class MyCustomTable1 ( Structure , Calculation ): pass # nothing else is required unless you want to add custom columns/features ... will build out a new database table with all the following columns - created_at - updated_at - source - structure - nsites - nelements - elements - chemical_system - density - density_atomic - volume - volume_molar - formula_full - formula_reduced - formula_anonymous - spacegroup (points to an entry in the Spacegroup table) However, we won't be able to import these tables or load data just yet. We will cover this next. Tip Here we just show Structure and Calculation data, but there are many more base_data_types that you can use. All types build out features for you automatically. Be sure to read through our guides in the simmate.database module for more info. Advanced database tables may require reading more on the base data types too.","title":"Build custom tables"},{"location":"getting_started/custom_tables_and_apps/create_a_custom_table/#add-tables-to-your-database","text":"Open up the example_app/models.py file and read through the example tables. We can add these table to our database by Simmate updating our database: simmate database update Check the output of your update command. You should see that your new app was detected and that your tables were added. Migrations for 'example_app' : example_app/migrations/0001_initial.py - Create model MyCustomTable2 - Create model MyCustomTable1 Make sure you can view the new tables in your database. Just remember that we need to connect to our database first. Note we are starting with MyCustomTable2 here: from simmate.database import connect from example_app.models import MyCustomTable2 MyCustomTable2 . objects . count () # should output 0 bc we haven't added data yet Danger Whenever you change the models.py file, be sure to either (1) reset your database or (2) run simmate database update in order for your changes to be applied to your database. Info In Django (which Simmate uses under the hood), a DatabaseTable is known as a Model . So a model and table can be viewed as the same thing. Because we are using Django, the file name models.py must stay this way. That's where Django looks for your custom database tables. Tip simmate database reset will also load your changes to the database. Just make sure you say no to the prebuilt database.","title":"Add tables to your database"},{"location":"getting_started/custom_tables_and_apps/create_a_custom_table/#add-data-to-your-new-table","text":"You can automatically fill this table using the from_toolkit method too: from simmate.database import connect from simmate.toolkit import Structure from example_app.models import MyCustomTable1 nacl = Structure . from_file ( \"NaCl.cif\" ) new_entry = MyCustomTable1 . from_toolkit ( structure = nacl ) new_entry . save () Tip Manually filling your table with data is often not necessary. You can instead attach your database to a workflow, which will fill it with data automatically. We will learn how to do this in a later step.","title":"Add data to your new table"},{"location":"getting_started/custom_tables_and_apps/create_a_custom_table/#search-your-new-data","text":"You can filter off results just like you would any other table. Be sure to go back through the earlier database tutorial if you need a refresher. df = MyCustomTable1 . objects . filter ( nsites__lte = 10 ) . to_dataframe ()","title":"Search your new data"},{"location":"getting_started/custom_tables_and_apps/quick_start/","text":"Build custom database tables and apps \u00b6 In this tutorial, you will learn how to build customized database table and also learn how to build out your customized features into a \"project\" and \"apps\". Note There is no \"quick tutorial\" for this topic. Even advanced users should read everything. Should you create a Simmate project? \u00b6 A custom Simmate project is required if you want to build a new database table or access your workflows in the website interface. There are many more reasons you'd want to create a project. This includes wanting to... use a custom database table to save our workflow results access the workflow in the website interface access our workflow from other scripts (and the get_workflow function) begin organizing our code into smaller files and easily import them share workflows among a team let others install your workflows after you publish a new paper All of these can be done using Simmate \"projects\". These projects are really just a folder of python files in a specific format (i.e. we have rules for naming files and what to put in them). Is this just creating a new package? \u00b6 Yep -- Projects are indeed building a new python package. In fact, our start-project command acts just like a \"cookie-cutter\" template. This can have huge implications for sharing research and code. With a fully-functional and published Simmate project, you can upload your code for other labs to use via Github and PyPi. Then the entire Simmate community can install and use your custom workflows with Simmate. For them, it'd be as easy as doing pip install my_new_project adding example_app.apps.ExampleAppConfig to their ~/simmate/my_env-apps.yaml Alternatively, you can request that your app be merged into our Simmate repository, so that it is installed by default for all users. Whichever route you choose, your hard work should be much more accessible to the community and new users! Note In the future, we hope to have a page that lists off apps available for download, but because Simmate is brand new, there currently aren't any existing apps outside of Simmate itself. Reach out to our team if you're interested in kickstarting a downloads page!","title":"Quickstart"},{"location":"getting_started/custom_tables_and_apps/quick_start/#build-custom-database-tables-and-apps","text":"In this tutorial, you will learn how to build customized database table and also learn how to build out your customized features into a \"project\" and \"apps\". Note There is no \"quick tutorial\" for this topic. Even advanced users should read everything.","title":"Build custom database tables and apps"},{"location":"getting_started/custom_tables_and_apps/quick_start/#should-you-create-a-simmate-project","text":"A custom Simmate project is required if you want to build a new database table or access your workflows in the website interface. There are many more reasons you'd want to create a project. This includes wanting to... use a custom database table to save our workflow results access the workflow in the website interface access our workflow from other scripts (and the get_workflow function) begin organizing our code into smaller files and easily import them share workflows among a team let others install your workflows after you publish a new paper All of these can be done using Simmate \"projects\". These projects are really just a folder of python files in a specific format (i.e. we have rules for naming files and what to put in them).","title":"Should you create a Simmate project?"},{"location":"getting_started/custom_tables_and_apps/quick_start/#is-this-just-creating-a-new-package","text":"Yep -- Projects are indeed building a new python package. In fact, our start-project command acts just like a \"cookie-cutter\" template. This can have huge implications for sharing research and code. With a fully-functional and published Simmate project, you can upload your code for other labs to use via Github and PyPi. Then the entire Simmate community can install and use your custom workflows with Simmate. For them, it'd be as easy as doing pip install my_new_project adding example_app.apps.ExampleAppConfig to their ~/simmate/my_env-apps.yaml Alternatively, you can request that your app be merged into our Simmate repository, so that it is installed by default for all users. Whichever route you choose, your hard work should be much more accessible to the community and new users! Note In the future, we hope to have a page that lists off apps available for download, but because Simmate is brand new, there currently aren't any existing apps outside of Simmate itself. Reach out to our team if you're interested in kickstarting a downloads page!","title":"Is this just creating a new package?"},{"location":"getting_started/custom_tables_and_apps/using_app_workflows/","text":"Using App Workflows \u00b6 Organizing app workflows \u00b6 By default, Simmate searches for a workflows.py (or workflows module) within your app and grabs all python classes within it -- assuming they are workflows you want registered. If you scripts contain non-workflow classes or even abstract base workflows, this can cause unexpected errors. You must therefore explicitly specify which workflows should be registered with your app. Note If you are seeing AttributeError: '...' object has no attribute 'name_full' or Exception: Make sure you are following Simmate naming conventions , then you likely have misorganized your workflows. in workflows.py file \u00b6 When you first create your project/app, you'll notice a single workflows.py file that has the following near the top: # ----------------------------------------------------------------------------- # Make sure to list out all workflows that you want registered # ----------------------------------------------------------------------------- __all__ = [ \"Example__Python__MyExample1\" , \"Relaxation__Vasp__MyExample2\" , ] You must explicitly list out all workflows that you want registered if you stay with a single workflow.py file format. in a workflows module \u00b6 As your app grows, you may want to store your workflows in separate scripts or submodules. You can do this by deleting the workflows.py and replacing it with a folder named workflows that has the following organization: # rest of example_app is organized the same as before example_app \u2514\u2500\u2500 workflows \u251c\u2500\u2500 __init__.py # <-- file used for registration \u251c\u2500\u2500 example_1.py \u251c\u2500\u2500 example_2.py \u251c\u2500\u2500 example_3.py \u2514\u2500\u2500 example_submodule \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 example_4.py \u2514\u2500\u2500 example_5.py Here, example_N.py files can be named whatever you'd like and do NOT require the __all__ tag to be set. This folder structure allows us to organize all of our workflows nicely. Instead of the __all__ tag, we can list out workflows to register in the workflows/__init__.py . We can use relative python imports to accomplish this with minimal code: # in workflows/__init__.py from .example_1 import Example__Python__MyExample1 from .example_2 import Example__Python__MyExample2 from .example_3 import Example__Python__MyExample3 from .example_submodule.example_4 import Example__Python__MyExample4 from .example_submodule.example_5 import Example__Python__MyExample5 Example If you prefer to learn by example, you can take a look at Simmate's built-in Vasp app. The workflows module of this app is located here . You can also see how our workflows/__init__.py file lists out all of our workflows that are registered ( here )). Basic use \u00b6 On the surface, our workflows here behave exactly the same as they did before. We can run them with a yaml file or directly in python. workflow_name : example_app/workflows:Example__Python__MyExample1 structure : NaCl.cif input_01 : 12345 input_02 : true However, now that they are in a Simmate Project and we registered the App, we can access some extra features. First, we can just use the workflow name and also access our workflow with the command line and get_workflow utilities: yaml python workflow_name : example.python.my-example1 structure : NaCl.cif input_01 : 12345 input_02 : true from simmate.workflows.utilities import get_workflow workflow = get_workflow ( \"example.python.my-example1\" ) workflow . run ( structure = \"NaCl.cif\" , input_01 = 12345 , input_02 = true , ) Check and see your workflow listed now too: command line simmate workflows list-all Danger Make sure you set the __all__ attribute in your workflows.py file. Otherwise, workflows will not be found or other errors may occur. This should just be a list of the workflows you want registered. Link Datatables to Workflows \u00b6 To have a workflow use a custom database table, the following requirements must be met: the table must use the Calculation mix-in the workflow must have database_table set to our table the table and workflow must be registered (already completed) Note that one our database tables and workflows already meet these conditions. For MyCustomTable1 , we can see it is using the Calculation mix-in in our models.py file: class MyCustomTable1 ( Structure , Calculation ): # ... custom columns hidden ... This table has already been linked to a workflow too. In our workflows.py file, we can see the following: class Example__Python__MyExample1 ( Workflow ): database_table = MyCustomTable1 This is completes our check-list -- so this database and workflow are already configured for us. Storing inputs parameters \u00b6 To have a workflow store input parameters at the start of a calculation, the following requirements must be met: the parameter must have been added as a column to the database a parameter of the same exact name must be an input option of run_config For our MyCustomTable1 and Example__Python__MyExample1 , we can see that the following inputs are matching for both the run_config input AND are table columns: input_01 input_02 structure Look through the relevant code that sets this up: MyCustomTable1 Example__Python__MyExample1 # structure --> through the Structure mix-in input_01 = table_column . FloatField ( null = True , blank = True ) input_02 = table_column . BooleanField ( null = True , blank = True ) def run_config ( input_01 , input_02 , structure , And that's it! You workflow will store these inputs to your database when a workflow run starts. Storing outputs and results \u00b6 To have a workflow store outputs at the end of a calculation, the following requirements must be met: the parameter must have been added as a column to the database the run_config must return a dictionary of columns that need to be updated a key of the same exact name must be in this dictionary For our MyCustomTable1 and Example__Python__MyExample1 , we can see that the following outputs are matching for both the run_config 's output dictionary AND are table columns: output_01 output_02 Look through the relevant code that sets this up: MyCustomTable1 Example__Python__MyExample1 output_01 = table_column . FloatField ( null = True , blank = True ) output_02 = table_column . BooleanField ( null = True , blank = True ) return { \"output_01\" : ... , \"output_02\" : ... , } And that's it! You workflow will store these results to your database when a workflow run completes. Tip Complex storing of results -- such as from toolkit objects or from files -- is possible too. These are covered in the full guides. Viewing Results \u00b6 Results are stored the same as any other workflow. You'll see a summary file written for you, and you can load all the data from you database. We only configure a small number of columns for our workflow + datatable, but check out all of the outputs! _DATABASE_TABLE_ : MyCustomTable1 _TABLE_ID_ : 1 _WEBSITE_URL_ : http://127.0.0.1:8000/workflows/example/python/my-example1/1 chemical_system : Cl-Na computer_system : digital-storm created_at : 2022-09-04 00:10:01.844798+00:00 density : 2.1053060843576104 density_atomic : 0.04338757298280908 directory : /home/jacksund/Documents/spyder_wd/simmate-task-ow6otw06 formula_anonymous : AB formula_full : Na4 Cl4 formula_reduced : NaCl id : 1 input_01 : 12345.0 input_02 : true nelements : 2 nsites : 8 output_01 : 1234500 output_02 : false run_id : 6872771c-c0d7-43b5-afea-b8ed87f6a5df spacegroup_id : 225 updated_at : 2022-09-04 00:10:01.875757+00:00 volume : 184.38459332974767 volume_molar : 13.87987468758872 workflow_name : example.python.my-example1 workflow_version : 0.10.0 Danger The _WEBSITE_URL_ is experimental and only works for common workflow types at the moment. In the future, you'll be able to explore your results in an automatically-built web interface.","title":"Using app workflows"},{"location":"getting_started/custom_tables_and_apps/using_app_workflows/#using-app-workflows","text":"","title":"Using App Workflows"},{"location":"getting_started/custom_tables_and_apps/using_app_workflows/#organizing-app-workflows","text":"By default, Simmate searches for a workflows.py (or workflows module) within your app and grabs all python classes within it -- assuming they are workflows you want registered. If you scripts contain non-workflow classes or even abstract base workflows, this can cause unexpected errors. You must therefore explicitly specify which workflows should be registered with your app. Note If you are seeing AttributeError: '...' object has no attribute 'name_full' or Exception: Make sure you are following Simmate naming conventions , then you likely have misorganized your workflows.","title":"Organizing app workflows"},{"location":"getting_started/custom_tables_and_apps/using_app_workflows/#in-workflowspy-file","text":"When you first create your project/app, you'll notice a single workflows.py file that has the following near the top: # ----------------------------------------------------------------------------- # Make sure to list out all workflows that you want registered # ----------------------------------------------------------------------------- __all__ = [ \"Example__Python__MyExample1\" , \"Relaxation__Vasp__MyExample2\" , ] You must explicitly list out all workflows that you want registered if you stay with a single workflow.py file format.","title":"in workflows.py file"},{"location":"getting_started/custom_tables_and_apps/using_app_workflows/#in-a-workflows-module","text":"As your app grows, you may want to store your workflows in separate scripts or submodules. You can do this by deleting the workflows.py and replacing it with a folder named workflows that has the following organization: # rest of example_app is organized the same as before example_app \u2514\u2500\u2500 workflows \u251c\u2500\u2500 __init__.py # <-- file used for registration \u251c\u2500\u2500 example_1.py \u251c\u2500\u2500 example_2.py \u251c\u2500\u2500 example_3.py \u2514\u2500\u2500 example_submodule \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 example_4.py \u2514\u2500\u2500 example_5.py Here, example_N.py files can be named whatever you'd like and do NOT require the __all__ tag to be set. This folder structure allows us to organize all of our workflows nicely. Instead of the __all__ tag, we can list out workflows to register in the workflows/__init__.py . We can use relative python imports to accomplish this with minimal code: # in workflows/__init__.py from .example_1 import Example__Python__MyExample1 from .example_2 import Example__Python__MyExample2 from .example_3 import Example__Python__MyExample3 from .example_submodule.example_4 import Example__Python__MyExample4 from .example_submodule.example_5 import Example__Python__MyExample5 Example If you prefer to learn by example, you can take a look at Simmate's built-in Vasp app. The workflows module of this app is located here . You can also see how our workflows/__init__.py file lists out all of our workflows that are registered ( here )).","title":"in a workflows module"},{"location":"getting_started/custom_tables_and_apps/using_app_workflows/#basic-use","text":"On the surface, our workflows here behave exactly the same as they did before. We can run them with a yaml file or directly in python. workflow_name : example_app/workflows:Example__Python__MyExample1 structure : NaCl.cif input_01 : 12345 input_02 : true However, now that they are in a Simmate Project and we registered the App, we can access some extra features. First, we can just use the workflow name and also access our workflow with the command line and get_workflow utilities: yaml python workflow_name : example.python.my-example1 structure : NaCl.cif input_01 : 12345 input_02 : true from simmate.workflows.utilities import get_workflow workflow = get_workflow ( \"example.python.my-example1\" ) workflow . run ( structure = \"NaCl.cif\" , input_01 = 12345 , input_02 = true , ) Check and see your workflow listed now too: command line simmate workflows list-all Danger Make sure you set the __all__ attribute in your workflows.py file. Otherwise, workflows will not be found or other errors may occur. This should just be a list of the workflows you want registered.","title":"Basic use"},{"location":"getting_started/custom_tables_and_apps/using_app_workflows/#link-datatables-to-workflows","text":"To have a workflow use a custom database table, the following requirements must be met: the table must use the Calculation mix-in the workflow must have database_table set to our table the table and workflow must be registered (already completed) Note that one our database tables and workflows already meet these conditions. For MyCustomTable1 , we can see it is using the Calculation mix-in in our models.py file: class MyCustomTable1 ( Structure , Calculation ): # ... custom columns hidden ... This table has already been linked to a workflow too. In our workflows.py file, we can see the following: class Example__Python__MyExample1 ( Workflow ): database_table = MyCustomTable1 This is completes our check-list -- so this database and workflow are already configured for us.","title":"Link Datatables to Workflows"},{"location":"getting_started/custom_tables_and_apps/using_app_workflows/#storing-inputs-parameters","text":"To have a workflow store input parameters at the start of a calculation, the following requirements must be met: the parameter must have been added as a column to the database a parameter of the same exact name must be an input option of run_config For our MyCustomTable1 and Example__Python__MyExample1 , we can see that the following inputs are matching for both the run_config input AND are table columns: input_01 input_02 structure Look through the relevant code that sets this up: MyCustomTable1 Example__Python__MyExample1 # structure --> through the Structure mix-in input_01 = table_column . FloatField ( null = True , blank = True ) input_02 = table_column . BooleanField ( null = True , blank = True ) def run_config ( input_01 , input_02 , structure , And that's it! You workflow will store these inputs to your database when a workflow run starts.","title":"Storing inputs parameters"},{"location":"getting_started/custom_tables_and_apps/using_app_workflows/#storing-outputs-and-results","text":"To have a workflow store outputs at the end of a calculation, the following requirements must be met: the parameter must have been added as a column to the database the run_config must return a dictionary of columns that need to be updated a key of the same exact name must be in this dictionary For our MyCustomTable1 and Example__Python__MyExample1 , we can see that the following outputs are matching for both the run_config 's output dictionary AND are table columns: output_01 output_02 Look through the relevant code that sets this up: MyCustomTable1 Example__Python__MyExample1 output_01 = table_column . FloatField ( null = True , blank = True ) output_02 = table_column . BooleanField ( null = True , blank = True ) return { \"output_01\" : ... , \"output_02\" : ... , } And that's it! You workflow will store these results to your database when a workflow run completes. Tip Complex storing of results -- such as from toolkit objects or from files -- is possible too. These are covered in the full guides.","title":"Storing outputs and results"},{"location":"getting_started/custom_tables_and_apps/using_app_workflows/#viewing-results","text":"Results are stored the same as any other workflow. You'll see a summary file written for you, and you can load all the data from you database. We only configure a small number of columns for our workflow + datatable, but check out all of the outputs! _DATABASE_TABLE_ : MyCustomTable1 _TABLE_ID_ : 1 _WEBSITE_URL_ : http://127.0.0.1:8000/workflows/example/python/my-example1/1 chemical_system : Cl-Na computer_system : digital-storm created_at : 2022-09-04 00:10:01.844798+00:00 density : 2.1053060843576104 density_atomic : 0.04338757298280908 directory : /home/jacksund/Documents/spyder_wd/simmate-task-ow6otw06 formula_anonymous : AB formula_full : Na4 Cl4 formula_reduced : NaCl id : 1 input_01 : 12345.0 input_02 : true nelements : 2 nsites : 8 output_01 : 1234500 output_02 : false run_id : 6872771c-c0d7-43b5-afea-b8ed87f6a5df spacegroup_id : 225 updated_at : 2022-09-04 00:10:01.875757+00:00 volume : 184.38459332974767 volume_molar : 13.87987468758872 workflow_name : example.python.my-example1 workflow_version : 0.10.0 Danger The _WEBSITE_URL_ is experimental and only works for common workflow types at the moment. In the future, you'll be able to explore your results in an automatically-built web interface.","title":"Viewing Results"},{"location":"getting_started/custom_workflows/creating_your_workflow/","text":"Create new & advanced workflows \u00b6 Note This guide only covers the bare-minimum. We highly recommend going through the full guides when building your custom workflows. Create a flow from scratch \u00b6 Simmate defines a base Workflow class to help with common material science analyses. The simplest possible workflow can look something like... from simmate.workflow_engine import Workflow class Example__Python__MyFavoriteSettings ( Workflow ): # Note, the long name of this workflow class is important! use_database = False # we don't have a database table yet @staticmethod def run_config ( ** kwargs ): print ( \"This workflow doesn't do much\" ) return 42 Modify an existing workflow \u00b6 Building a workflow from scratch can be a lot of work. Most often, we don't want to create a new workflow. We just want to take an existing one and update a few settings. In python, we can do that with... from simmate.workflows.utilities import get_workflow original_workflow = get_workflow ( \"static-energy.vasp.matproj\" ) class StaticEnergy__Vasp__MyCustomPreset ( original_workflow ): # NOTE: the name we gave is important! # Don't skip reading the guide above # give a version to help you and you team keep track of what changes version = \"2022.07.04\" incar = original_workflow . incar . copy () # Make sure you copy! incar . update ( dict ( NPAR = 1 , ENCUT =- 1 , ) ) # make sure we have new settings updated and that we didn't change the original assert original_workflow . incar != StaticEnergy__Vasp__MyCustomPreset Danger Updating workflows can often run into unexpected problems -- because not workflows behave the same. More often then not, you should create your own custom VaspWorkflow . Learn more in the full-guides . Running your workflow \u00b6 You can now run and interact with your workflow like any other one! state = StaticEnergy__Vasp__MyCustomPreset . run ( structure = \"NaCl.cif\" ) result = state . result () Tip You can also run workflows from a YAML file too. Check out the full-guides to learn more.","title":"Creating your workflow"},{"location":"getting_started/custom_workflows/creating_your_workflow/#create-new-advanced-workflows","text":"Note This guide only covers the bare-minimum. We highly recommend going through the full guides when building your custom workflows.","title":"Create new &amp; advanced workflows"},{"location":"getting_started/custom_workflows/creating_your_workflow/#create-a-flow-from-scratch","text":"Simmate defines a base Workflow class to help with common material science analyses. The simplest possible workflow can look something like... from simmate.workflow_engine import Workflow class Example__Python__MyFavoriteSettings ( Workflow ): # Note, the long name of this workflow class is important! use_database = False # we don't have a database table yet @staticmethod def run_config ( ** kwargs ): print ( \"This workflow doesn't do much\" ) return 42","title":"Create a flow from scratch"},{"location":"getting_started/custom_workflows/creating_your_workflow/#modify-an-existing-workflow","text":"Building a workflow from scratch can be a lot of work. Most often, we don't want to create a new workflow. We just want to take an existing one and update a few settings. In python, we can do that with... from simmate.workflows.utilities import get_workflow original_workflow = get_workflow ( \"static-energy.vasp.matproj\" ) class StaticEnergy__Vasp__MyCustomPreset ( original_workflow ): # NOTE: the name we gave is important! # Don't skip reading the guide above # give a version to help you and you team keep track of what changes version = \"2022.07.04\" incar = original_workflow . incar . copy () # Make sure you copy! incar . update ( dict ( NPAR = 1 , ENCUT =- 1 , ) ) # make sure we have new settings updated and that we didn't change the original assert original_workflow . incar != StaticEnergy__Vasp__MyCustomPreset Danger Updating workflows can often run into unexpected problems -- because not workflows behave the same. More often then not, you should create your own custom VaspWorkflow . Learn more in the full-guides .","title":"Modify an existing workflow"},{"location":"getting_started/custom_workflows/creating_your_workflow/#running-your-workflow","text":"You can now run and interact with your workflow like any other one! state = StaticEnergy__Vasp__MyCustomPreset . run ( structure = \"NaCl.cif\" ) result = state . result () Tip You can also run workflows from a YAML file too. Check out the full-guides to learn more.","title":"Running your workflow"},{"location":"getting_started/custom_workflows/name_your_new_workflow/","text":"Naming workflows \u00b6 Why are there naming rules? \u00b6 Naming your new workflow is an important step in Simmate. Features (such as the website interface) require that workflow names follow a certain format because this let's us do things such as determine where we can find your new workflow in the website interface. We follow a set of rules to arrive at workflow names like relaxation.vasp.mit . For example, here's how a workflow name looks in different contexts: readable name website url python class name static-energy.vasp.matproj https://simmate.org/workflows/static-energy/vasp/matproj StaticEnergy__Vasp__Matproj Overview of naming conventions \u00b6 Simmate's naming conventions involves 3 parts: The type of analysis the workflow is doing The \"calculator\" (or program) that the workflow uses to run A unique name to identify the settings used Examples for each part would be: relaxation, static-energy, dynamics, ... vasp, abinit, qe, deepmd, ... jacks-test, matproj, quality00, ... Together, an example workflow names would be: relaxation.vasp.jacks-test static-energy.abinit.matproj dynamics.qe.quality00 When converting this to our workflow name in python, we need to replace periods with 2 underscores each and convert our words to pascal case . For example, our workflow names become: Relaxation__Vasp__JacksTest StaticEnergy__Abinit__Matproj Dynamics__Qe__Quality00 Warning Capitalization is very important here so make sure you double check your workflow names. Trying it out in python \u00b6 Now let's test this out in python using a similar workflow name: from simmate.workflow_engine import Workflow class Example__Python__MyFavoriteSettings ( Workflow ): pass # we will build the rest of workflow later # These names can be long and unfriendly, so it can be nice to # link them to a variable name for easier access. my_workflow = Example__Python__MyFavoriteSettings # Now check that our naming convention works as expected assert my_workflow . name_full == \"example.python.my-favorite-settings\" assert my_workflow . name_type == \"example\" assert my_workflow . name_calculator == \"python\" assert my_workflow . name_preset == \"my-favorite-settings\" You now have a ready-to-use workflow name! Tip assert effectively means \"make sure this statement returns True \". It's frequently used by python coders to make sure their code works as expected.","title":"Name your new workflow"},{"location":"getting_started/custom_workflows/name_your_new_workflow/#naming-workflows","text":"","title":"Naming workflows"},{"location":"getting_started/custom_workflows/name_your_new_workflow/#why-are-there-naming-rules","text":"Naming your new workflow is an important step in Simmate. Features (such as the website interface) require that workflow names follow a certain format because this let's us do things such as determine where we can find your new workflow in the website interface. We follow a set of rules to arrive at workflow names like relaxation.vasp.mit . For example, here's how a workflow name looks in different contexts: readable name website url python class name static-energy.vasp.matproj https://simmate.org/workflows/static-energy/vasp/matproj StaticEnergy__Vasp__Matproj","title":"Why are there naming rules?"},{"location":"getting_started/custom_workflows/name_your_new_workflow/#overview-of-naming-conventions","text":"Simmate's naming conventions involves 3 parts: The type of analysis the workflow is doing The \"calculator\" (or program) that the workflow uses to run A unique name to identify the settings used Examples for each part would be: relaxation, static-energy, dynamics, ... vasp, abinit, qe, deepmd, ... jacks-test, matproj, quality00, ... Together, an example workflow names would be: relaxation.vasp.jacks-test static-energy.abinit.matproj dynamics.qe.quality00 When converting this to our workflow name in python, we need to replace periods with 2 underscores each and convert our words to pascal case . For example, our workflow names become: Relaxation__Vasp__JacksTest StaticEnergy__Abinit__Matproj Dynamics__Qe__Quality00 Warning Capitalization is very important here so make sure you double check your workflow names.","title":"Overview of naming conventions"},{"location":"getting_started/custom_workflows/name_your_new_workflow/#trying-it-out-in-python","text":"Now let's test this out in python using a similar workflow name: from simmate.workflow_engine import Workflow class Example__Python__MyFavoriteSettings ( Workflow ): pass # we will build the rest of workflow later # These names can be long and unfriendly, so it can be nice to # link them to a variable name for easier access. my_workflow = Example__Python__MyFavoriteSettings # Now check that our naming convention works as expected assert my_workflow . name_full == \"example.python.my-favorite-settings\" assert my_workflow . name_type == \"example\" assert my_workflow . name_calculator == \"python\" assert my_workflow . name_preset == \"my-favorite-settings\" You now have a ready-to-use workflow name! Tip assert effectively means \"make sure this statement returns True \". It's frequently used by python coders to make sure their code works as expected.","title":"Trying it out in python"},{"location":"getting_started/custom_workflows/next_steps/","text":"Taking things to next level \u00b6 There are still a lot of things we would want to do with our new workflow. For example, what if we want to... modify a complex workflow (such as diffusion.vasp.neb-all-paths-mit ) create a custom workflow using a new program like USPEX or ABINIT use a custom database table to save our workflow results access the workflow in the website interface access our workflow from other scripts (and the get_workflow function) We will cover these topics in the next tutorial -- while we tackle custom database tables as the same time. Tip For creating complex workflows and databases, you'll need to read through our API documentation, where we cover advanced cases. Also, don't be hesitate to post a question on our forum . We can also tell you the best place to start.","title":"Next steps"},{"location":"getting_started/custom_workflows/next_steps/#taking-things-to-next-level","text":"There are still a lot of things we would want to do with our new workflow. For example, what if we want to... modify a complex workflow (such as diffusion.vasp.neb-all-paths-mit ) create a custom workflow using a new program like USPEX or ABINIT use a custom database table to save our workflow results access the workflow in the website interface access our workflow from other scripts (and the get_workflow function) We will cover these topics in the next tutorial -- while we tackle custom database tables as the same time. Tip For creating complex workflows and databases, you'll need to read through our API documentation, where we cover advanced cases. Also, don't be hesitate to post a question on our forum . We can also tell you the best place to start.","title":"Taking things to next level"},{"location":"getting_started/custom_workflows/quick_start/","text":"Build custom workflows \u00b6 In this tutorial, you will learn how to build customized workflows. This tutorial only covers the bare minimum to creating a custom workflow. See the full guides and reference for more information. Warning There is no \"quick tutorial\" for this topic. Even advanced users should read everything!","title":"Quickstart"},{"location":"getting_started/custom_workflows/quick_start/#build-custom-workflows","text":"In this tutorial, you will learn how to build customized workflows. This tutorial only covers the bare minimum to creating a custom workflow. See the full guides and reference for more information. Warning There is no \"quick tutorial\" for this topic. Even advanced users should read everything!","title":"Build custom workflows"},{"location":"getting_started/custom_workflows/update_an_existing_workflow/","text":"Updating settings for a workflow \u00b6 Danger The methods described in this section are generally considered bad practice , but they are still useful for getting things started. After this section, we will cover to best way to update settings and create new workflows. Why isn't there a custom_settings option? \u00b6 We intentionally avoid the use of workflow.run(custom_settings=...) . This will NOT work. Simmate does this because we do not want to store results from customized settings in the same results table -- as this would (a) complicate analysis of many structures/systems and (b) make navigating results extremely difficult for beginners. For example, reducing the ENCUT or changing the dispersion correction of a VASP calculation makes it so energies cannot be compared between all materials in the table, and thus, features like calculated hull energies would become inaccruate. Instead, Simmate encourages the creation of new workflows and result tables when you want to customize settings. This puts Simmate's emphasis on \"scaling up\" workflows (i.e. running a fixed workflow on thousands on materials) as opposed to \"scaling out\" workflows (i.e. a flexible workflow that changes on a structure-by-structure basis). Update settings for an existing workflow \u00b6 For very quick testing, it is still useful to customize a workflow's settings without having to create a new workflow altogether. There are two approaches you can take to edit your settings: OPTION 1 \u00b6 Writing input files and manually submitting a separate program # This simply writes input files simmate workflows setup-only static-energy.vasp.mit --structure POSCAR # access your files in the new directory cd static-energy.vasp.mit.SETUP-ONLY # Customize input files as you see fit. # For example, you may want to edit INCAR settings nano INCAR # You can then submit VASP manually. Note, this will not use # simmate at all! So there is no error handling and no results # will be saved to your database. vasp_std > vasp.out OPTION 2 \u00b6 Using the \"customized\" workflow for a calculator (e.g. customized.vasp.user-config ) # In a file named \"my_example.yaml\". # Indicates we want to change the settings, using a specific workflow as a starting-point workflow_name : customized.vasp.user-config workflow_base : static-energy.vasp.mit # \"Updated settings\" indicated that we are updating some class attribute. # These fundamentally change the settings of a workflow. # Currently, only updating dictionary-based attributes are supported updated_settings : incar : ENCUT : 600 KPOINTS : 0.25 potcar_mappings : Y : Y_sv # Then the remaining inputs are the same as the base workflow input_parameters : structure : POSCAR command : mpirun -n 5 vasp_std > vasp.out # Now run our workflow from the settings file above. # Results will be stored in a separate table from the # base workflow's results. simmate workflows run-yaml my_example.yaml Warning These approaches are only possible with single-calcultion workflows (i.e. \"nested\" workflows that call several workflows within them are not supported) Avoid these approaches if possible! \u00b6 Both of options shown above are only suitable for customizing settings for a few calculations -- and also you lose some key Simmate features. If you are submitting many calculations (>20) and this process doesn't suit your needs, keep reading!","title":"Update an existing workflow"},{"location":"getting_started/custom_workflows/update_an_existing_workflow/#updating-settings-for-a-workflow","text":"Danger The methods described in this section are generally considered bad practice , but they are still useful for getting things started. After this section, we will cover to best way to update settings and create new workflows.","title":"Updating settings for a workflow"},{"location":"getting_started/custom_workflows/update_an_existing_workflow/#why-isnt-there-a-custom_settings-option","text":"We intentionally avoid the use of workflow.run(custom_settings=...) . This will NOT work. Simmate does this because we do not want to store results from customized settings in the same results table -- as this would (a) complicate analysis of many structures/systems and (b) make navigating results extremely difficult for beginners. For example, reducing the ENCUT or changing the dispersion correction of a VASP calculation makes it so energies cannot be compared between all materials in the table, and thus, features like calculated hull energies would become inaccruate. Instead, Simmate encourages the creation of new workflows and result tables when you want to customize settings. This puts Simmate's emphasis on \"scaling up\" workflows (i.e. running a fixed workflow on thousands on materials) as opposed to \"scaling out\" workflows (i.e. a flexible workflow that changes on a structure-by-structure basis).","title":"Why isn't there a custom_settings option?"},{"location":"getting_started/custom_workflows/update_an_existing_workflow/#update-settings-for-an-existing-workflow","text":"For very quick testing, it is still useful to customize a workflow's settings without having to create a new workflow altogether. There are two approaches you can take to edit your settings:","title":"Update settings for an existing workflow"},{"location":"getting_started/custom_workflows/update_an_existing_workflow/#option-1","text":"Writing input files and manually submitting a separate program # This simply writes input files simmate workflows setup-only static-energy.vasp.mit --structure POSCAR # access your files in the new directory cd static-energy.vasp.mit.SETUP-ONLY # Customize input files as you see fit. # For example, you may want to edit INCAR settings nano INCAR # You can then submit VASP manually. Note, this will not use # simmate at all! So there is no error handling and no results # will be saved to your database. vasp_std > vasp.out","title":"OPTION 1"},{"location":"getting_started/custom_workflows/update_an_existing_workflow/#option-2","text":"Using the \"customized\" workflow for a calculator (e.g. customized.vasp.user-config ) # In a file named \"my_example.yaml\". # Indicates we want to change the settings, using a specific workflow as a starting-point workflow_name : customized.vasp.user-config workflow_base : static-energy.vasp.mit # \"Updated settings\" indicated that we are updating some class attribute. # These fundamentally change the settings of a workflow. # Currently, only updating dictionary-based attributes are supported updated_settings : incar : ENCUT : 600 KPOINTS : 0.25 potcar_mappings : Y : Y_sv # Then the remaining inputs are the same as the base workflow input_parameters : structure : POSCAR command : mpirun -n 5 vasp_std > vasp.out # Now run our workflow from the settings file above. # Results will be stored in a separate table from the # base workflow's results. simmate workflows run-yaml my_example.yaml Warning These approaches are only possible with single-calcultion workflows (i.e. \"nested\" workflows that call several workflows within them are not supported)","title":"OPTION 2"},{"location":"getting_started/custom_workflows/update_an_existing_workflow/#avoid-these-approaches-if-possible","text":"Both of options shown above are only suitable for customizing settings for a few calculations -- and also you lose some key Simmate features. If you are submitting many calculations (>20) and this process doesn't suit your needs, keep reading!","title":"Avoid these approaches if possible!"},{"location":"getting_started/evolutionary_search/quick_start/","text":"In this tutorial, you will learn how to submit an evolutionary search and other massively parallel workflows. Danger Make sure you are using a cloud database if your workers are on separate machines or an HPC cluster. The default database backend (sqlite) is not built for parallel connections from different computers. Your calculations will be slower and error-prone with sqlite. If you are seeing the error database is locked , then you have exceeded the capabilities of sqlite. Warning Just like with earlier tutorials, a working VASP installation is required. The quick tutorial \u00b6 A fixed-composition search \u00b6 1) View input options and default settings of the structure-prediction.toolkit.fixed-composition workflow simmate workflows explore 2) (optional) If you wish for prototype structures or known materials to be included at the start of your search, make sure you have loaded the data into your cloud database. This was covered in an earlier tutorial . 3) Build our input yaml file (e.g. my_search.yaml ). Be sure to look at the full example for best-practices. basic input best-practice input workflow_name : structure-prediction.toolkit.fixed-composition composition : Ca8N4F8 workflow_name : structure-prediction.toolkit.fixed-composition composition : Ca8N4F8 subworkflow_kwargs : # (1) command : mpirun -n 8 vasp_std > vasp.out # (2) compress_output : true # (3) # see 'simmate workflows explore' on `relaxation.vasp.staged` # for other optional subworkflow_kwargs sleep_step : 300 # (4) nsteadystate : 100 # (5) # see 'simmate workflows explore' output for other optional inputs One of the default input parameters is subworkflow: relaxation.vasp.staged , which is used to analyze each structure. We can modify how this workflow behaves with the subworkflow_kwargs setting. The command that each relaxation step will be called with. You likely won't be reading through the output files of each calculation, and these resuklts will take up a lot of file space. This setting will convert completed calculations to zip files and help save on disk space. The status of individual structures & the writing of output files is checked on a cycle. If you know your average relaxation will take 30 minutes, then you likely don't need to check/update every 60 seconds (the default). Longer sleep steps help reduce database load. By default, nsteadystate is set to 40, which means the search will maintain 40 total calculations in the queue at all times -- regardless of the number of workers available. The number of workers (not nsteadystate ) controls the number of parallel calculations. This value should only be changed if you want more than 40 calculations ran in parallel. This means >40 workers must be running. Therefore, increase this value if you desire but do NOT decrease nsteadystate . 4) Submit the workflow settings file, which will start scheduling jobs. simmate workflows run my_search.yaml 5) Start workers by submitting the start-worker command to a cluster's queue (e.g. SLURM) or whereever you'd like jobs to run. Submit as many workers as workflows that you want ran in parallel: basic worker best-pracitce worker simmate workflow-engine start-worker # (1) simmate workflow-engine start-worker --close-on-empty-queue --nitems-max 10 --close-on-empty-queue shuts down the worker when the queue is empty. This helps release computational resources for others. --nitems-max limits the number of workflows each worker will run. Short-lived workers help maintain the health of a cluster and also allow other SLURM jobs to cycle through the queue -- avoiding the hogging of resources. 6) Monitor the output and log files for any issues. Important error information can also be accessed in the command line: simmate workflow-engine show-stats simmate workflow-engine show-error-summary 7) Submit new workers or cancel stale workers as needed. A binary-system search \u00b6 Follow the steps from the fixed-composition search above. The only difference here will be (i) the input yaml and (ii) follow-up searches. Make sure you read and understand best practices as well . Build your input yaml file: workflow_name : structure-prediction.toolkit.binary-system chemical_sysyem : Ca-N Submit the search simmate workflows run my_search.yaml Submit workers as-needed simmate workflow-engine start-worker","title":"Quickstart"},{"location":"getting_started/evolutionary_search/quick_start/#the-quick-tutorial","text":"","title":"The quick tutorial"},{"location":"getting_started/evolutionary_search/quick_start/#a-fixed-composition-search","text":"1) View input options and default settings of the structure-prediction.toolkit.fixed-composition workflow simmate workflows explore 2) (optional) If you wish for prototype structures or known materials to be included at the start of your search, make sure you have loaded the data into your cloud database. This was covered in an earlier tutorial . 3) Build our input yaml file (e.g. my_search.yaml ). Be sure to look at the full example for best-practices. basic input best-practice input workflow_name : structure-prediction.toolkit.fixed-composition composition : Ca8N4F8 workflow_name : structure-prediction.toolkit.fixed-composition composition : Ca8N4F8 subworkflow_kwargs : # (1) command : mpirun -n 8 vasp_std > vasp.out # (2) compress_output : true # (3) # see 'simmate workflows explore' on `relaxation.vasp.staged` # for other optional subworkflow_kwargs sleep_step : 300 # (4) nsteadystate : 100 # (5) # see 'simmate workflows explore' output for other optional inputs One of the default input parameters is subworkflow: relaxation.vasp.staged , which is used to analyze each structure. We can modify how this workflow behaves with the subworkflow_kwargs setting. The command that each relaxation step will be called with. You likely won't be reading through the output files of each calculation, and these resuklts will take up a lot of file space. This setting will convert completed calculations to zip files and help save on disk space. The status of individual structures & the writing of output files is checked on a cycle. If you know your average relaxation will take 30 minutes, then you likely don't need to check/update every 60 seconds (the default). Longer sleep steps help reduce database load. By default, nsteadystate is set to 40, which means the search will maintain 40 total calculations in the queue at all times -- regardless of the number of workers available. The number of workers (not nsteadystate ) controls the number of parallel calculations. This value should only be changed if you want more than 40 calculations ran in parallel. This means >40 workers must be running. Therefore, increase this value if you desire but do NOT decrease nsteadystate . 4) Submit the workflow settings file, which will start scheduling jobs. simmate workflows run my_search.yaml 5) Start workers by submitting the start-worker command to a cluster's queue (e.g. SLURM) or whereever you'd like jobs to run. Submit as many workers as workflows that you want ran in parallel: basic worker best-pracitce worker simmate workflow-engine start-worker # (1) simmate workflow-engine start-worker --close-on-empty-queue --nitems-max 10 --close-on-empty-queue shuts down the worker when the queue is empty. This helps release computational resources for others. --nitems-max limits the number of workflows each worker will run. Short-lived workers help maintain the health of a cluster and also allow other SLURM jobs to cycle through the queue -- avoiding the hogging of resources. 6) Monitor the output and log files for any issues. Important error information can also be accessed in the command line: simmate workflow-engine show-stats simmate workflow-engine show-error-summary 7) Submit new workers or cancel stale workers as needed.","title":"A fixed-composition search"},{"location":"getting_started/evolutionary_search/quick_start/#a-binary-system-search","text":"Follow the steps from the fixed-composition search above. The only difference here will be (i) the input yaml and (ii) follow-up searches. Make sure you read and understand best practices as well . Build your input yaml file: workflow_name : structure-prediction.toolkit.binary-system chemical_sysyem : Ca-N Submit the search simmate workflows run my_search.yaml Submit workers as-needed simmate workflow-engine start-worker","title":"A binary-system search"},{"location":"getting_started/explore_the_code/exploring_simmate_modules/","text":"Exploring Simmate's Modules \u00b6 Now that we know Simmate is just a bunch of classes organized into folders, let's explore a bit. We'll start with the toolkit module ( here , but try finding it yourself without the link). When you open it up, you'll see an overview/guide. You can also access this module using from simmate import toolkit and getting help directly in spyder. from simmate import toolkit toolkit # use ctrl+I before hitting enter A good folder to look through is the simmate.toolkit.creators module, which provides many ways to create lattices, sites, and structures (e.g. randomly, random symmetry, etc.) and also incorporates third-party codes. Take some time to look through the features and functions. Always feel free to ask if a feature exists, and if not, request one too. Post those questions in our discussions page . Warning because simmate is still at the early stages, some folders will be more complete than others. Keep this in mind while exploring. If you aren't seeing a guide or documentation, we probably haven't finished that module yet.","title":"Exploring Simmate modules"},{"location":"getting_started/explore_the_code/exploring_simmate_modules/#exploring-simmates-modules","text":"Now that we know Simmate is just a bunch of classes organized into folders, let's explore a bit. We'll start with the toolkit module ( here , but try finding it yourself without the link). When you open it up, you'll see an overview/guide. You can also access this module using from simmate import toolkit and getting help directly in spyder. from simmate import toolkit toolkit # use ctrl+I before hitting enter A good folder to look through is the simmate.toolkit.creators module, which provides many ways to create lattices, sites, and structures (e.g. randomly, random symmetry, etc.) and also incorporates third-party codes. Take some time to look through the features and functions. Always feel free to ask if a feature exists, and if not, request one too. Post those questions in our discussions page . Warning because simmate is still at the early stages, some folders will be more complete than others. Keep this in mind while exploring. If you aren't seeing a guide or documentation, we probably haven't finished that module yet.","title":"Exploring Simmate's Modules"},{"location":"getting_started/explore_the_code/getting_help_in_spyder/","text":"Getting help through Spyder \u00b6 Auto-complete in python \u00b6 Where we left off in the last tutorial, we saw how to list all available properties and methods on an object. We did this by typing the object name plus a period (ex: nacl_structure. ) and then hitting tab : from simmate.toolkit import Structure nacl_structure = Structure . from_file ( \"POSCAR\" ) nacl_structure . # hit \"tab\" on your keyboard Warning for this next part, pymatgen's documentation isn't always complete or beginner-friendly. This is why you won't see much. We're working on this at Simmate, so we hope this improves in the future. For now, don't expect too much guidance from the Structure class. Searching for a method's guides \u00b6 Now let's take a step back and get a full guide on a these methods and properties. We'll start with the Structure class that we previously imported using from simmate.toolkit import Structure and try the line Structure? : from simmate.toolkit import Structure Structure ? # <-- the ? here will bring up documentation help What pops up is the documentation. Just like how we were using --help in the command-line for tutorial 1, we can use ? in python to get help with python classes and objects! We can also format this nicely using Spyder. In bottom part of Spyder's top-right window, select the help tab. And in the search bar (with \"object\") right next to it, type in Structure . You'll see the help information pop up again, but now it's nicely formatted for us. Let's try this with our NaCl structure from before. Now try typing nacl_structure.get_primitive_structure in our help window. We can now see a description of what this does and the arguments/options (\"Args\") that it accepts. You can also get this help information by typing nacl_structure.get_primitive_structure in the python terminal and then using the ctrl+I shortcut. nacl_structure . get_primitive_structure # hit \"ctrl+I\" BEFORE hitting enter on this line Tip If the documentation is too short and not what you're looking for, chances are that we have more guides in the \"Full Guides & Reference\" section","title":"Hints in Spyder"},{"location":"getting_started/explore_the_code/getting_help_in_spyder/#getting-help-through-spyder","text":"","title":"Getting help through Spyder"},{"location":"getting_started/explore_the_code/getting_help_in_spyder/#auto-complete-in-python","text":"Where we left off in the last tutorial, we saw how to list all available properties and methods on an object. We did this by typing the object name plus a period (ex: nacl_structure. ) and then hitting tab : from simmate.toolkit import Structure nacl_structure = Structure . from_file ( \"POSCAR\" ) nacl_structure . # hit \"tab\" on your keyboard Warning for this next part, pymatgen's documentation isn't always complete or beginner-friendly. This is why you won't see much. We're working on this at Simmate, so we hope this improves in the future. For now, don't expect too much guidance from the Structure class.","title":"Auto-complete in python"},{"location":"getting_started/explore_the_code/getting_help_in_spyder/#searching-for-a-methods-guides","text":"Now let's take a step back and get a full guide on a these methods and properties. We'll start with the Structure class that we previously imported using from simmate.toolkit import Structure and try the line Structure? : from simmate.toolkit import Structure Structure ? # <-- the ? here will bring up documentation help What pops up is the documentation. Just like how we were using --help in the command-line for tutorial 1, we can use ? in python to get help with python classes and objects! We can also format this nicely using Spyder. In bottom part of Spyder's top-right window, select the help tab. And in the search bar (with \"object\") right next to it, type in Structure . You'll see the help information pop up again, but now it's nicely formatted for us. Let's try this with our NaCl structure from before. Now try typing nacl_structure.get_primitive_structure in our help window. We can now see a description of what this does and the arguments/options (\"Args\") that it accepts. You can also get this help information by typing nacl_structure.get_primitive_structure in the python terminal and then using the ctrl+I shortcut. nacl_structure . get_primitive_structure # hit \"ctrl+I\" BEFORE hitting enter on this line Tip If the documentation is too short and not what you're looking for, chances are that we have more guides in the \"Full Guides & Reference\" section","title":"Searching for a method's guides"},{"location":"getting_started/explore_the_code/intro_to_python_modules/","text":"Introduction to Python Modules \u00b6 One big question still remains though: how did we know to type from simmate.toolkit import Structure ? Here, you should learn to think of python packages (such as Simmate) as many classes and functions organized into folders. Conceptual examples \u00b6 As an example, you can read from simmate.toolkit import Structure as \"Inside of the simmate folder, go to the toolkit file and load the Structure class\". Here's a second example: from simmate.toolkit.base_data_types.lattice import Lattice is the same as saying \"Go to the simmate folder --> toolkit folder --> base_data_types folder --> lattice.py file --> grab the Lattice class\". So whenever you see an import line, it's just telling you where the actual code is located. All of Simmate's code (and all python codes everywhere) is organized like this. A walk-through example \u00b6 To prove it, let's go through these steps: on Simmate's github homepage, go the src/simmate folder (src = source code) You'll see the toolkit folder that we were using before. Navigate through the folders. simmate --> database --> base_data_types --> calculation.py . You see a Calculation class where all of it's methods and properties are defined. Each of these folders and files are referred to as python \"modules\" -- it's just python terminology.","title":"Intro to Python modules"},{"location":"getting_started/explore_the_code/intro_to_python_modules/#introduction-to-python-modules","text":"One big question still remains though: how did we know to type from simmate.toolkit import Structure ? Here, you should learn to think of python packages (such as Simmate) as many classes and functions organized into folders.","title":"Introduction to Python Modules"},{"location":"getting_started/explore_the_code/intro_to_python_modules/#conceptual-examples","text":"As an example, you can read from simmate.toolkit import Structure as \"Inside of the simmate folder, go to the toolkit file and load the Structure class\". Here's a second example: from simmate.toolkit.base_data_types.lattice import Lattice is the same as saying \"Go to the simmate folder --> toolkit folder --> base_data_types folder --> lattice.py file --> grab the Lattice class\". So whenever you see an import line, it's just telling you where the actual code is located. All of Simmate's code (and all python codes everywhere) is organized like this.","title":"Conceptual examples"},{"location":"getting_started/explore_the_code/intro_to_python_modules/#a-walk-through-example","text":"To prove it, let's go through these steps: on Simmate's github homepage, go the src/simmate folder (src = source code) You'll see the toolkit folder that we were using before. Navigate through the folders. simmate --> database --> base_data_types --> calculation.py . You see a Calculation class where all of it's methods and properties are defined. Each of these folders and files are referred to as python \"modules\" -- it's just python terminology.","title":"A walk-through example"},{"location":"getting_started/explore_the_code/quick_start/","text":"Explore the code \u00b6 Note If you are familiar with how to explore python code and documentation, you can skip this tutorial. Even though you can run workflows without understanding what happens behind the scenes, you should avoid using Simmate like a black box . In this tutorial, you will learn how to explore Simmate's code and features, rather than just copying/pasting code from other tutorials.","title":"Quickstart"},{"location":"getting_started/explore_the_code/quick_start/#explore-the-code","text":"Note If you are familiar with how to explore python code and documentation, you can skip this tutorial. Even though you can run workflows without understanding what happens behind the scenes, you should avoid using Simmate like a black box . In this tutorial, you will learn how to explore Simmate's code and features, rather than just copying/pasting code from other tutorials.","title":"Explore the code"},{"location":"getting_started/installation/command_line/","text":"Switching to the Command-line \u00b6 While the Anaconda Navigator interface is convenient if you've never written code before, it is much quicker (and easier!) to use the command-line. Don't worry, it's much simpler than you probably expect. Even if you don't know how to code, you can master the command-line in just a few minutes. Each command can be viewed as a \"button\". For example, the command cd stands for \"change directory\". When you call it, it just opens up a new folder to view the contents -- so the same thing as double-clicking a folder to open it up. Running our first command \u00b6 Let's try this out with our command-line. On Windows, search for and open \"Anaconda Powershell Prompt\" using your Start menu. On Mac and Linux, search for and open the app named \"Terminal\" You should see something like this: You'll see (base) at the start of the line. This is our anaconda enviornment that we are currently using. After that, you'll see the \"current working directory\", which is the folder we currently have open and are sitting in. On Windows this will be your user folder (e.g. C:\\Users\\jacksund ) and for Mac/Linux you'll see ~ which is shorthand for your user folder (e.g. home/jacksund ). Now, try typing in the command cd Desktop and then hit enter. This will open up your Desktop folder. Then enter the command ls , which will list all files and folders on your Desktop. # run these two commands cd Desktop ls Learning new commands \u00b6 Tip For other simple commands, you can take a look at this cheat sheet or take a full tutorial . Memorizing commands will come slowly over time, so keep this cheat-sheet handy. We highly recommend that you spend 30 minutes going through these links once you finish this tutorial. Obviously, the tricky part with the command-line is knowing what to type. Fortunately, however, most programs have a single command that forms the base of more complex commands. For anaconda, the command is conda . If you aren't sure what it does or how to use it, you just add --help to it. Type in the command conda --help and you'll see an output like this: conda --help usage: conda [ -h ] [ -V ] command ... conda is a tool for managing and deploying applications, environments and packages. Options: positional arguments: command clean Remove unused packages and caches. compare Compare packages between conda environments. config Modify configuration values in .condarc. This is modeled after the git config command. Writes to the user .condarc file ( /home/jacksund/.condarc ) by default. create Create a new conda environment from a list of specified packages. info Display information about current conda install. init Initialize conda for shell interaction. [ Experimental ] install Installs a list of packages into a specified conda environment. list List linked packages in a conda environment. package Low-level conda package utility. ( EXPERIMENTAL ) remove Remove a list of packages from a specified conda environment. uninstall Alias for conda remove. run Run an executable in a conda environment. search Search for packages and display associated information. The input is a MatchSpec, a query language for conda packages. See examples below. update Updates conda packages to the latest compatible version. upgrade Alias for conda update. optional arguments: -h, --help Show this help message and exit. -V, --version Show the conda version number and exit. conda commands available from other packages: build content-trust convert debug develop env index inspect metapackage pack render repo server skeleton token verify Don't get overwhelmed by the amount of information printed out. Each line is getting accross a simple idea. For example, the line -h, --help Show this help message and exit. is telling us what the conda --help command does! It also tells us that we could have done conda -h for the same output. This help message also tells us there are other \"subcommands\" available. One is create which says it creates a new environment. To learn more about that one, we can run the command conda create --help . There's a bunch here... But again, you don't need to memorize all of this. Just remember how to get this help page when you need it. Up next, we'll use these commands to create our environment and install Simmate.","title":"Intro to the command line"},{"location":"getting_started/installation/command_line/#switching-to-the-command-line","text":"While the Anaconda Navigator interface is convenient if you've never written code before, it is much quicker (and easier!) to use the command-line. Don't worry, it's much simpler than you probably expect. Even if you don't know how to code, you can master the command-line in just a few minutes. Each command can be viewed as a \"button\". For example, the command cd stands for \"change directory\". When you call it, it just opens up a new folder to view the contents -- so the same thing as double-clicking a folder to open it up.","title":"Switching to the Command-line"},{"location":"getting_started/installation/command_line/#running-our-first-command","text":"Let's try this out with our command-line. On Windows, search for and open \"Anaconda Powershell Prompt\" using your Start menu. On Mac and Linux, search for and open the app named \"Terminal\" You should see something like this: You'll see (base) at the start of the line. This is our anaconda enviornment that we are currently using. After that, you'll see the \"current working directory\", which is the folder we currently have open and are sitting in. On Windows this will be your user folder (e.g. C:\\Users\\jacksund ) and for Mac/Linux you'll see ~ which is shorthand for your user folder (e.g. home/jacksund ). Now, try typing in the command cd Desktop and then hit enter. This will open up your Desktop folder. Then enter the command ls , which will list all files and folders on your Desktop. # run these two commands cd Desktop ls","title":"Running our first command"},{"location":"getting_started/installation/command_line/#learning-new-commands","text":"Tip For other simple commands, you can take a look at this cheat sheet or take a full tutorial . Memorizing commands will come slowly over time, so keep this cheat-sheet handy. We highly recommend that you spend 30 minutes going through these links once you finish this tutorial. Obviously, the tricky part with the command-line is knowing what to type. Fortunately, however, most programs have a single command that forms the base of more complex commands. For anaconda, the command is conda . If you aren't sure what it does or how to use it, you just add --help to it. Type in the command conda --help and you'll see an output like this: conda --help usage: conda [ -h ] [ -V ] command ... conda is a tool for managing and deploying applications, environments and packages. Options: positional arguments: command clean Remove unused packages and caches. compare Compare packages between conda environments. config Modify configuration values in .condarc. This is modeled after the git config command. Writes to the user .condarc file ( /home/jacksund/.condarc ) by default. create Create a new conda environment from a list of specified packages. info Display information about current conda install. init Initialize conda for shell interaction. [ Experimental ] install Installs a list of packages into a specified conda environment. list List linked packages in a conda environment. package Low-level conda package utility. ( EXPERIMENTAL ) remove Remove a list of packages from a specified conda environment. uninstall Alias for conda remove. run Run an executable in a conda environment. search Search for packages and display associated information. The input is a MatchSpec, a query language for conda packages. See examples below. update Updates conda packages to the latest compatible version. upgrade Alias for conda update. optional arguments: -h, --help Show this help message and exit. -V, --version Show the conda version number and exit. conda commands available from other packages: build content-trust convert debug develop env index inspect metapackage pack render repo server skeleton token verify Don't get overwhelmed by the amount of information printed out. Each line is getting accross a simple idea. For example, the line -h, --help Show this help message and exit. is telling us what the conda --help command does! It also tells us that we could have done conda -h for the same output. This help message also tells us there are other \"subcommands\" available. One is create which says it creates a new environment. To learn more about that one, we can run the command conda create --help . There's a bunch here... But again, you don't need to memorize all of this. Just remember how to get this help page when you need it. Up next, we'll use these commands to create our environment and install Simmate.","title":"Learning new commands"},{"location":"getting_started/installation/create_your_env/","text":"Creating our environment and installing Simmate \u00b6 For now, let's create a new environment that uses the conda-forge channel. A channel is where anaconda downloads packages from -- and to keep things simple, we will ALWAYS use conda-forge (this is the standard in the python community). 1. Create the environment \u00b6 Now let's run this command below. Note you can switch out the name my_env for whatever you want here, just make sure you use underscores instead of spaces ( my_env will work while my env will give an error). conda create -c conda-forge -n my_env python = 3 .10 Say yes when the installation asks for confirmation. 2. Activate the environment \u00b6 Next, switch to this new environment. To do that we use... conda activate my_env You'll see the start of your command line switch from (base) to (my_env) if this worked successfully. 3. Install Simmate \u00b6 Next, we want to install Simmate. conda install -c conda-forge -n my_env simmate This may take a few minutes to run and install. But once it's done, you've now successfully installed Simmate! If you ran into any errors with this very last command, please let our team know immediately by posting a new issue here . 4. Install Spyder \u00b6 As an extra, let's use Anaconda to install Spyder . Spyder is what we will use to write python in later tutorials. But now that we have Anaconda set up, installing new programs can be done in just one line: conda install -c conda-forge -n my_env spyder","title":"Create your environment"},{"location":"getting_started/installation/create_your_env/#creating-our-environment-and-installing-simmate","text":"For now, let's create a new environment that uses the conda-forge channel. A channel is where anaconda downloads packages from -- and to keep things simple, we will ALWAYS use conda-forge (this is the standard in the python community).","title":"Creating our environment and installing Simmate"},{"location":"getting_started/installation/create_your_env/#1-create-the-environment","text":"Now let's run this command below. Note you can switch out the name my_env for whatever you want here, just make sure you use underscores instead of spaces ( my_env will work while my env will give an error). conda create -c conda-forge -n my_env python = 3 .10 Say yes when the installation asks for confirmation.","title":"1. Create the environment"},{"location":"getting_started/installation/create_your_env/#2-activate-the-environment","text":"Next, switch to this new environment. To do that we use... conda activate my_env You'll see the start of your command line switch from (base) to (my_env) if this worked successfully.","title":"2. Activate the environment"},{"location":"getting_started/installation/create_your_env/#3-install-simmate","text":"Next, we want to install Simmate. conda install -c conda-forge -n my_env simmate This may take a few minutes to run and install. But once it's done, you've now successfully installed Simmate! If you ran into any errors with this very last command, please let our team know immediately by posting a new issue here .","title":"3. Install Simmate"},{"location":"getting_started/installation/create_your_env/#4-install-spyder","text":"As an extra, let's use Anaconda to install Spyder . Spyder is what we will use to write python in later tutorials. But now that we have Anaconda set up, installing new programs can be done in just one line: conda install -c conda-forge -n my_env spyder","title":"4. Install Spyder"},{"location":"getting_started/installation/explore_simmate_cli/","text":"Exploring Simmate's command-line \u00b6 Just like we used conda --help above, we can also ask for help with Simmate. Start with running the command simmate --help and you should see the following output: simmate --help Usage: simmate [OPTIONS] COMMAND [ARGS]... This is the base command that all other Simmate commands stem from \ud83d\udd25\ud83d\udd25\ud83d\ude80 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 If you are a beginner to the command line, be sure to start with our tutorials. Below you will see a list of sub-commands to try. For example, you can run simmate database --help to learn more about it. TIP: Many Simmate commands are long and verbose. You can use --install-completion to add ipython-like autocomplete to your shell. \u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 --install-completion Install completion for the current shell. \u2502 \u2502 --show-completion Show completion for the current shell, to copy it or customize the \u2502 \u2502 installation. \u2502 \u2502 --help Show this message and exit. \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 database A group of commands for managing your database \u2502 \u2502 run-server Runs a local test server for the Simmate website interface \u2502 \u2502 start-project Creates a new folder and fills it with an example project to get you started with custom \u2502 \u2502 Simmate workflows/datatables \u2502 \u2502 utilities A group of commands for various simple tasks (such as file handling) \u2502 \u2502 workflow-engine A group of commands for starting up computational resources (Workers, Agents, and Clusters) \u2502 \u2502 workflows A group of commands for running workflows or viewing their settings \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f You can see there are many other commands like simmate database and simmate workflows that we will explore in other tutorials.","title":"Explore Simmate commands"},{"location":"getting_started/installation/explore_simmate_cli/#exploring-simmates-command-line","text":"Just like we used conda --help above, we can also ask for help with Simmate. Start with running the command simmate --help and you should see the following output: simmate --help Usage: simmate [OPTIONS] COMMAND [ARGS]... This is the base command that all other Simmate commands stem from \ud83d\udd25\ud83d\udd25\ud83d\ude80 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 If you are a beginner to the command line, be sure to start with our tutorials. Below you will see a list of sub-commands to try. For example, you can run simmate database --help to learn more about it. TIP: Many Simmate commands are long and verbose. You can use --install-completion to add ipython-like autocomplete to your shell. \u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 --install-completion Install completion for the current shell. \u2502 \u2502 --show-completion Show completion for the current shell, to copy it or customize the \u2502 \u2502 installation. \u2502 \u2502 --help Show this message and exit. \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 database A group of commands for managing your database \u2502 \u2502 run-server Runs a local test server for the Simmate website interface \u2502 \u2502 start-project Creates a new folder and fills it with an example project to get you started with custom \u2502 \u2502 Simmate workflows/datatables \u2502 \u2502 utilities A group of commands for various simple tasks (such as file handling) \u2502 \u2502 workflow-engine A group of commands for starting up computational resources (Workers, Agents, and Clusters) \u2502 \u2502 workflows A group of commands for running workflows or viewing their settings \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f You can see there are many other commands like simmate database and simmate workflows that we will explore in other tutorials.","title":"Exploring Simmate's command-line"},{"location":"getting_started/installation/extra/","text":"Extra Resources \u00b6 Before going to the next tutorial, we recommend going through the following guides. These will save you TONS of time in the long run -- so don't rush ahead. A command-line cheat sheet --> print this out and keep it at your desk Anconda's getting-started guides --> take ~10min to glance through A full tutorial for the command-line --> go through parts 1 and 2 to master the CLI (1-2hrs)","title":"Extra learning resources"},{"location":"getting_started/installation/extra/#extra-resources","text":"Before going to the next tutorial, we recommend going through the following guides. These will save you TONS of time in the long run -- so don't rush ahead. A command-line cheat sheet --> print this out and keep it at your desk Anconda's getting-started guides --> take ~10min to glance through A full tutorial for the command-line --> go through parts 1 and 2 to master the CLI (1-2hrs)","title":"Extra Resources"},{"location":"getting_started/installation/install_anaconda/","text":"Installing Anaconda \u00b6 Why Anaconda? \u00b6 Ideally, you could download Simmate like any desktop app and then be good to go. But have you ever updated your operating system and then everything else on your computer goes haywire? With python, the chances of that happening are extremely high, so we want to be careful. For example, say I wanted two packages installed: Simmate version 1.0, which requires python version 3.10 or greater NumPy version 1.15, which requires python version 2.7 The conflicting versions of python creates a problem. To solve this, we use Anaconda . Anaconda installs python and all of our extra packages, including Simmate. To make sure nothing ever breaks, it separates each of our installations into folders known as \"environments\". Package versions can also conflict with each other. Therefore, Anaconda also prevents you from installing conflicting versions within a single environment. Example Using the 2 programs given above, we could have two environments: one named simmate_env and another named numpy_env (these can be named anything). The two different python versions and codes would be installed into separate folders so that they don't interact. Example Simmate requires the package Django (version >3.2), so if you installed Django 2.0, it would break your Simmate installation. Anaconda will catch your mistake before it causes confusing errors elsewhere. Installing Anaconda and a first look \u00b6 For this tutorial, we'll install Anaconda to your local desktop/laptop. Even if you'll be using a university supercomputer (or some other remote computer system) to run workflows, stick to your local computer. We'll switch to your remote supercomputer in a later tutorial. 1. Install Anaconda \u00b6 To install Anaconda, you don't need to make an account on their website. Just use their download page and install Anaconda. Use all of the default options when installing. 2. Open Anaconda \u00b6 Once the download is finished, open up the application. The application will be called Anaconda Navigator . On the homescreen, you'll see several IDEs, such as Orange3, Jupyter Notebook, Spyder, and others. These IDEs are for you to write your own python code. Just like how there is Microsoft Word, Google Docs, LibreOffice, and others for writing papers, all of these IDEs are different ways to write python. Our prefered IDE is Spyder , which we will introduce in a later tutorial. 3. View environments \u00b6 On the left side of the application window, you'll see an Environments tab. Go ahead and open it. When you first install Anaconda, there will only be a \"base\" environment that already has popular packages installed. You can create new environments here and install new packages into each -- all without affecting what's already installed. Final comments \u00b6 That's really it to the Anaconda interface! While we can install Simmate with this interface, it's actually even easier with the command-line. The remainder of this tutorial will use the command-line instead of the Anaconda Navigator interface. Tip If you want a more complete overview of Anaconda, they have a series of getting-started guides available, but these guides aren't required for using Simmate (so don't spend any more than 10 minutes looking through them).","title":"Install Anaconda"},{"location":"getting_started/installation/install_anaconda/#installing-anaconda","text":"","title":"Installing Anaconda"},{"location":"getting_started/installation/install_anaconda/#why-anaconda","text":"Ideally, you could download Simmate like any desktop app and then be good to go. But have you ever updated your operating system and then everything else on your computer goes haywire? With python, the chances of that happening are extremely high, so we want to be careful. For example, say I wanted two packages installed: Simmate version 1.0, which requires python version 3.10 or greater NumPy version 1.15, which requires python version 2.7 The conflicting versions of python creates a problem. To solve this, we use Anaconda . Anaconda installs python and all of our extra packages, including Simmate. To make sure nothing ever breaks, it separates each of our installations into folders known as \"environments\". Package versions can also conflict with each other. Therefore, Anaconda also prevents you from installing conflicting versions within a single environment. Example Using the 2 programs given above, we could have two environments: one named simmate_env and another named numpy_env (these can be named anything). The two different python versions and codes would be installed into separate folders so that they don't interact. Example Simmate requires the package Django (version >3.2), so if you installed Django 2.0, it would break your Simmate installation. Anaconda will catch your mistake before it causes confusing errors elsewhere.","title":"Why Anaconda?"},{"location":"getting_started/installation/install_anaconda/#installing-anaconda-and-a-first-look","text":"For this tutorial, we'll install Anaconda to your local desktop/laptop. Even if you'll be using a university supercomputer (or some other remote computer system) to run workflows, stick to your local computer. We'll switch to your remote supercomputer in a later tutorial.","title":"Installing Anaconda and a first look"},{"location":"getting_started/installation/install_anaconda/#1-install-anaconda","text":"To install Anaconda, you don't need to make an account on their website. Just use their download page and install Anaconda. Use all of the default options when installing.","title":"1. Install Anaconda"},{"location":"getting_started/installation/install_anaconda/#2-open-anaconda","text":"Once the download is finished, open up the application. The application will be called Anaconda Navigator . On the homescreen, you'll see several IDEs, such as Orange3, Jupyter Notebook, Spyder, and others. These IDEs are for you to write your own python code. Just like how there is Microsoft Word, Google Docs, LibreOffice, and others for writing papers, all of these IDEs are different ways to write python. Our prefered IDE is Spyder , which we will introduce in a later tutorial.","title":"2. Open Anaconda"},{"location":"getting_started/installation/install_anaconda/#3-view-environments","text":"On the left side of the application window, you'll see an Environments tab. Go ahead and open it. When you first install Anaconda, there will only be a \"base\" environment that already has popular packages installed. You can create new environments here and install new packages into each -- all without affecting what's already installed.","title":"3. View environments"},{"location":"getting_started/installation/install_anaconda/#final-comments","text":"That's really it to the Anaconda interface! While we can install Simmate with this interface, it's actually even easier with the command-line. The remainder of this tutorial will use the command-line instead of the Anaconda Navigator interface. Tip If you want a more complete overview of Anaconda, they have a series of getting-started guides available, but these guides aren't required for using Simmate (so don't spend any more than 10 minutes looking through them).","title":"Final comments"},{"location":"getting_started/installation/intro_to_python_terms/","text":"A quick introduction to python terms \u00b6 As you read through our tutorials, we try to explain many of the python terms and concepts, but for your reference, we have written out some quick definitions here. You won't be expected to understand all of these terms until tutorial 06, so revisit these definitions after each tutorial to review your understanding. package \u00b6 A package is, essentially, a program written in python. Simmate is a package package manager \u00b6 A package manager is what installs Python and any extra packages for us. Importantly, it makes sure we install the correct version of Python and packages. Anaconda is our package manager, and it installs Python and Simmate for us. environment \u00b6 Python packages are installed into a specific environment on your computer. An environment is (effectively) a folder containing many installed packages that are each compatible each other. A single computer can have many environments, where you only ever use one at a time. In this tutorial, \"my_env\" is the name of our environment and Simmate is installed in it (along with many other packages such as pymatgen and numpy). module \u00b6 A python package is made from modules. Each module provides a related set of functions. For example, \"simmate.database\" is a module that holds all of the code used to run our database. IDE \u00b6 The program we write and run python code in. Microsoft Word would be an IDE for writing essays. Spyder is our IDE for python. IDE is short for integrated development environment. Command line vs. Python \u00b6 Don't confuse two types of programming language we will use: command-line : Whether you are on Windows, Mac, or Linux, the command line (aka the terminal) is used for common functions like changing directories (cd Desktop) or running programs. This includes running commands, such as: simmate workflows run . python : used when we write complex logic and customize its settings. workflow.run(structure=...) is python code used in our IDE.","title":"Intro to python terms"},{"location":"getting_started/installation/intro_to_python_terms/#a-quick-introduction-to-python-terms","text":"As you read through our tutorials, we try to explain many of the python terms and concepts, but for your reference, we have written out some quick definitions here. You won't be expected to understand all of these terms until tutorial 06, so revisit these definitions after each tutorial to review your understanding.","title":"A quick introduction to python terms"},{"location":"getting_started/installation/intro_to_python_terms/#package","text":"A package is, essentially, a program written in python. Simmate is a package","title":"package"},{"location":"getting_started/installation/intro_to_python_terms/#package-manager","text":"A package manager is what installs Python and any extra packages for us. Importantly, it makes sure we install the correct version of Python and packages. Anaconda is our package manager, and it installs Python and Simmate for us.","title":"package manager"},{"location":"getting_started/installation/intro_to_python_terms/#environment","text":"Python packages are installed into a specific environment on your computer. An environment is (effectively) a folder containing many installed packages that are each compatible each other. A single computer can have many environments, where you only ever use one at a time. In this tutorial, \"my_env\" is the name of our environment and Simmate is installed in it (along with many other packages such as pymatgen and numpy).","title":"environment"},{"location":"getting_started/installation/intro_to_python_terms/#module","text":"A python package is made from modules. Each module provides a related set of functions. For example, \"simmate.database\" is a module that holds all of the code used to run our database.","title":"module"},{"location":"getting_started/installation/intro_to_python_terms/#ide","text":"The program we write and run python code in. Microsoft Word would be an IDE for writing essays. Spyder is our IDE for python. IDE is short for integrated development environment.","title":"IDE"},{"location":"getting_started/installation/intro_to_python_terms/#command-line-vs-python","text":"Don't confuse two types of programming language we will use: command-line : Whether you are on Windows, Mac, or Linux, the command line (aka the terminal) is used for common functions like changing directories (cd Desktop) or running programs. This includes running commands, such as: simmate workflows run . python : used when we write complex logic and customize its settings. workflow.run(structure=...) is python code used in our IDE.","title":"Command line vs. Python"},{"location":"getting_started/installation/local_server_setup/","text":"Starting your local test server \u00b6 On our official website, you are able to explore all results from past workflows that we've ran. Even though you haven't ran any yet, you can do the same thing on your local computer too. All that's required are two simple commands. 1. reset the database \u00b6 First, we need to setup our database. We'll explain what this is doing in the next tutorial, but for now, think of this as building an empty excel spreadsheet that we can later fill with data. This is done with... simmate database reset # (1) When prompted, you can confirm that you want to reset/delete your \"old\" database. Also agree to download and use a prebuilt database. Tip Unless you want to remove all of your data and start from scratch, you'll never have to run that command again. 2. start the server \u00b6 And then as our second step, we simply tell Simmate to start the server: simmate run-server ... and after a few seconds, you should see the output ... Watching for file changes with StatReloader April 05 , 2022 - 00 :06:54 Django version 4 .0.2, using settings 'simmate.configuration.django.settings' Starting development server at http://127.0.0.1:8000/ Quit the server with CTRL-BREAK. Leave this command running in your terminal and open up the link http://127.0.0.1:8000/ in your preferred browser (Chrome, Firefox, etc.). You should see what looks like the simmate.org website! Can I share these links? \u00b6 This website is running on your local computer . No one can access it through the internet, and as soon as your close your terminal running the simmate run-server , this site will stop working. However, Simmate becomes especially powerful when we switch to a server that accessible through the internet -- that way, you can share results and computational resources with your entire team. Also, there are many more features in python and command-line than through the website interface. To explore these, you'll have to keep reading our tutorials!","title":"Set up a Local Server"},{"location":"getting_started/installation/local_server_setup/#starting-your-local-test-server","text":"On our official website, you are able to explore all results from past workflows that we've ran. Even though you haven't ran any yet, you can do the same thing on your local computer too. All that's required are two simple commands.","title":"Starting your local test server"},{"location":"getting_started/installation/local_server_setup/#1-reset-the-database","text":"First, we need to setup our database. We'll explain what this is doing in the next tutorial, but for now, think of this as building an empty excel spreadsheet that we can later fill with data. This is done with... simmate database reset # (1) When prompted, you can confirm that you want to reset/delete your \"old\" database. Also agree to download and use a prebuilt database. Tip Unless you want to remove all of your data and start from scratch, you'll never have to run that command again.","title":"1. reset the database"},{"location":"getting_started/installation/local_server_setup/#2-start-the-server","text":"And then as our second step, we simply tell Simmate to start the server: simmate run-server ... and after a few seconds, you should see the output ... Watching for file changes with StatReloader April 05 , 2022 - 00 :06:54 Django version 4 .0.2, using settings 'simmate.configuration.django.settings' Starting development server at http://127.0.0.1:8000/ Quit the server with CTRL-BREAK. Leave this command running in your terminal and open up the link http://127.0.0.1:8000/ in your preferred browser (Chrome, Firefox, etc.). You should see what looks like the simmate.org website!","title":"2. start the server"},{"location":"getting_started/installation/local_server_setup/#can-i-share-these-links","text":"This website is running on your local computer . No one can access it through the internet, and as soon as your close your terminal running the simmate run-server , this site will stop working. However, Simmate becomes especially powerful when we switch to a server that accessible through the internet -- that way, you can share results and computational resources with your entire team. Also, there are many more features in python and command-line than through the website interface. To explore these, you'll have to keep reading our tutorials!","title":"Can I share these links?"},{"location":"getting_started/installation/quick_start/","text":"Installation \u00b6 In this tutorial, you will learn how to install Simmate with Anaconda and start up a local server. Beginners will also be introduced to the command-line. Danger In this tutorial and others, beginners without coding experience should skip the quick-start tutorial section and jump straight to the full tutorial. The critical steps in each are exactly the same , but the full tutorial includes extra exploration of the software and how to use it. Quick start \u00b6 Install anaconda Create a conda environment, install Simmate in it, and activate it. (note: Spyder is our recommended IDE but optional) conda create -n my_env -c conda-forge python = 3 .10 simmate conda install -n my_env -c conda-forge spyder # optional but recommended conda activate my_env Run the simmate --help command to make sure it's installed correctly For first-time setup, initialize your local database with simmate database reset Run the command simmate run-server (and leave this command running) Go to http://127.0.0.1:8000/ and you'll see you local server! Note, this server is only accessible on your local computer. For a production-ready server, see our website documentation . Note Simmate itself is <2MB, but when installed to a clean conda environment, the entire download for all it's dependencies comes to ~1.2GB. Additional disk space is also needed for optional downloads -- such as third-party data .","title":"Quickstart"},{"location":"getting_started/installation/quick_start/#installation","text":"In this tutorial, you will learn how to install Simmate with Anaconda and start up a local server. Beginners will also be introduced to the command-line. Danger In this tutorial and others, beginners without coding experience should skip the quick-start tutorial section and jump straight to the full tutorial. The critical steps in each are exactly the same , but the full tutorial includes extra exploration of the software and how to use it.","title":"Installation"},{"location":"getting_started/installation/quick_start/#quick-start","text":"Install anaconda Create a conda environment, install Simmate in it, and activate it. (note: Spyder is our recommended IDE but optional) conda create -n my_env -c conda-forge python = 3 .10 simmate conda install -n my_env -c conda-forge spyder # optional but recommended conda activate my_env Run the simmate --help command to make sure it's installed correctly For first-time setup, initialize your local database with simmate database reset Run the command simmate run-server (and leave this command running) Go to http://127.0.0.1:8000/ and you'll see you local server! Note, this server is only accessible on your local computer. For a production-ready server, see our website documentation . Note Simmate itself is <2MB, but when installed to a clean conda environment, the entire download for all it's dependencies comes to ~1.2GB. Additional disk space is also needed for optional downloads -- such as third-party data .","title":"Quick start"},{"location":"getting_started/run_a_workflow/configure_database/","text":"Setting up our database \u00b6 We often want to run the same calculation on many materials, so Simmate pre-builds database tables for us to fill. This just means we make tables (like those used in Excel), where we have all the column headers ready to go. For example, you can imagine that a table of structures would have columns for formula, density, and number of sites, among other things. Simmate builds these tables for you and automatically fills all the columns with data after a calculation finishes. 1. Reset the database \u00b6 We will explore what these tables look like in tutorial 5, but for now, we want Simmate to create them. All we have to do is run the following command simmate database reset When you call this command, Simmate will print out a bunch of information -- this can be ignored for now. It's just making all of your tables. Warning Every time you run the command simmate database reset , your database is deleted and a new one is written with empty tables. If you want to keep your previous runs, you should save a copy of your database, which can be done by just copy and pasting the database file 2. Finding our database file \u00b6 So where is the database stored? After running simmate database reset , you'll find it in a file named ~/simmate/my_env-database.sqlite3 . Note Notice that your conda envirnment ( my_env here) is used in the database file name. Simmate does this so you can easily switch between databases just by switching your Anaconda environment. This is useful for testing and developing new workflows, which we will cover in a later tutorial. To find this file: remember from tutorial 1 that ~ is short for our home directory -- typically something like /home/jacksund/ or C:\\Users\\jacksund . have \"show hidden files\" turned on in your File Explorer (on Windows, check \"show file name extensions\" under the \"View\" tab). Then you'll see a file named my_env-database.sqlite3 instead of just my_env-database . You won't be able to double-click this file. Just like how you need Excel to open and read Excel (.xlsx) files, we need a separate program to read database (.sqlite3) files. We'll use Simmate to do this later on. But just after that one command, our database is setup any ready to use! We can now run workflows and start adding data to it.","title":"Configure the database"},{"location":"getting_started/run_a_workflow/configure_database/#setting-up-our-database","text":"We often want to run the same calculation on many materials, so Simmate pre-builds database tables for us to fill. This just means we make tables (like those used in Excel), where we have all the column headers ready to go. For example, you can imagine that a table of structures would have columns for formula, density, and number of sites, among other things. Simmate builds these tables for you and automatically fills all the columns with data after a calculation finishes.","title":"Setting up our database"},{"location":"getting_started/run_a_workflow/configure_database/#1-reset-the-database","text":"We will explore what these tables look like in tutorial 5, but for now, we want Simmate to create them. All we have to do is run the following command simmate database reset When you call this command, Simmate will print out a bunch of information -- this can be ignored for now. It's just making all of your tables. Warning Every time you run the command simmate database reset , your database is deleted and a new one is written with empty tables. If you want to keep your previous runs, you should save a copy of your database, which can be done by just copy and pasting the database file","title":"1. Reset the database"},{"location":"getting_started/run_a_workflow/configure_database/#2-finding-our-database-file","text":"So where is the database stored? After running simmate database reset , you'll find it in a file named ~/simmate/my_env-database.sqlite3 . Note Notice that your conda envirnment ( my_env here) is used in the database file name. Simmate does this so you can easily switch between databases just by switching your Anaconda environment. This is useful for testing and developing new workflows, which we will cover in a later tutorial. To find this file: remember from tutorial 1 that ~ is short for our home directory -- typically something like /home/jacksund/ or C:\\Users\\jacksund . have \"show hidden files\" turned on in your File Explorer (on Windows, check \"show file name extensions\" under the \"View\" tab). Then you'll see a file named my_env-database.sqlite3 instead of just my_env-database . You won't be able to double-click this file. Just like how you need Excel to open and read Excel (.xlsx) files, we need a separate program to read database (.sqlite3) files. We'll use Simmate to do this later on. But just after that one command, our database is setup any ready to use! We can now run workflows and start adding data to it.","title":"2. Finding our database file"},{"location":"getting_started/run_a_workflow/configure_potcars/","text":"Configuring Potentials (for VASP users) \u00b6 Warning once Simmate switches from VASP to a free DFT alternative, this section of the tutorial will be optional and removed. What is VASP? \u00b6 VASP is a very popular software for running DFT calculations, but our team can't install it for you because VASP is commercially licensed (i.e. you need to purchase it from their team , which we are not affiliated with). Simmate is working to switch to another DFT software -- specifically one that is free/open-source, that can be preinstalled for you, and that you can use on Windows+Mac+Linux. Until Simmate reaches this milestone, you'll have to use VASP. We apologize for the inconvenience. Configuring VASP \u00b6 While VASP can only be installed on Linux, we will still practice configuring VASP on our local computer -- even if it's Windows or Mac. To do this, you only need the Potentials that are distrubited with the VASP installation files. You can either... Grab these from the VASP installation files. You can find them at vasp/5.x.x/dist/Potentials . Be sure to unpack the tar.gz files. Ask a team member or your IT team for a copy of these files. Once you have the potentials, paste them into a folder named ~/simmate/vasp/Potentials . Note, this is same directory that your database is in ( ~/simmate ) where you need to make a new folder named vasp . This folder will have the potentials that came with VASP -- and with their original folder+file names. Once you have all of this done, you're folder should look like this: # Located at /home/my_username (~) simmate/ \u251c\u2500\u2500 my_env-database.sqlite3 \u2514\u2500\u2500 vasp \u2514\u2500\u2500 Potentials \u251c\u2500\u2500 LDA \u2502 \u251c\u2500\u2500 potpaw_LDA \u2502 \u251c\u2500\u2500 potpaw_LDA.52 \u2502 \u251c\u2500\u2500 potpaw_LDA.54 \u2502 \u2514\u2500\u2500 potUSPP_LDA \u251c\u2500\u2500 PBE \u2502 \u251c\u2500\u2500 potpaw_PBE \u2502 \u251c\u2500\u2500 potpaw_PBE.52 \u2502 \u2514\u2500\u2500 potpaw_PBE.54 \u2514\u2500\u2500 PW91 \u251c\u2500\u2500 potpaw_GGA \u2514\u2500\u2500 potUSPP_GGA Checking your configuration \u00b6 If you made this folder incorrectly, commands that you use later will fail with an error like... FileNotFoundError : [ Errno 2 ] No such file or directory : '/home/jacksund/simmate/vasp/Potentials/PBE/potpaw_PBE.54/Na/POTCAR' If you see this error, double-check your folder setup. Danger our team only has access to VASP v5.4.4, so if your folder structure differs for newer versions of VASP, please let our know by opening an issue .","title":"Configure VASP potentials"},{"location":"getting_started/run_a_workflow/configure_potcars/#configuring-potentials-for-vasp-users","text":"Warning once Simmate switches from VASP to a free DFT alternative, this section of the tutorial will be optional and removed.","title":"Configuring Potentials (for VASP users)"},{"location":"getting_started/run_a_workflow/configure_potcars/#what-is-vasp","text":"VASP is a very popular software for running DFT calculations, but our team can't install it for you because VASP is commercially licensed (i.e. you need to purchase it from their team , which we are not affiliated with). Simmate is working to switch to another DFT software -- specifically one that is free/open-source, that can be preinstalled for you, and that you can use on Windows+Mac+Linux. Until Simmate reaches this milestone, you'll have to use VASP. We apologize for the inconvenience.","title":"What is VASP?"},{"location":"getting_started/run_a_workflow/configure_potcars/#configuring-vasp","text":"While VASP can only be installed on Linux, we will still practice configuring VASP on our local computer -- even if it's Windows or Mac. To do this, you only need the Potentials that are distrubited with the VASP installation files. You can either... Grab these from the VASP installation files. You can find them at vasp/5.x.x/dist/Potentials . Be sure to unpack the tar.gz files. Ask a team member or your IT team for a copy of these files. Once you have the potentials, paste them into a folder named ~/simmate/vasp/Potentials . Note, this is same directory that your database is in ( ~/simmate ) where you need to make a new folder named vasp . This folder will have the potentials that came with VASP -- and with their original folder+file names. Once you have all of this done, you're folder should look like this: # Located at /home/my_username (~) simmate/ \u251c\u2500\u2500 my_env-database.sqlite3 \u2514\u2500\u2500 vasp \u2514\u2500\u2500 Potentials \u251c\u2500\u2500 LDA \u2502 \u251c\u2500\u2500 potpaw_LDA \u2502 \u251c\u2500\u2500 potpaw_LDA.52 \u2502 \u251c\u2500\u2500 potpaw_LDA.54 \u2502 \u2514\u2500\u2500 potUSPP_LDA \u251c\u2500\u2500 PBE \u2502 \u251c\u2500\u2500 potpaw_PBE \u2502 \u251c\u2500\u2500 potpaw_PBE.52 \u2502 \u2514\u2500\u2500 potpaw_PBE.54 \u2514\u2500\u2500 PW91 \u251c\u2500\u2500 potpaw_GGA \u2514\u2500\u2500 potUSPP_GGA","title":"Configuring VASP"},{"location":"getting_started/run_a_workflow/configure_potcars/#checking-your-configuration","text":"If you made this folder incorrectly, commands that you use later will fail with an error like... FileNotFoundError : [ Errno 2 ] No such file or directory : '/home/jacksund/simmate/vasp/Potentials/PBE/potpaw_PBE.54/Na/POTCAR' If you see this error, double-check your folder setup. Danger our team only has access to VASP v5.4.4, so if your folder structure differs for newer versions of VASP, please let our know by opening an issue .","title":"Checking your configuration"},{"location":"getting_started/run_a_workflow/make_a_structure/","text":"Making a structure file \u00b6 Before we run a workflow, we need a crystal structure to run it on. There are many ways to get a crystal structure -- such as downloading one online or using a program to create one from scratch. Here, in order to learn about structure files, we are going to make one from scratch without any program. Making a text (txt) file \u00b6 First, make a new text file on your Desktop named POSCAR.txt . You can use which text editor you prefer (Notepad, Sublime, etc.). You can also create the file using the command like with: nano POSCAR.txt Note, we can see the .txt ending because we enabled \"show file name extensions\" above. Once you have this file, copy/paste this text into it: Na1 Cl1 1.0 3.485437 0.000000 2.012318 1.161812 3.286101 2.012318 0.000000 0.000000 4.024635 Na Cl 1 1 direct 0.000000 0.000000 0.000000 Na 0.500000 0.500000 0.500000 Cl This text is everything we need to represent a structure, which is just a lattice and a list of atomic sites. The lattice is defined by a 3x3 matrix (lines 3-5) and the sites are just a list of xyz coordinates with an element (lines 8-9 show fractional coordinates). Trying a different format (cif) \u00b6 There are many different ways to write structure information; above we are using the VASP's \"POSCAR\" format. Another popular format is CIF. It's not as clean and tidy as a POSCAR, but it holds similar information: data_NaCl _symmetry_space_group_name_H-M 'P 1' _cell_length_a 4.02463542 _cell_length_b 4.02463542 _cell_length_c 4.02463542 _cell_angle_alpha 60.00000000 _cell_angle_beta 60.00000000 _cell_angle_gamma 60.00000000 _symmetry_Int_Tables_number 1 _chemical_formula_structural NaCl _chemical_formula_sum 'Na1 Cl1' _cell_volume 46.09614833 _cell_formula_units_Z 1 loop_ _symmetry_equiv_pos_site_id _symmetry_equiv_pos_as_xyz 1 'x, y, z' loop_ _atom_site_type_symbol _atom_site_label _atom_site_symmetry_multiplicity _atom_site_fract_x _atom_site_fract_y _atom_site_fract_z _atom_site_occupancy Na Na0 1 0.00000000 0.00000000 0.00000000 1 Cl Cl1 1 0.50000000 0.50000000 0.50000000 1 You can use either CIFs or POSCAR formats when using Simmate. It's up to you. Understanding file extensions \u00b6 Nearly all files that you will interact with are text files -- just in different formats. That's where file extensions come in ( .txt , .cif , .csv , ...). These ending indicate what format we are using. Files named something.cif just tell programs we have a text file written in the CIF structure format. VASP uses the name POSCAR (without any file extension) to show its format. If we renamed our file from POSCAR.txt to POSCAR , and now all programs (VESTA, OVITO, and others) will know what to do with your structure. Note In Windows, you will often receive a warning about changing the file extension. Ignore the warning and change the extension. Fun-fact a Microsoft Word document is just a folder of text files. The .docx file ending tells Word that we have the folder in their desired format. Try renaming a word file from my_file.docx to my_file.zip and open it up to explore. Nearly all programs do something like this! Rename your file in the command-line \u00b6 If you're using the command-line to create/edit this file, you can use the copy ( cp ) command to move your POSCAR.txt file into a file named POSCAR : cp POSCAR.txt POSCAR We now have our structure ready to go!","title":"Make a input structure"},{"location":"getting_started/run_a_workflow/make_a_structure/#making-a-structure-file","text":"Before we run a workflow, we need a crystal structure to run it on. There are many ways to get a crystal structure -- such as downloading one online or using a program to create one from scratch. Here, in order to learn about structure files, we are going to make one from scratch without any program.","title":"Making a structure file"},{"location":"getting_started/run_a_workflow/make_a_structure/#making-a-text-txt-file","text":"First, make a new text file on your Desktop named POSCAR.txt . You can use which text editor you prefer (Notepad, Sublime, etc.). You can also create the file using the command like with: nano POSCAR.txt Note, we can see the .txt ending because we enabled \"show file name extensions\" above. Once you have this file, copy/paste this text into it: Na1 Cl1 1.0 3.485437 0.000000 2.012318 1.161812 3.286101 2.012318 0.000000 0.000000 4.024635 Na Cl 1 1 direct 0.000000 0.000000 0.000000 Na 0.500000 0.500000 0.500000 Cl This text is everything we need to represent a structure, which is just a lattice and a list of atomic sites. The lattice is defined by a 3x3 matrix (lines 3-5) and the sites are just a list of xyz coordinates with an element (lines 8-9 show fractional coordinates).","title":"Making a text (txt) file"},{"location":"getting_started/run_a_workflow/make_a_structure/#trying-a-different-format-cif","text":"There are many different ways to write structure information; above we are using the VASP's \"POSCAR\" format. Another popular format is CIF. It's not as clean and tidy as a POSCAR, but it holds similar information: data_NaCl _symmetry_space_group_name_H-M 'P 1' _cell_length_a 4.02463542 _cell_length_b 4.02463542 _cell_length_c 4.02463542 _cell_angle_alpha 60.00000000 _cell_angle_beta 60.00000000 _cell_angle_gamma 60.00000000 _symmetry_Int_Tables_number 1 _chemical_formula_structural NaCl _chemical_formula_sum 'Na1 Cl1' _cell_volume 46.09614833 _cell_formula_units_Z 1 loop_ _symmetry_equiv_pos_site_id _symmetry_equiv_pos_as_xyz 1 'x, y, z' loop_ _atom_site_type_symbol _atom_site_label _atom_site_symmetry_multiplicity _atom_site_fract_x _atom_site_fract_y _atom_site_fract_z _atom_site_occupancy Na Na0 1 0.00000000 0.00000000 0.00000000 1 Cl Cl1 1 0.50000000 0.50000000 0.50000000 1 You can use either CIFs or POSCAR formats when using Simmate. It's up to you.","title":"Trying a different format (cif)"},{"location":"getting_started/run_a_workflow/make_a_structure/#understanding-file-extensions","text":"Nearly all files that you will interact with are text files -- just in different formats. That's where file extensions come in ( .txt , .cif , .csv , ...). These ending indicate what format we are using. Files named something.cif just tell programs we have a text file written in the CIF structure format. VASP uses the name POSCAR (without any file extension) to show its format. If we renamed our file from POSCAR.txt to POSCAR , and now all programs (VESTA, OVITO, and others) will know what to do with your structure. Note In Windows, you will often receive a warning about changing the file extension. Ignore the warning and change the extension. Fun-fact a Microsoft Word document is just a folder of text files. The .docx file ending tells Word that we have the folder in their desired format. Try renaming a word file from my_file.docx to my_file.zip and open it up to explore. Nearly all programs do something like this!","title":"Understanding file extensions"},{"location":"getting_started/run_a_workflow/make_a_structure/#rename-your-file-in-the-command-line","text":"If you're using the command-line to create/edit this file, you can use the copy ( cp ) command to move your POSCAR.txt file into a file named POSCAR : cp POSCAR.txt POSCAR We now have our structure ready to go!","title":"Rename your file in the command-line"},{"location":"getting_started/run_a_workflow/quick_start/","text":"Run a workflow \u00b6 In this tutorial, you will use the command line to view all available workflows and their settings. Beginners will also be introduced to remote terminals (SSH) and jobs queue (such as SLURM). The quick tutorial \u00b6 Danger we assume you have VASP installed and that the vasp_std command is in the available path. In the future, we hope to update this tutorial with a workflow that doesn't require VASP or remote Linux cluster. Until then, we apologize for the inconvenience. Before running a workflow, we must initialize our Simmate database with simmate database reset . Your database will be built at ~/simmate/my_env-database.sqlite3 , where \"my_env\" is the name of your active conda environment. To practice calculating, make structure file for tablesalt (NaCl). Name it POSCAR , where the contents are... Na1 Cl1 1.0 3.485437 0.000000 2.012318 1.161812 3.286101 2.012318 0.000000 0.000000 4.024635 Na Cl 1 1 direct 0.000000 0.000000 0.000000 Na 0.500000 0.500000 0.500000 Cl View a list of all workflows available with simmate workflows list-all Interactively learn about all workflows with simmate workflows explore View the settings used for the static-energy.vasp.mit workflow with simmate workflows show-config static-energy.vasp.mit Copy and paste VASP POTCAR files to the folder ~/simmate/vasp/Potentials . Be sure to unpack the tar.gz files. This folder will have the potentials that came with VASP -- and with their original folder+file names: # Located at /home/my_username (~) simmate/ \u2514\u2500\u2500 vasp \u2514\u2500\u2500 Potentials \u251c\u2500\u2500 LDA \u2502 \u251c\u2500\u2500 potpaw_LDA \u2502 \u251c\u2500\u2500 potpaw_LDA.52 \u2502 \u251c\u2500\u2500 potpaw_LDA.54 \u2502 \u2514\u2500\u2500 potUSPP_LDA \u251c\u2500\u2500 PBE \u2502 \u251c\u2500\u2500 potpaw_PBE \u2502 \u251c\u2500\u2500 potpaw_PBE.52 \u2502 \u2514\u2500\u2500 potpaw_PBE.54 \u2514\u2500\u2500 PW91 \u251c\u2500\u2500 potpaw_GGA \u2514\u2500\u2500 potUSPP_GGA With everything configured, there are now two ways you can submit your workflow using the command-line. This can be done in the CLI or in python. Here, let's use a settings file in yaml format: # In a file named \"my_example.yaml\". # Note, different workflows accept different settings here. workflow_name : static-energy.vasp.mit structure : POSCAR command : mpirun -n 5 vasp_std > vasp.out # OPTIONAL directory : my_new_folder # OPTIONAL Now run the workflow configuration file we just made simmate workflows run my_example.yaml Once the workflow completes, you will see files named simmate_metadata.yaml and simmate_summary.yaml which contains some quick information for you. Other workflows (such as band-structure calculations) will also write out plots for you. While the plots and summary files are nice for quick testing, much more useful information is stored in our database. We will cover how to access your database in a following tutorial. Tip This tutorial only convers the CLI and YAML input files, but submitting workflows through python or TOML files is covered in the full guides Tip Want to customize a specific setting (e.g. set ENCUT to a custom value)? Customizing workflow settings is covered in the \"Build Custom Workflows\" tutorial. However, try to resist jumping ahead! There are still several important steps to learn before customizing workflows.","title":"Quickstart"},{"location":"getting_started/run_a_workflow/quick_start/#run-a-workflow","text":"In this tutorial, you will use the command line to view all available workflows and their settings. Beginners will also be introduced to remote terminals (SSH) and jobs queue (such as SLURM).","title":"Run a workflow"},{"location":"getting_started/run_a_workflow/quick_start/#the-quick-tutorial","text":"Danger we assume you have VASP installed and that the vasp_std command is in the available path. In the future, we hope to update this tutorial with a workflow that doesn't require VASP or remote Linux cluster. Until then, we apologize for the inconvenience. Before running a workflow, we must initialize our Simmate database with simmate database reset . Your database will be built at ~/simmate/my_env-database.sqlite3 , where \"my_env\" is the name of your active conda environment. To practice calculating, make structure file for tablesalt (NaCl). Name it POSCAR , where the contents are... Na1 Cl1 1.0 3.485437 0.000000 2.012318 1.161812 3.286101 2.012318 0.000000 0.000000 4.024635 Na Cl 1 1 direct 0.000000 0.000000 0.000000 Na 0.500000 0.500000 0.500000 Cl View a list of all workflows available with simmate workflows list-all Interactively learn about all workflows with simmate workflows explore View the settings used for the static-energy.vasp.mit workflow with simmate workflows show-config static-energy.vasp.mit Copy and paste VASP POTCAR files to the folder ~/simmate/vasp/Potentials . Be sure to unpack the tar.gz files. This folder will have the potentials that came with VASP -- and with their original folder+file names: # Located at /home/my_username (~) simmate/ \u2514\u2500\u2500 vasp \u2514\u2500\u2500 Potentials \u251c\u2500\u2500 LDA \u2502 \u251c\u2500\u2500 potpaw_LDA \u2502 \u251c\u2500\u2500 potpaw_LDA.52 \u2502 \u251c\u2500\u2500 potpaw_LDA.54 \u2502 \u2514\u2500\u2500 potUSPP_LDA \u251c\u2500\u2500 PBE \u2502 \u251c\u2500\u2500 potpaw_PBE \u2502 \u251c\u2500\u2500 potpaw_PBE.52 \u2502 \u2514\u2500\u2500 potpaw_PBE.54 \u2514\u2500\u2500 PW91 \u251c\u2500\u2500 potpaw_GGA \u2514\u2500\u2500 potUSPP_GGA With everything configured, there are now two ways you can submit your workflow using the command-line. This can be done in the CLI or in python. Here, let's use a settings file in yaml format: # In a file named \"my_example.yaml\". # Note, different workflows accept different settings here. workflow_name : static-energy.vasp.mit structure : POSCAR command : mpirun -n 5 vasp_std > vasp.out # OPTIONAL directory : my_new_folder # OPTIONAL Now run the workflow configuration file we just made simmate workflows run my_example.yaml Once the workflow completes, you will see files named simmate_metadata.yaml and simmate_summary.yaml which contains some quick information for you. Other workflows (such as band-structure calculations) will also write out plots for you. While the plots and summary files are nice for quick testing, much more useful information is stored in our database. We will cover how to access your database in a following tutorial. Tip This tutorial only convers the CLI and YAML input files, but submitting workflows through python or TOML files is covered in the full guides Tip Want to customize a specific setting (e.g. set ENCUT to a custom value)? Customizing workflow settings is covered in the \"Build Custom Workflows\" tutorial. However, try to resist jumping ahead! There are still several important steps to learn before customizing workflows.","title":"The quick tutorial"},{"location":"getting_started/run_a_workflow/running_the_workflow/","text":"Finally running our workflow! \u00b6 Warning Unless you have VASP installed on your local computer, these next commands will fail. That is okay! Let's go ahead and try running these commands anyways. It will be helpful to see how Simmate workflows fail when VASP is not configured properly. If you don't have VASP installed, you'll see an error stating that the vasp_std command isn't known (such as vasp_std: not found on Linux). We'll switch to a remote computer with VASP installed in the next section. Run a workflow with the command-line \u00b6 The default Simmate settings will run everything immediately and locally on your desktop. When running the workflow, it will create a new folder, write the inputs in it, run the calculation, and save the results to your database. The command to do this with our POSCAR and static-energy/mit workflow is... simmate workflows run-quick static-energy.vasp.mit --structure POSCAR Tip You'll notice the commands in this section are long a pain to write out. Plus even more difficult to remember. Don't worry, this will go away once we learn how to submit using YAML files in the next section Tip We call this command run-quick becuase it is (typically) only ever used in quick testing by advanced users. 99% of the time, you'll be using the run command, which we will cover below By default, Simmate uses the command vasp_std > vasp.out and creates a new simmate-task folder with a unique identifier (ex: simmate-task-j8djk3mn8 ). What if we wanted to change this command or the directory it's ran in? Recall the output from the simmate workflows explore command, which listed parameters for us. We can use any of these to update how our worklfow runs. For example, we can change our folder name ( --directory ) as well as the command used to run VASP ( --command ). Using these, we can update our command to this: simmate workflows run-quick static-energy.vasp.mit --structure POSCAR --command \"mpirun -n 4 vasp_std > vasp.out\" --directory my_custom_folder If any errors come up, please let our team know by posting a question . If not, congrats !!! You now know how to run workflows with a single command and understand what Simmate is doing behind the scenes. Run a workflow with a settings file \u00b6 In the last section, you probably noticed that our simmate workflows run command was getting extremely long and will therefore be difficult to remember. Instead of writing out this long command every time, we can make a settings file that contains all of this information. Here, we will write our settings into a YAML file, which is just a simple text file. The name of our settings file doesn't matter, so here we'll just use my_settings.yaml . To create this file, complete the following: nano my_settings.yaml ... and write in the following information ... workflow_name : static-energy.vasp.mit structure : POSCAR command : mpirun -n 4 vasp_std > vasp.out # OPTIONAL directory : my_custom_folder # OPTIONAL Note that this file contains all of the information that was in our simmate workflows run-quick command from above. But now we have it stored in a file that we can read/edit later on if we need to. To submit this file, we simply run... simmate workflows run my_settings.yaml And your workflow will run the same as before. Note, it is entirely up to you whether workflows are ran submit using a yaml file or using the longer command. Tip recall from earlier how the command simmate workflows explore printed out all of the parameters for us to use. These are all of your options when sumbitting the workflow. In the example command above, we decided to set two of the optional parameters Tip Want to customize a specific setting (e.g. set ENCUT to a custom value)? Customizing workflow settings is covered in tutorial 6. However, try to resist jumping ahead! There are still several important steps to learn before customizing workflows. Mastering workflow options \u00b6 Above, we gave our input structure as a POSCAR -- but what if we wanted to use a different format? Or even use a structure from a previous calculation or the Materials Project database? Recall from earlier, we can look at the Parameters section of our documentation. When we scroll down to structure , we see that we can do... cif or poscar files reference a database entry point to a third-party database use advanced python objects For example, you can try running the following workflow: workflow_name : static-energy.vasp.mit structure : database_table : MatprojStructure database_id : mp-123 command : mpirun -n 4 vasp_std > vasp.out # OPTIONAL directory : my_custom_folder # OPTIONAL Even though we didn't build a structure file, Simmate grabbed one for us from the Materials Project database.","title":"Run a workflow"},{"location":"getting_started/run_a_workflow/running_the_workflow/#finally-running-our-workflow","text":"Warning Unless you have VASP installed on your local computer, these next commands will fail. That is okay! Let's go ahead and try running these commands anyways. It will be helpful to see how Simmate workflows fail when VASP is not configured properly. If you don't have VASP installed, you'll see an error stating that the vasp_std command isn't known (such as vasp_std: not found on Linux). We'll switch to a remote computer with VASP installed in the next section.","title":"Finally running our workflow!"},{"location":"getting_started/run_a_workflow/running_the_workflow/#run-a-workflow-with-the-command-line","text":"The default Simmate settings will run everything immediately and locally on your desktop. When running the workflow, it will create a new folder, write the inputs in it, run the calculation, and save the results to your database. The command to do this with our POSCAR and static-energy/mit workflow is... simmate workflows run-quick static-energy.vasp.mit --structure POSCAR Tip You'll notice the commands in this section are long a pain to write out. Plus even more difficult to remember. Don't worry, this will go away once we learn how to submit using YAML files in the next section Tip We call this command run-quick becuase it is (typically) only ever used in quick testing by advanced users. 99% of the time, you'll be using the run command, which we will cover below By default, Simmate uses the command vasp_std > vasp.out and creates a new simmate-task folder with a unique identifier (ex: simmate-task-j8djk3mn8 ). What if we wanted to change this command or the directory it's ran in? Recall the output from the simmate workflows explore command, which listed parameters for us. We can use any of these to update how our worklfow runs. For example, we can change our folder name ( --directory ) as well as the command used to run VASP ( --command ). Using these, we can update our command to this: simmate workflows run-quick static-energy.vasp.mit --structure POSCAR --command \"mpirun -n 4 vasp_std > vasp.out\" --directory my_custom_folder If any errors come up, please let our team know by posting a question . If not, congrats !!! You now know how to run workflows with a single command and understand what Simmate is doing behind the scenes.","title":"Run a workflow with the command-line"},{"location":"getting_started/run_a_workflow/running_the_workflow/#run-a-workflow-with-a-settings-file","text":"In the last section, you probably noticed that our simmate workflows run command was getting extremely long and will therefore be difficult to remember. Instead of writing out this long command every time, we can make a settings file that contains all of this information. Here, we will write our settings into a YAML file, which is just a simple text file. The name of our settings file doesn't matter, so here we'll just use my_settings.yaml . To create this file, complete the following: nano my_settings.yaml ... and write in the following information ... workflow_name : static-energy.vasp.mit structure : POSCAR command : mpirun -n 4 vasp_std > vasp.out # OPTIONAL directory : my_custom_folder # OPTIONAL Note that this file contains all of the information that was in our simmate workflows run-quick command from above. But now we have it stored in a file that we can read/edit later on if we need to. To submit this file, we simply run... simmate workflows run my_settings.yaml And your workflow will run the same as before. Note, it is entirely up to you whether workflows are ran submit using a yaml file or using the longer command. Tip recall from earlier how the command simmate workflows explore printed out all of the parameters for us to use. These are all of your options when sumbitting the workflow. In the example command above, we decided to set two of the optional parameters Tip Want to customize a specific setting (e.g. set ENCUT to a custom value)? Customizing workflow settings is covered in tutorial 6. However, try to resist jumping ahead! There are still several important steps to learn before customizing workflows.","title":"Run a workflow with a settings file"},{"location":"getting_started/run_a_workflow/running_the_workflow/#mastering-workflow-options","text":"Above, we gave our input structure as a POSCAR -- but what if we wanted to use a different format? Or even use a structure from a previous calculation or the Materials Project database? Recall from earlier, we can look at the Parameters section of our documentation. When we scroll down to structure , we see that we can do... cif or poscar files reference a database entry point to a third-party database use advanced python objects For example, you can try running the following workflow: workflow_name : static-energy.vasp.mit structure : database_table : MatprojStructure database_id : mp-123 command : mpirun -n 4 vasp_std > vasp.out # OPTIONAL directory : my_custom_folder # OPTIONAL Even though we didn't build a structure file, Simmate grabbed one for us from the Materials Project database.","title":"Mastering workflow options"},{"location":"getting_started/run_a_workflow/stages_of_a_workflow/","text":"The 4 stages of a workflow \u00b6 Note For the full tutorial, you will start on your local computer, even if you don't have VASP installed. By the end of the tutorial, you will have switched to a computer with VASP (likely a remote university or federal supercomputer). the 4 key stages \u00b6 Before running any workflows, it is important to understand what's happening behind the scenes. All workflows carry out four steps: configure : chooses our desired settings for the calculation (such as VASP's INCAR settings) schedule : decides whether to run the workflow immediately or send off to a job queue (e.g. SLURM, PBS, or remote computers) execute : writes our input files, runs the calculation (e.g. VASP), and checks the results for errors save : saves the results to our database changing each stage \u00b6 There are many different scenarios where we may want to change the behavior of these steps. For example, what if I want to execute on a remote computer instead of my local one? Or if I want to save results to a cloud database that my entire lab shares? These can be configured easily, but because they require extra setup, we will save them for a later tutorial. simmate defaults \u00b6 For now, we just want to run a workflow using Simmate's default settings. Without setting anything up, here is what Simmate will do: configure : take the default settings from the workflow you request schedule : decides that we want to run the workflow immediately execute : runs the calculation directly on our local computer save : saves the results on our local computer check-list before running workflows \u00b6 Before we can actually run a workflow, we must: tell Simmate where our VASP files are set up our database so results can be saved select a structure for our calculation The next three sections will address each of these requirements.","title":"Stages of a Workflow"},{"location":"getting_started/run_a_workflow/stages_of_a_workflow/#the-4-stages-of-a-workflow","text":"Note For the full tutorial, you will start on your local computer, even if you don't have VASP installed. By the end of the tutorial, you will have switched to a computer with VASP (likely a remote university or federal supercomputer).","title":"The 4 stages of a workflow"},{"location":"getting_started/run_a_workflow/stages_of_a_workflow/#the-4-key-stages","text":"Before running any workflows, it is important to understand what's happening behind the scenes. All workflows carry out four steps: configure : chooses our desired settings for the calculation (such as VASP's INCAR settings) schedule : decides whether to run the workflow immediately or send off to a job queue (e.g. SLURM, PBS, or remote computers) execute : writes our input files, runs the calculation (e.g. VASP), and checks the results for errors save : saves the results to our database","title":"the 4 key stages"},{"location":"getting_started/run_a_workflow/stages_of_a_workflow/#changing-each-stage","text":"There are many different scenarios where we may want to change the behavior of these steps. For example, what if I want to execute on a remote computer instead of my local one? Or if I want to save results to a cloud database that my entire lab shares? These can be configured easily, but because they require extra setup, we will save them for a later tutorial.","title":"changing each stage"},{"location":"getting_started/run_a_workflow/stages_of_a_workflow/#simmate-defaults","text":"For now, we just want to run a workflow using Simmate's default settings. Without setting anything up, here is what Simmate will do: configure : take the default settings from the workflow you request schedule : decides that we want to run the workflow immediately execute : runs the calculation directly on our local computer save : saves the results on our local computer","title":"simmate defaults"},{"location":"getting_started/run_a_workflow/stages_of_a_workflow/#check-list-before-running-workflows","text":"Before we can actually run a workflow, we must: tell Simmate where our VASP files are set up our database so results can be saved select a structure for our calculation The next three sections will address each of these requirements.","title":"check-list before running workflows"},{"location":"getting_started/run_a_workflow/submit_to_a_cluster/","text":"Switching to a remote cluster \u00b6 Warning This section can be extremely difficult for beginners. If you can, try to sit down with an experienced user or someone from your IT department as you work through it. Don't get discouraged if this section takes your more than an hour -- it's a lot to learn! Thus far, you've been running Simmate on your local desktop or laptop, but we saw in the previous section, that we actually need VASP (which needs to be on Linux) for Simmate's workflows to run. 99% of the time, you'll be using a University or Federal supercomputer (aka \"high performance computing (HPC) clusters\"), which will have VASP already installed. Cluster-specific guides \u00b6 For teams that are actively using Simmate, we have extra notes and examples below on submitting to that particular cluster. This includes: WarWulf : The Warren lab's \"BeoWulf\" cluster at UNC Chapel Hill LongLeaf : UNC's university cluster most use-cases (1 node limit) DogWood : UNC's university cluster built for massively parallel jobs (>1 node) Tip If your cluster/university is not listed, contact your IT team for help in completing this tutorial. A check-list for clusters \u00b6 For workflows to run correctly, the following requirements need to be met: a VASP license for your team ( purchased on their site ) a remote cluster that you have a profile with (e.g. UNC's LongLeaf ) VASP installed on the remote cluster Anaconda installed on the remote cluster Make sure you have these steps completed before starting below Tip For the Warren Lab, these items are configured for you already on WarWulf , LongLeaf , and DogWood . 1. Sign in to the cluster \u00b6 If you've never signed into a remote cluster before, we will do this by using SSH (Secure Shell). Run the following command in your local terminal: example WarWulf LongLeaf DogWood ssh my_username@my_cluster.edu ssh WarrenLab@warwulf.net Note everyone shares the profile \"WarrenLab\". Ask Scott for the password (scw@email.unc.edu) ssh my_onyen@longleaf.unc.edu ssh my_username@my_cluster.edu Danger on windows, use your Command-prompt -- not the Anaconda Powershell Prompt After entering your password, you are now using a terminal on the remote supercomputer. Try running the command pwd (\"print working directory\") to show that your terminal is indeed running commands on the remote cluster, not your desktop: # This is the same for all linux clusters pwd 2. Load VASP \u00b6 To load VASP into your environment, you typically need to run a 'load module' command: example WarWulf LongLeaf DogWood module load vasp module load vasp ; source /opt/ohpc/pub/intel/bin/ifortvars.sh ; module load vasp/5.4.4 module load vasp/5.4.4 Then check that the vasp command is found. If the vasp_std command worked correctly, you will see the following output (bc their command doesn't print help information like simmate or conda ): vasp_std # Error output may vary between different VASP versions Error reading item 'VCAIMAGES' from file INCAR. 3. Build your personal Simmate env \u00b6 Next we need to ensure Simmate is installed. If you see (base) at the start of your command-line, Anaconda is already installed. If not, ask your IT team how they want you install it. Typically it's by using miniconda which is just anaconda without the graphical user interface). With Anaconda set up, you can create your environment and install Simmate just like we did in the first tutorial: # Create your conda env with... conda create -n my_env -c conda-forge python = 3 .10 simmate conda activate my_env # Initialize your database on this new installation. simmate database reset Danger On WarWulf, we share a profile so make sure you name your environment something unique. For example, use yourname_env (e.g. jacks_env ). 4. Set up VASP potentials \u00b6 Note This step is already completed for you on the WarWulf cluster Next, copy your Potentials into ~/simmate/vasp/Potentials and also copy the POSCAR file above onto your cluster. It can be diffult in the command line to move files around or even transfer them back and forth from your local computer to the supercomputer. It's much easier with a program like FileZilla , MobaXTerm , or another file transfer program. We recommend FileZilla, but it's entirely optional and up to you. Review our POTCAR guide from before if you need help on this step. 5. Move to your 'scratch' directory \u00b6 Typically, clusters will have a \"scratch\" directory that you should submit jobs from -- which is different from your home directory. Make sure you switch to that before submitting and workflows. (note, your POSCAR and all input files should be in this directory too): example WarWulf LongLeaf DogWood cd /path/to/my/scratch/space/ cd /media/synology/user/your_name cd /pine/scr/j/a/jacksund cd /21dayscratch/scr/y/o/youronyen 6. Build our input files \u00b6 Just like we did on our laptop, we need to make our input files. For now, let's use this sample YAML file: workflow_name : static-energy.vasp.mit structure : database_table : MatprojStructure database_id : mp-22862 command : mpirun -n 4 vasp_std > vasp.out # OPTIONAL directory : my_custom_folder # OPTIONAL Put this in a file named my_settings.yaml in your scratch directory. Danger Take note of the -n 4 in our command. This is the number of cores that we want our calculation to use. Make sure this number matches your cpus-per-task setting in the next section 7. Build our submit script \u00b6 Earlier in this tutorial, we called simmate workflows run ... directly in our terminal, but this should NEVER be done on a supercomputer. Instead we should submit the workflow to the cluster's job queue. Typically, supercomputers use SLURM or PBS to submit jobs. For example, UNC's WarWulf , LongLeaf , and DogWood clusters each use SLURM . To submit, we would make a file named submit.sh : nano submit.sh ... and use contents likes ... example WarWulf LongLeaf DogWood #! /bin/sh #SBATCH --job-name=my_example_job #SBATCH --nodes=1 #SBATCH --ntasks=1 #SBATCH --cpus-per-task=4 #SBATCH --mem=4GB #SBATCH --time=01:00:00 #SBATCH --partition=general #SBATCH --output=slurm.out #SBATCH --mail-type=ALL #SBATCH --mail-user=my_username@live.unc.edu simmate workflows run my_settings.yaml #!/bin/bash #. /opt/ohpc/pub/suppress.sh #supress infiniband output, set vasp path #SBATCH --job-name=my_example_job #SBATCH --nodes=1 #SBATCH --ntasks=1 #SBATCH --cpus-per-task=4 #SBATCH --mem=4GB #SBATCH --time=01:00:00 #SBATCH --partition=p1 #SBATCH --output=slurm.out #SBATCH --mail-type=ALL #SBATCH --mail-user=my_username@live.unc.edu simmate workflows run my_settings.yaml #! /bin/sh #SBATCH --job-name=my_example_job #SBATCH --nodes=20 #SBATCH --ntasks=1 #SBATCH --mem=40g #SBATCH --partition=general #SBATCH --output=slurm.out #SBATCH --mail-type=FAIL #SBATCH --mail-user=youronyen@live.unc.edu #SBATCH --time=11-00:00 simmate workflows run my_settings.yaml Danger Note the massive ntasks and node values here. DogWood is only meant for large calculations, so talk with our team before submitting. #!/bin/sh #SBATCH --job-name=NEB #SBATCH --ntasks=704 #SBATCH --nodes=16 #SBATCH --time=2-00:00 #SBATCH --mem=300g #SBATCH --partition=2112_queue #SBATCH --mail-type=ALL #SBATCH --mail-user=lamcrae@live.unc.edu simmate workflows run my_settings.yaml Info Each of these SBATCH parameters set how we would like to sumbit a job and how many resources we expect to use. These are explained in SLURM's documnetation for sbatch , but you may need help from your IT team to update them. But to break down these example parameters... job-name : the name that identifies your job. It will be visible when you check the status of your job nodes : the number of server nodes (or CPUs) that you request. Typically leave this at 1. ntasks : the number tasks that you'll be running. We run one workflow at a time here, so we use 1. cpus-per-task : the number of CPU tasks required for each run. We run our workflow using 4 cores ( mpirun -n 4 ) so we need to request 4 cores for it here mem : the memory requested for this job. If it is exceeded, the job will be terminated. time : the maximum time requested for this job. If it is exceeded, the job will be terminated. partition : the group of nodes that we request resources on. You can often remove this line and use the cluster's default. output : the name of the file to write the job output (including errors) mail-type + mail-user : will send an email alerts when a jobs starts/stops/fails/etc. 8. Double check everything \u00b6 Let's go back through our check list before we submit loaded the VASP module activated your conda environment in the temporary working directory have our yaml file (+ extra inputs like a POSCAR) in the directory have our submit.sh in the directory structure file (e.g. POSCAR ) is present in working directory If all of these are set, you're good to go. 9. Submit a workflow to the queue \u00b6 Finally, let's submit to our cluster! sbatch submit.sh 10. Monitor its progress \u00b6 You can then monitor your jobs progress with: example WarWulf LongLeaf DogWood squeue -u my_username sq # or sq | grep my_name Example sq | grep jack squeue -u my_onyen squeue -u my_onyen Success! \u00b6 You've now submitted a Simmate workflow to a remote cluster !!! Tip Be sure to go back through this section a few times before moving on. Submitting remote jobs can be tedious but it's important to understand. Advanced features of Simmate will let you skip a lot of this work down the road, but that won't happen until we reaching the \"Adding Computational Resources\" guide.","title":"Submit to a cluster"},{"location":"getting_started/run_a_workflow/submit_to_a_cluster/#switching-to-a-remote-cluster","text":"Warning This section can be extremely difficult for beginners. If you can, try to sit down with an experienced user or someone from your IT department as you work through it. Don't get discouraged if this section takes your more than an hour -- it's a lot to learn! Thus far, you've been running Simmate on your local desktop or laptop, but we saw in the previous section, that we actually need VASP (which needs to be on Linux) for Simmate's workflows to run. 99% of the time, you'll be using a University or Federal supercomputer (aka \"high performance computing (HPC) clusters\"), which will have VASP already installed.","title":"Switching to a remote cluster"},{"location":"getting_started/run_a_workflow/submit_to_a_cluster/#cluster-specific-guides","text":"For teams that are actively using Simmate, we have extra notes and examples below on submitting to that particular cluster. This includes: WarWulf : The Warren lab's \"BeoWulf\" cluster at UNC Chapel Hill LongLeaf : UNC's university cluster most use-cases (1 node limit) DogWood : UNC's university cluster built for massively parallel jobs (>1 node) Tip If your cluster/university is not listed, contact your IT team for help in completing this tutorial.","title":"Cluster-specific guides"},{"location":"getting_started/run_a_workflow/submit_to_a_cluster/#a-check-list-for-clusters","text":"For workflows to run correctly, the following requirements need to be met: a VASP license for your team ( purchased on their site ) a remote cluster that you have a profile with (e.g. UNC's LongLeaf ) VASP installed on the remote cluster Anaconda installed on the remote cluster Make sure you have these steps completed before starting below Tip For the Warren Lab, these items are configured for you already on WarWulf , LongLeaf , and DogWood .","title":"A check-list for clusters"},{"location":"getting_started/run_a_workflow/submit_to_a_cluster/#1-sign-in-to-the-cluster","text":"If you've never signed into a remote cluster before, we will do this by using SSH (Secure Shell). Run the following command in your local terminal: example WarWulf LongLeaf DogWood ssh my_username@my_cluster.edu ssh WarrenLab@warwulf.net Note everyone shares the profile \"WarrenLab\". Ask Scott for the password (scw@email.unc.edu) ssh my_onyen@longleaf.unc.edu ssh my_username@my_cluster.edu Danger on windows, use your Command-prompt -- not the Anaconda Powershell Prompt After entering your password, you are now using a terminal on the remote supercomputer. Try running the command pwd (\"print working directory\") to show that your terminal is indeed running commands on the remote cluster, not your desktop: # This is the same for all linux clusters pwd","title":"1. Sign in to the cluster"},{"location":"getting_started/run_a_workflow/submit_to_a_cluster/#2-load-vasp","text":"To load VASP into your environment, you typically need to run a 'load module' command: example WarWulf LongLeaf DogWood module load vasp module load vasp ; source /opt/ohpc/pub/intel/bin/ifortvars.sh ; module load vasp/5.4.4 module load vasp/5.4.4 Then check that the vasp command is found. If the vasp_std command worked correctly, you will see the following output (bc their command doesn't print help information like simmate or conda ): vasp_std # Error output may vary between different VASP versions Error reading item 'VCAIMAGES' from file INCAR.","title":"2. Load VASP"},{"location":"getting_started/run_a_workflow/submit_to_a_cluster/#3-build-your-personal-simmate-env","text":"Next we need to ensure Simmate is installed. If you see (base) at the start of your command-line, Anaconda is already installed. If not, ask your IT team how they want you install it. Typically it's by using miniconda which is just anaconda without the graphical user interface). With Anaconda set up, you can create your environment and install Simmate just like we did in the first tutorial: # Create your conda env with... conda create -n my_env -c conda-forge python = 3 .10 simmate conda activate my_env # Initialize your database on this new installation. simmate database reset Danger On WarWulf, we share a profile so make sure you name your environment something unique. For example, use yourname_env (e.g. jacks_env ).","title":"3. Build your personal Simmate env"},{"location":"getting_started/run_a_workflow/submit_to_a_cluster/#4-set-up-vasp-potentials","text":"Note This step is already completed for you on the WarWulf cluster Next, copy your Potentials into ~/simmate/vasp/Potentials and also copy the POSCAR file above onto your cluster. It can be diffult in the command line to move files around or even transfer them back and forth from your local computer to the supercomputer. It's much easier with a program like FileZilla , MobaXTerm , or another file transfer program. We recommend FileZilla, but it's entirely optional and up to you. Review our POTCAR guide from before if you need help on this step.","title":"4. Set up VASP potentials"},{"location":"getting_started/run_a_workflow/submit_to_a_cluster/#5-move-to-your-scratch-directory","text":"Typically, clusters will have a \"scratch\" directory that you should submit jobs from -- which is different from your home directory. Make sure you switch to that before submitting and workflows. (note, your POSCAR and all input files should be in this directory too): example WarWulf LongLeaf DogWood cd /path/to/my/scratch/space/ cd /media/synology/user/your_name cd /pine/scr/j/a/jacksund cd /21dayscratch/scr/y/o/youronyen","title":"5. Move to your 'scratch' directory"},{"location":"getting_started/run_a_workflow/submit_to_a_cluster/#6-build-our-input-files","text":"Just like we did on our laptop, we need to make our input files. For now, let's use this sample YAML file: workflow_name : static-energy.vasp.mit structure : database_table : MatprojStructure database_id : mp-22862 command : mpirun -n 4 vasp_std > vasp.out # OPTIONAL directory : my_custom_folder # OPTIONAL Put this in a file named my_settings.yaml in your scratch directory. Danger Take note of the -n 4 in our command. This is the number of cores that we want our calculation to use. Make sure this number matches your cpus-per-task setting in the next section","title":"6. Build our input files"},{"location":"getting_started/run_a_workflow/submit_to_a_cluster/#7-build-our-submit-script","text":"Earlier in this tutorial, we called simmate workflows run ... directly in our terminal, but this should NEVER be done on a supercomputer. Instead we should submit the workflow to the cluster's job queue. Typically, supercomputers use SLURM or PBS to submit jobs. For example, UNC's WarWulf , LongLeaf , and DogWood clusters each use SLURM . To submit, we would make a file named submit.sh : nano submit.sh ... and use contents likes ... example WarWulf LongLeaf DogWood #! /bin/sh #SBATCH --job-name=my_example_job #SBATCH --nodes=1 #SBATCH --ntasks=1 #SBATCH --cpus-per-task=4 #SBATCH --mem=4GB #SBATCH --time=01:00:00 #SBATCH --partition=general #SBATCH --output=slurm.out #SBATCH --mail-type=ALL #SBATCH --mail-user=my_username@live.unc.edu simmate workflows run my_settings.yaml #!/bin/bash #. /opt/ohpc/pub/suppress.sh #supress infiniband output, set vasp path #SBATCH --job-name=my_example_job #SBATCH --nodes=1 #SBATCH --ntasks=1 #SBATCH --cpus-per-task=4 #SBATCH --mem=4GB #SBATCH --time=01:00:00 #SBATCH --partition=p1 #SBATCH --output=slurm.out #SBATCH --mail-type=ALL #SBATCH --mail-user=my_username@live.unc.edu simmate workflows run my_settings.yaml #! /bin/sh #SBATCH --job-name=my_example_job #SBATCH --nodes=20 #SBATCH --ntasks=1 #SBATCH --mem=40g #SBATCH --partition=general #SBATCH --output=slurm.out #SBATCH --mail-type=FAIL #SBATCH --mail-user=youronyen@live.unc.edu #SBATCH --time=11-00:00 simmate workflows run my_settings.yaml Danger Note the massive ntasks and node values here. DogWood is only meant for large calculations, so talk with our team before submitting. #!/bin/sh #SBATCH --job-name=NEB #SBATCH --ntasks=704 #SBATCH --nodes=16 #SBATCH --time=2-00:00 #SBATCH --mem=300g #SBATCH --partition=2112_queue #SBATCH --mail-type=ALL #SBATCH --mail-user=lamcrae@live.unc.edu simmate workflows run my_settings.yaml Info Each of these SBATCH parameters set how we would like to sumbit a job and how many resources we expect to use. These are explained in SLURM's documnetation for sbatch , but you may need help from your IT team to update them. But to break down these example parameters... job-name : the name that identifies your job. It will be visible when you check the status of your job nodes : the number of server nodes (or CPUs) that you request. Typically leave this at 1. ntasks : the number tasks that you'll be running. We run one workflow at a time here, so we use 1. cpus-per-task : the number of CPU tasks required for each run. We run our workflow using 4 cores ( mpirun -n 4 ) so we need to request 4 cores for it here mem : the memory requested for this job. If it is exceeded, the job will be terminated. time : the maximum time requested for this job. If it is exceeded, the job will be terminated. partition : the group of nodes that we request resources on. You can often remove this line and use the cluster's default. output : the name of the file to write the job output (including errors) mail-type + mail-user : will send an email alerts when a jobs starts/stops/fails/etc.","title":"7. Build our submit script"},{"location":"getting_started/run_a_workflow/submit_to_a_cluster/#8-double-check-everything","text":"Let's go back through our check list before we submit loaded the VASP module activated your conda environment in the temporary working directory have our yaml file (+ extra inputs like a POSCAR) in the directory have our submit.sh in the directory structure file (e.g. POSCAR ) is present in working directory If all of these are set, you're good to go.","title":"8. Double check everything"},{"location":"getting_started/run_a_workflow/submit_to_a_cluster/#9-submit-a-workflow-to-the-queue","text":"Finally, let's submit to our cluster! sbatch submit.sh","title":"9. Submit a workflow to the queue"},{"location":"getting_started/run_a_workflow/submit_to_a_cluster/#10-monitor-its-progress","text":"You can then monitor your jobs progress with: example WarWulf LongLeaf DogWood squeue -u my_username sq # or sq | grep my_name Example sq | grep jack squeue -u my_onyen squeue -u my_onyen","title":"10. Monitor its progress"},{"location":"getting_started/run_a_workflow/submit_to_a_cluster/#success","text":"You've now submitted a Simmate workflow to a remote cluster !!! Tip Be sure to go back through this section a few times before moving on. Submitting remote jobs can be tedious but it's important to understand. Advanced features of Simmate will let you skip a lot of this work down the road, but that won't happen until we reaching the \"Adding Computational Resources\" guide.","title":"Success!"},{"location":"getting_started/run_a_workflow/view_all_workflows/","text":"Viewing all available workflows \u00b6 Review \u00b6 In the last three sections, we completed our check list for running a workflow: tell Simmate where our VASP files are set up our database so results can be saved select a structure for our calculation Now we can explore the different workflows available and choose one to run. Viewing all workflows \u00b6 At the most basic level, you'll want to use Simmate to calculate a material's energy, structure, or properties. For each type of task, we have prebuilt workflows. All of these are accessible through the simmate workflows command. Let's start by seeing what is available by running: simmate workflows list-all The output will be similar to... These are the workflows that have been registerd: (01) customized.vasp.user-config (02) diffusion.vasp.neb-all-paths-mit (03) diffusion.vasp.neb-from-endpoints-mit (04) diffusion.vasp.neb-from-images-mit (05) diffusion.vasp.neb-single-path-mit (06) dynamics.vasp.matproj (07) dynamics.vasp.mit (08) dynamics.vasp.mvl-npt (09) electronic-structure.vasp.matproj-full ... << plus others that are cut-off for clarity >> Learning about a workflow \u00b6 Next, try out the explore command, which gives us a more interactive way to view the available workflows. simmate workflows explore When prompted to choose a type of workflow or a specific preset, choose whichever you'd like! A description of the workflow will be printed at the very end. As an example, here's the output of an example workflow relaxation.vasp.staged which is commonly used in our evolutionary search algorithm. To get this output, we used the simmate workflows explore then selected option 6 (relaxation) and then option 1 (matproj): ===================== relaxation.vasp.matproj ===================== Description: This task is a reimplementation of pymatgen's MPRelaxSet. Runs a VASP geometry optimization using Materials Project settings. Materials Project settings are often considered the minimum-required quality for publication and is sufficient for most applications. If you are looking at one structure in detail (for electronic, vibrational, and other properties), you should still test for convergence using higher-quality settings. Parameters: REQUIRED PARAMETERS -------------------- - structure OPTIONAL PARAMETERS (+ their defaults): --------------------------------------- - directory: null - command: vasp_std > vasp.out - is_restart: false - run_id: null - compress_output: false - source: null - standardize_structure: false - symmetry_precision: 0.01 - angle_tolerance: 0.5 *** 'null' indicates the parameter is set with advanced logic To understand each parameter, you can read through our parameter docs, which give full descriptions and examples. ================================================================== Learning about parameters \u00b6 In the message above, there's a link to our Parameter docs . You can access this page by clicking the Parameters section at the top of this webpage (or click here ). This page lists out ALL parameters for ALL workflows. If there's an input you'd like to learn more above -- this is the place to start. Selecting our practice workflow \u00b6 In the rest of tutorial, we will be using static-energy.vasp.mit which runs a simple static energy calculation using MIT Project settings (these settings are based on pymatgen's MITRelaxSet ).","title":"View all workflows"},{"location":"getting_started/run_a_workflow/view_all_workflows/#viewing-all-available-workflows","text":"","title":"Viewing all available workflows"},{"location":"getting_started/run_a_workflow/view_all_workflows/#review","text":"In the last three sections, we completed our check list for running a workflow: tell Simmate where our VASP files are set up our database so results can be saved select a structure for our calculation Now we can explore the different workflows available and choose one to run.","title":"Review"},{"location":"getting_started/run_a_workflow/view_all_workflows/#viewing-all-workflows","text":"At the most basic level, you'll want to use Simmate to calculate a material's energy, structure, or properties. For each type of task, we have prebuilt workflows. All of these are accessible through the simmate workflows command. Let's start by seeing what is available by running: simmate workflows list-all The output will be similar to... These are the workflows that have been registerd: (01) customized.vasp.user-config (02) diffusion.vasp.neb-all-paths-mit (03) diffusion.vasp.neb-from-endpoints-mit (04) diffusion.vasp.neb-from-images-mit (05) diffusion.vasp.neb-single-path-mit (06) dynamics.vasp.matproj (07) dynamics.vasp.mit (08) dynamics.vasp.mvl-npt (09) electronic-structure.vasp.matproj-full ... << plus others that are cut-off for clarity >>","title":"Viewing all workflows"},{"location":"getting_started/run_a_workflow/view_all_workflows/#learning-about-a-workflow","text":"Next, try out the explore command, which gives us a more interactive way to view the available workflows. simmate workflows explore When prompted to choose a type of workflow or a specific preset, choose whichever you'd like! A description of the workflow will be printed at the very end. As an example, here's the output of an example workflow relaxation.vasp.staged which is commonly used in our evolutionary search algorithm. To get this output, we used the simmate workflows explore then selected option 6 (relaxation) and then option 1 (matproj): ===================== relaxation.vasp.matproj ===================== Description: This task is a reimplementation of pymatgen's MPRelaxSet. Runs a VASP geometry optimization using Materials Project settings. Materials Project settings are often considered the minimum-required quality for publication and is sufficient for most applications. If you are looking at one structure in detail (for electronic, vibrational, and other properties), you should still test for convergence using higher-quality settings. Parameters: REQUIRED PARAMETERS -------------------- - structure OPTIONAL PARAMETERS (+ their defaults): --------------------------------------- - directory: null - command: vasp_std > vasp.out - is_restart: false - run_id: null - compress_output: false - source: null - standardize_structure: false - symmetry_precision: 0.01 - angle_tolerance: 0.5 *** 'null' indicates the parameter is set with advanced logic To understand each parameter, you can read through our parameter docs, which give full descriptions and examples. ==================================================================","title":"Learning about a workflow"},{"location":"getting_started/run_a_workflow/view_all_workflows/#learning-about-parameters","text":"In the message above, there's a link to our Parameter docs . You can access this page by clicking the Parameters section at the top of this webpage (or click here ). This page lists out ALL parameters for ALL workflows. If there's an input you'd like to learn more above -- this is the place to start.","title":"Learning about parameters"},{"location":"getting_started/run_a_workflow/view_all_workflows/#selecting-our-practice-workflow","text":"In the rest of tutorial, we will be using static-energy.vasp.mit which runs a simple static energy calculation using MIT Project settings (these settings are based on pymatgen's MITRelaxSet ).","title":"Selecting our practice workflow"},{"location":"getting_started/run_a_workflow/view_the_results/","text":"Viewing the workflow's results \u00b6 Basic output files \u00b6 Once your job completes, you may notice a few extra files in your output. One of them is simmate_summary.yaml , which contains some quick information for you. The information in this file is a snippet of what's available in the database: _DATABASE_TABLE_ : StaticEnergy _TABLE_ID_ : 1 _WEBSITE_URL_ : http://127.0.0.1:8000/workflows/static-energy/vasp/mit/1 band_gap : 4.9924 chemical_system : Cl-Na computer_system : digital-storm conduction_band_minimum : 4.306 corrections : [] created_at : 2022-09-10 14:32:35.857088+00:00 density : 2.1053060843576104 density_atomic : 0.04338757298280908 directory : /home/jacksund/Documents/spyder_wd/simmate-task-e9tddsyw energy : -27.25515165 energy_fermi : -0.63610593 energy_per_atom : -3.40689395625 formula_anonymous : AB formula_full : Na4 Cl4 formula_reduced : NaCl id : 42 is_gap_direct : true lattice_stress_norm : 8.428394235089161 lattice_stress_norm_per_atom : 1.0535492793861452 nelements : 2 nsites : 8 run_id : 3a1bd23f-705c-4947-96fa-3740865ed12d site_force_norm_max : 1.4907796617877505e-05 site_forces_norm : 2.257345786537809e-05 site_forces_norm_per_atom : 2.8216822331722614e-06 spacegroup_id : 225 updated_at : 2022-09-10 14:33:09.419637+00:00 valence_band_maximum : -0.6864 volume : 184.38459332974767 volume_molar : 13.87987468758872 workflow_name : static-energy.vasp.mit workflow_version : 0.10.0 Other workflows will also write out plots for you. For example, electronic-structure workflows will calculate a band structure using Materials Project settings, and write an image of your final band structure to band_structure.png . These extra files and plots vary for each workflow, but they make checking your results nice and quick. The website view \u00b6 In the simmate_summary.yaml file, there is the _WEBSITE_URL_ . You can copy/paste this URL into your browser and view your results in an interactive format. Just make sure you are running your local server first: simmate run-server Then open the link given by _WEBSITE_URL_ : http://127.0.0.1:8000/workflows/static-energy/vasp/mit/1 Note Remember that the server and your database are limited to your local computer. Trying to access a URL on a computer that doesn't share the same database file will not work -- so you may need to copy your database file from the cluster to your local computer. Or even better -- if you would like to access results through the internet, then you have to switch to a cloud database (which is covered in a later tutorial). Advanced data analysis \u00b6 We can analyze our final structure and the full results with Simmate's toolkit and database. Accessing these require Python, so our next tutorial will introduce you to Python by directly interacting with the toolkit. We will then work our way up to accessing our database in a follow-up tutorial.","title":"Viewing the results"},{"location":"getting_started/run_a_workflow/view_the_results/#viewing-the-workflows-results","text":"","title":"Viewing the workflow's results"},{"location":"getting_started/run_a_workflow/view_the_results/#basic-output-files","text":"Once your job completes, you may notice a few extra files in your output. One of them is simmate_summary.yaml , which contains some quick information for you. The information in this file is a snippet of what's available in the database: _DATABASE_TABLE_ : StaticEnergy _TABLE_ID_ : 1 _WEBSITE_URL_ : http://127.0.0.1:8000/workflows/static-energy/vasp/mit/1 band_gap : 4.9924 chemical_system : Cl-Na computer_system : digital-storm conduction_band_minimum : 4.306 corrections : [] created_at : 2022-09-10 14:32:35.857088+00:00 density : 2.1053060843576104 density_atomic : 0.04338757298280908 directory : /home/jacksund/Documents/spyder_wd/simmate-task-e9tddsyw energy : -27.25515165 energy_fermi : -0.63610593 energy_per_atom : -3.40689395625 formula_anonymous : AB formula_full : Na4 Cl4 formula_reduced : NaCl id : 42 is_gap_direct : true lattice_stress_norm : 8.428394235089161 lattice_stress_norm_per_atom : 1.0535492793861452 nelements : 2 nsites : 8 run_id : 3a1bd23f-705c-4947-96fa-3740865ed12d site_force_norm_max : 1.4907796617877505e-05 site_forces_norm : 2.257345786537809e-05 site_forces_norm_per_atom : 2.8216822331722614e-06 spacegroup_id : 225 updated_at : 2022-09-10 14:33:09.419637+00:00 valence_band_maximum : -0.6864 volume : 184.38459332974767 volume_molar : 13.87987468758872 workflow_name : static-energy.vasp.mit workflow_version : 0.10.0 Other workflows will also write out plots for you. For example, electronic-structure workflows will calculate a band structure using Materials Project settings, and write an image of your final band structure to band_structure.png . These extra files and plots vary for each workflow, but they make checking your results nice and quick.","title":"Basic output files"},{"location":"getting_started/run_a_workflow/view_the_results/#the-website-view","text":"In the simmate_summary.yaml file, there is the _WEBSITE_URL_ . You can copy/paste this URL into your browser and view your results in an interactive format. Just make sure you are running your local server first: simmate run-server Then open the link given by _WEBSITE_URL_ : http://127.0.0.1:8000/workflows/static-energy/vasp/mit/1 Note Remember that the server and your database are limited to your local computer. Trying to access a URL on a computer that doesn't share the same database file will not work -- so you may need to copy your database file from the cluster to your local computer. Or even better -- if you would like to access results through the internet, then you have to switch to a cloud database (which is covered in a later tutorial).","title":"The website view"},{"location":"getting_started/run_a_workflow/view_the_results/#advanced-data-analysis","text":"We can analyze our final structure and the full results with Simmate's toolkit and database. Accessing these require Python, so our next tutorial will introduce you to Python by directly interacting with the toolkit. We will then work our way up to accessing our database in a follow-up tutorial.","title":"Advanced data analysis"},{"location":"getting_started/run_a_workflow/view_workflow_settings/","text":"Viewing a workflow's settings and inputs \u00b6 Warning This section is aimed at experienced VASP users that want to preview settings used. Beginners will rarely use the commands shared here. Viewing settings (basic) \u00b6 To view a workflow's configuration before using it, we type the command simmate workflows show-config . Try this out by running: simmate workflows show-config relaxation.vasp.quality00 VASP users will recognize that this specifies the contents of a VASP INCAR file. The quality00 is the most basic workflow configuration because the INCAR will not depend on the structure or composition of your crystal. Viewing settings (complex) \u00b6 Next, look at a more advanced calculation. Run the command: simmate workflows show-config static-energy.vasp.mit Here, you'll see that some INCAR settings rely on composition and that we have a list of error handlers to help ensure that the calculation finishes successfully. Generating input files (optional) \u00b6 Now, let's go one step further and provide a specific structure (the POSCAR we just made) into a specific workflow (static-energy/mit). To do this, make sure our terminal has the same folder open as where our file is! For example, if your POSCAR is on your Desktop while your terminal is in your home directory, you can type cd Desktop to change your active folder to your Desktop. Then run the command: simmate workflows setup-only static-energy.vasp.mit --structure POSCAR You'll see a new folder created named static-energy.vasp.mit.SETUP-ONLY . When you open it, you'll see all the files that Simmate made for VASP to use. This is useful when you're an advanced user who wants to alter these files before running VASP manually -- this could happen when you want to test new workflows or unique systems. Note For absolute beginners, you don't immediately need to understand these files, but they will eventually be important for understanding the scientific limitations of your results or for running your own custom calculations. Whether you use VASP , ABINIT , or another program, be sure to go through their tutorials, rather than always depending on Simmate to run the program for you. Until you reach that point, we'll have Simmate do it all for us.","title":"View workflow settings"},{"location":"getting_started/run_a_workflow/view_workflow_settings/#viewing-a-workflows-settings-and-inputs","text":"Warning This section is aimed at experienced VASP users that want to preview settings used. Beginners will rarely use the commands shared here.","title":"Viewing a workflow's settings and inputs"},{"location":"getting_started/run_a_workflow/view_workflow_settings/#viewing-settings-basic","text":"To view a workflow's configuration before using it, we type the command simmate workflows show-config . Try this out by running: simmate workflows show-config relaxation.vasp.quality00 VASP users will recognize that this specifies the contents of a VASP INCAR file. The quality00 is the most basic workflow configuration because the INCAR will not depend on the structure or composition of your crystal.","title":"Viewing settings (basic)"},{"location":"getting_started/run_a_workflow/view_workflow_settings/#viewing-settings-complex","text":"Next, look at a more advanced calculation. Run the command: simmate workflows show-config static-energy.vasp.mit Here, you'll see that some INCAR settings rely on composition and that we have a list of error handlers to help ensure that the calculation finishes successfully.","title":"Viewing settings (complex)"},{"location":"getting_started/run_a_workflow/view_workflow_settings/#generating-input-files-optional","text":"Now, let's go one step further and provide a specific structure (the POSCAR we just made) into a specific workflow (static-energy/mit). To do this, make sure our terminal has the same folder open as where our file is! For example, if your POSCAR is on your Desktop while your terminal is in your home directory, you can type cd Desktop to change your active folder to your Desktop. Then run the command: simmate workflows setup-only static-energy.vasp.mit --structure POSCAR You'll see a new folder created named static-energy.vasp.mit.SETUP-ONLY . When you open it, you'll see all the files that Simmate made for VASP to use. This is useful when you're an advanced user who wants to alter these files before running VASP manually -- this could happen when you want to test new workflows or unique systems. Note For absolute beginners, you don't immediately need to understand these files, but they will eventually be important for understanding the scientific limitations of your results or for running your own custom calculations. Whether you use VASP , ABINIT , or another program, be sure to go through their tutorials, rather than always depending on Simmate to run the program for you. Until you reach that point, we'll have Simmate do it all for us.","title":"Generating input files (optional)"},{"location":"getting_started/use_a_cloud_database/build_a_postgres_database/","text":"Tip Make sure you have read the previous section! Setting up a database can be tricky, and the majority of users can avoid it altogether. Choosing your database engine \u00b6 Simmate uses Django ORM to build and manage its database, so any Django-supported database can be used with Simmate. This includes PostgreSQL, MariaDB, MySQL, Oracle, SQLite, and others through third-parties. noSQL databases like MongoDB are supported through djongo . The full documentation for django databases is available here . However, we strongly recommended choosing Postgres , which we cover in the next section. Warning Our team uses SQLite (for local testing) and PostgreSQL (for production), so at the moment, we can only offer guidance on these two backends. You are welcome to use the others, but be wary that we haven't thuroughly tested these backends and won't be able to help you troubleshoot if errors arise. Intro to Postgres set up \u00b6 PostgreSQL is free and open-source, so you can avoid costs and set it up manually. However, it's MUCH easier to use a database service such as DigitalOcean , Linode , GoogleCloud , AWS , Azure , or another provider. These providers set up the database for you through a nice user-interface. If you still want to manually build a postgres server, there are many tutorials and guides available on how to do this ( 1 , 2 , etc.). Just be aware that this is can take a lot of time AND your final database connection may be slower if your team works accross multiple locations. Setup Postgres with DigitalOcean \u00b6 Intro & expected costs \u00b6 Our team uses DigitialOcean, where the starter database server (~$15/month) is plenty for Simmate usage. You'll only need >10GB if you are running >100,000 structure relaxations or frequently using unitcells with >1000 atoms. (i) create an account \u00b6 To start, make an account on DigitalOcean using this link (which uses our refferal). We recommend using your Github account to sign in. This referral link does two things: DigitialOcean gives you $100 credit for servers (for 60 days) DigitialOcean gives the Simmate team $10 credit, which will help fund our servers If you have any issues, please make sure that DigitalOcean is still actually offering this deal here . Simmate is not affiliated with DigitalOcean. (ii) create the cloud database \u00b6 On our DigitalOcean dashboard, click the green \"Create\" button in the top right and then select \"Database\". It should bring you to this page . For \"database engine\", select the newest version of PostgreSQL (currently v14) The remainder of the page's options can be left at their default values. Select Create a Database Cluster when you're ready. For the new homepage on your cluster, there is a \"Get Started\" button. We will go through this dialog in the next section. Note, this is the database cluster , which can host multiple databases on it (each with all their own tables). (iii) connect to the database \u00b6 Before we set up our database on this cluster, we are are first going to try connecting the default database on it (named defaultdb ). On your new database's page, you'll see a \"Getting Started\" dialog -- select it! For \"Restrict inbound connections\", this is completely optional and beginneers should skip this for now. We skip this because if you'll be running calculations on some supercomputer/cluster, then you'll need to add ALL of the associated IP addresses in order for connections to work properly. That's a lot of IP addresses to grab and configure properly -- so we leave this to advanced users. \"Connection details\" is what we need to give to Simmate/Django. Let's copy this information. As an example, here is what the details look like on DigitalOcean: username = doadmin password = asd87a9sd867fasd host = db-postgresql-nyc3-49797-do-user-8843535-0.b.db.ondigitalocean.com port = 25060 database = defaultdb sslmode = require In your simmate python environment, make sure you have the Postgres engine installed. The package is psycopg2 , which let's Django talk with Postgres. To install this, run the command: conda install -n my_env -c conda-forge psycopg2 We need to pass this information to Simmate (which connects using Django). To do this, add a file named my_env-database.yaml (using your conda env name) to your simmate config directory ( ~/simmate ) with the following content -- be sure substute in your connection information and note that ENGINE tells Django we are using Postgres: default : ENGINE : django.db.backends.postgresql_psycopg2 HOST : db-postgresql-nyc3-49797-do-user-8843535-0.b.db.ondigitalocean.com NAME : defaultdb USER : doadmin PASSWORD : asd87a9sd867fasd PORT : 25060 OPTIONS : sslmode : require Make sure you can connect to this database on your local computer by running the following in Spyder: from simmate.configuration.django.settings import DATABASES print ( DATABASES ) # this should give your connect info! (iv) make a separate the database for testing (on the same server) \u00b6 Just like how we don't use the (base) environment in Anaconda, we don't want to use the default database defaultdb on our cluster. Here will make a new database -- one that we can delete if we'd like to restart. On DigitalOcean with your Database Cluster page, select the \"Users&Databases\" tab. Create a new database using the \"Add new database\" button and name this simmate-database-00 . We name it this way because you may want to make new/separate databases and numbering is a quick way to keep track of these. In your connection settings (from the section above), switch the NAME from defaultdb to simmate-database-00 . You will change this in your my_env-database.yaml file. (v) create a connection pool \u00b6 When we have a bunch of calculations running at once, we need to make sure our database can handle all of these connections. Therefore, we make a connection pool which allows for thousands of connections. This \"pool\" works like a waitlist where the database handles each connection request in order. Select the \"Connection Pools\" tab and then \"Create a Connection Pool\" Name your pool simmate-database-00-pool and select simmate-database-00 for the database Select \"Transaction\" for our mode (the default) and set our pool size to 10 (or modify this value as you wish) Create the pool when you're ready! You'll have to update your my_env-database.yaml file to these connection settings. At this point your file will look similar to this (note, our NAME and PORT values have changed): default : ENGINE : django.db.backends.postgresql_psycopg2 HOST : db-postgresql-nyc3-49797-do-user-8843535-0.b.db.ondigitalocean.com NAME : simmate-database-00-pool # THIS LINE WAS UPDATED USER : doadmin PASSWORD : asd87a9sd867fasd PORT : 25061 OPTIONS : sslmode : require (vi) build our database tables \u00b6 Now that we set up and connected to our database, we can now make our Simmate database tables and start filling them with data! We do this the same way we did without a cloud database: In your terminal, make sure you have you Simmate enviornment activated Run the following command: simmate database reset You're now ready to start using Simmate with your new database! (vii) load third-party data \u00b6 This step is optional. With Sqlite, we were able to download a prebuilt database with data from third-parties already in it. However, creating our postgres database means our database is entirely empty. To load ALL third-party data (~5GB total), you can use the following command. We can also use Dask to run this in parallel and speed things up. Depending on your internet connection and CPU speed, this can take up to 24hrs. simmate database load-remote-archives --parallel Warning --parallel will use all cores on your CPU. Keep this in mind if you are running other programs/calculations on your computer already. (viii) sharing the database \u00b6 If you want to share this database with others, you simply need to have them copy your config file: my_env-database.yaml . They won't need to run simmate database reset because you did it for them.","title":"Building a database"},{"location":"getting_started/use_a_cloud_database/build_a_postgres_database/#choosing-your-database-engine","text":"Simmate uses Django ORM to build and manage its database, so any Django-supported database can be used with Simmate. This includes PostgreSQL, MariaDB, MySQL, Oracle, SQLite, and others through third-parties. noSQL databases like MongoDB are supported through djongo . The full documentation for django databases is available here . However, we strongly recommended choosing Postgres , which we cover in the next section. Warning Our team uses SQLite (for local testing) and PostgreSQL (for production), so at the moment, we can only offer guidance on these two backends. You are welcome to use the others, but be wary that we haven't thuroughly tested these backends and won't be able to help you troubleshoot if errors arise.","title":"Choosing your database engine"},{"location":"getting_started/use_a_cloud_database/build_a_postgres_database/#intro-to-postgres-set-up","text":"PostgreSQL is free and open-source, so you can avoid costs and set it up manually. However, it's MUCH easier to use a database service such as DigitalOcean , Linode , GoogleCloud , AWS , Azure , or another provider. These providers set up the database for you through a nice user-interface. If you still want to manually build a postgres server, there are many tutorials and guides available on how to do this ( 1 , 2 , etc.). Just be aware that this is can take a lot of time AND your final database connection may be slower if your team works accross multiple locations.","title":"Intro to Postgres set up"},{"location":"getting_started/use_a_cloud_database/build_a_postgres_database/#setup-postgres-with-digitalocean","text":"","title":"Setup Postgres with DigitalOcean"},{"location":"getting_started/use_a_cloud_database/build_a_postgres_database/#intro-expected-costs","text":"Our team uses DigitialOcean, where the starter database server (~$15/month) is plenty for Simmate usage. You'll only need >10GB if you are running >100,000 structure relaxations or frequently using unitcells with >1000 atoms.","title":"Intro &amp; expected costs"},{"location":"getting_started/use_a_cloud_database/build_a_postgres_database/#i-create-an-account","text":"To start, make an account on DigitalOcean using this link (which uses our refferal). We recommend using your Github account to sign in. This referral link does two things: DigitialOcean gives you $100 credit for servers (for 60 days) DigitialOcean gives the Simmate team $10 credit, which will help fund our servers If you have any issues, please make sure that DigitalOcean is still actually offering this deal here . Simmate is not affiliated with DigitalOcean.","title":"(i) create an account"},{"location":"getting_started/use_a_cloud_database/build_a_postgres_database/#ii-create-the-cloud-database","text":"On our DigitalOcean dashboard, click the green \"Create\" button in the top right and then select \"Database\". It should bring you to this page . For \"database engine\", select the newest version of PostgreSQL (currently v14) The remainder of the page's options can be left at their default values. Select Create a Database Cluster when you're ready. For the new homepage on your cluster, there is a \"Get Started\" button. We will go through this dialog in the next section. Note, this is the database cluster , which can host multiple databases on it (each with all their own tables).","title":"(ii) create the cloud database"},{"location":"getting_started/use_a_cloud_database/build_a_postgres_database/#iii-connect-to-the-database","text":"Before we set up our database on this cluster, we are are first going to try connecting the default database on it (named defaultdb ). On your new database's page, you'll see a \"Getting Started\" dialog -- select it! For \"Restrict inbound connections\", this is completely optional and beginneers should skip this for now. We skip this because if you'll be running calculations on some supercomputer/cluster, then you'll need to add ALL of the associated IP addresses in order for connections to work properly. That's a lot of IP addresses to grab and configure properly -- so we leave this to advanced users. \"Connection details\" is what we need to give to Simmate/Django. Let's copy this information. As an example, here is what the details look like on DigitalOcean: username = doadmin password = asd87a9sd867fasd host = db-postgresql-nyc3-49797-do-user-8843535-0.b.db.ondigitalocean.com port = 25060 database = defaultdb sslmode = require In your simmate python environment, make sure you have the Postgres engine installed. The package is psycopg2 , which let's Django talk with Postgres. To install this, run the command: conda install -n my_env -c conda-forge psycopg2 We need to pass this information to Simmate (which connects using Django). To do this, add a file named my_env-database.yaml (using your conda env name) to your simmate config directory ( ~/simmate ) with the following content -- be sure substute in your connection information and note that ENGINE tells Django we are using Postgres: default : ENGINE : django.db.backends.postgresql_psycopg2 HOST : db-postgresql-nyc3-49797-do-user-8843535-0.b.db.ondigitalocean.com NAME : defaultdb USER : doadmin PASSWORD : asd87a9sd867fasd PORT : 25060 OPTIONS : sslmode : require Make sure you can connect to this database on your local computer by running the following in Spyder: from simmate.configuration.django.settings import DATABASES print ( DATABASES ) # this should give your connect info!","title":"(iii) connect to the database"},{"location":"getting_started/use_a_cloud_database/build_a_postgres_database/#iv-make-a-separate-the-database-for-testing-on-the-same-server","text":"Just like how we don't use the (base) environment in Anaconda, we don't want to use the default database defaultdb on our cluster. Here will make a new database -- one that we can delete if we'd like to restart. On DigitalOcean with your Database Cluster page, select the \"Users&Databases\" tab. Create a new database using the \"Add new database\" button and name this simmate-database-00 . We name it this way because you may want to make new/separate databases and numbering is a quick way to keep track of these. In your connection settings (from the section above), switch the NAME from defaultdb to simmate-database-00 . You will change this in your my_env-database.yaml file.","title":"(iv) make a separate the database for testing (on the same server)"},{"location":"getting_started/use_a_cloud_database/build_a_postgres_database/#v-create-a-connection-pool","text":"When we have a bunch of calculations running at once, we need to make sure our database can handle all of these connections. Therefore, we make a connection pool which allows for thousands of connections. This \"pool\" works like a waitlist where the database handles each connection request in order. Select the \"Connection Pools\" tab and then \"Create a Connection Pool\" Name your pool simmate-database-00-pool and select simmate-database-00 for the database Select \"Transaction\" for our mode (the default) and set our pool size to 10 (or modify this value as you wish) Create the pool when you're ready! You'll have to update your my_env-database.yaml file to these connection settings. At this point your file will look similar to this (note, our NAME and PORT values have changed): default : ENGINE : django.db.backends.postgresql_psycopg2 HOST : db-postgresql-nyc3-49797-do-user-8843535-0.b.db.ondigitalocean.com NAME : simmate-database-00-pool # THIS LINE WAS UPDATED USER : doadmin PASSWORD : asd87a9sd867fasd PORT : 25061 OPTIONS : sslmode : require","title":"(v) create a connection pool"},{"location":"getting_started/use_a_cloud_database/build_a_postgres_database/#vi-build-our-database-tables","text":"Now that we set up and connected to our database, we can now make our Simmate database tables and start filling them with data! We do this the same way we did without a cloud database: In your terminal, make sure you have you Simmate enviornment activated Run the following command: simmate database reset You're now ready to start using Simmate with your new database!","title":"(vi) build our database tables"},{"location":"getting_started/use_a_cloud_database/build_a_postgres_database/#vii-load-third-party-data","text":"This step is optional. With Sqlite, we were able to download a prebuilt database with data from third-parties already in it. However, creating our postgres database means our database is entirely empty. To load ALL third-party data (~5GB total), you can use the following command. We can also use Dask to run this in parallel and speed things up. Depending on your internet connection and CPU speed, this can take up to 24hrs. simmate database load-remote-archives --parallel Warning --parallel will use all cores on your CPU. Keep this in mind if you are running other programs/calculations on your computer already.","title":"(vii) load third-party data"},{"location":"getting_started/use_a_cloud_database/build_a_postgres_database/#viii-sharing-the-database","text":"If you want to share this database with others, you simply need to have them copy your config file: my_env-database.yaml . They won't need to run simmate database reset because you did it for them.","title":"(viii) sharing the database"},{"location":"getting_started/use_a_cloud_database/private_vs_collab/","text":"Should I set up my own database? \u00b6 Share a database with others \u00b6 A cloud database lets you save your results to a remote computer through an internet connection, and once a database is set up, you can add as many users and connections as you'd like. Therefore, if you are part of a team, you only need ONE person to setup and manage ONE cloud database. Anyone can collaborate if they have a username and password. Collaborating with the Warren Lab \u00b6 We could (theoretically) have the entire scientific community working together and sharing their results. To this end, our Simmate team tries to get as many labs collaborating as possible. If you would like join this effort, simply send an email to simmate.team@gmail.com and ask. Once you're on our team, you won't have to setup or manage any cloud database. Note If you decide to collaborate, we will take on the costs of the cloud database for now, but as our database and community grows, we may need help with funding. Until then, don't hesitate to ask for our status. Using a private database \u00b6 If you would instead like a private database for your team, designate one person to be the database manager. Only that person needs to complete the next section (on setting up your cloud database). All other members, wait until you get connection information and then jump to the final section (on connecting to your cloud database). So to summarize, only create your own cloud database if both of these conditions are met: you prefer a private database instead of Simmate's collaborative effort you are the point-person for managing your team's private database Connecting to a cloud database \u00b6 If you are collaborating with someone that set up a database already, then connecting to it will be the easiest thing we've done yet! Once you have the connection parameters for your cloud database, simply create the file ~/simmate/my_env-database.yaml and add the connection parameters that your point-person provided. As an example, this my_env-database.yaml file gives a default database to use: default : ENGINE : django.db.backends.postgresql_psycopg2 HOST : simmate-database-do-user-8843535-0.b.db.ondigitalocean.com NAME : simmate-database-00-pool USER : doadmin PASSWORD : ryGEc5PDxC2IHDSM PORT : 25061 OPTIONS : sslmode : require That's it! When you run a new workflow, results will be saved to this cloud database instead of your local file. Danger If you lab uses postgres, make sure you have extra database dependencies installed. For postgres, run the command: conda install -n my_env -c conda-forge psycopg2 Tip If you would like to share the database with anyone or any other computer, just share this connection file with them.","title":"Private vs shared"},{"location":"getting_started/use_a_cloud_database/private_vs_collab/#should-i-set-up-my-own-database","text":"","title":"Should I set up my own database?"},{"location":"getting_started/use_a_cloud_database/private_vs_collab/#share-a-database-with-others","text":"A cloud database lets you save your results to a remote computer through an internet connection, and once a database is set up, you can add as many users and connections as you'd like. Therefore, if you are part of a team, you only need ONE person to setup and manage ONE cloud database. Anyone can collaborate if they have a username and password.","title":"Share a database with others"},{"location":"getting_started/use_a_cloud_database/private_vs_collab/#collaborating-with-the-warren-lab","text":"We could (theoretically) have the entire scientific community working together and sharing their results. To this end, our Simmate team tries to get as many labs collaborating as possible. If you would like join this effort, simply send an email to simmate.team@gmail.com and ask. Once you're on our team, you won't have to setup or manage any cloud database. Note If you decide to collaborate, we will take on the costs of the cloud database for now, but as our database and community grows, we may need help with funding. Until then, don't hesitate to ask for our status.","title":"Collaborating with the Warren Lab"},{"location":"getting_started/use_a_cloud_database/private_vs_collab/#using-a-private-database","text":"If you would instead like a private database for your team, designate one person to be the database manager. Only that person needs to complete the next section (on setting up your cloud database). All other members, wait until you get connection information and then jump to the final section (on connecting to your cloud database). So to summarize, only create your own cloud database if both of these conditions are met: you prefer a private database instead of Simmate's collaborative effort you are the point-person for managing your team's private database","title":"Using a private database"},{"location":"getting_started/use_a_cloud_database/private_vs_collab/#connecting-to-a-cloud-database","text":"If you are collaborating with someone that set up a database already, then connecting to it will be the easiest thing we've done yet! Once you have the connection parameters for your cloud database, simply create the file ~/simmate/my_env-database.yaml and add the connection parameters that your point-person provided. As an example, this my_env-database.yaml file gives a default database to use: default : ENGINE : django.db.backends.postgresql_psycopg2 HOST : simmate-database-do-user-8843535-0.b.db.ondigitalocean.com NAME : simmate-database-00-pool USER : doadmin PASSWORD : ryGEc5PDxC2IHDSM PORT : 25061 OPTIONS : sslmode : require That's it! When you run a new workflow, results will be saved to this cloud database instead of your local file. Danger If you lab uses postgres, make sure you have extra database dependencies installed. For postgres, run the command: conda install -n my_env -c conda-forge psycopg2 Tip If you would like to share the database with anyone or any other computer, just share this connection file with them.","title":"Connecting to a cloud database"},{"location":"getting_started/use_a_cloud_database/quick_start/","text":"Use a cloud database \u00b6 In this tutorial, you will learn how to switch from saving results locally to saving them to a collaborative remote (or \"cloud\") database. The quick tutorial \u00b6 Consider collaborating! Simmate is built for sharing results, so email simmate.team@gmail.com to discuss joining our effort. This will let you avoid the complexities of managing your own database. If you decide to join, you'll only have to complete steps 3 and 4 of this tutorial. Set up a cloud database that is supported by django . We highly recommend setting up a connection pool for your database as well. If you need help with this setup, you can use our \"deploy\" button in the next section. 10GB is plenty to get started. Make sure you have extra database dependencies installed. For postgres, run the command: conda install -n my_env -c conda-forge psycopg2 Add the file ~/simmate/my_env-database.yaml with your connection details that match django format . As an example, this my_env-database.yaml file gives a default database to use: default : ENGINE : django.db.backends.postgresql_psycopg2 HOST : simmate-database-do-user-8843535-0.b.db.ondigitalocean.com NAME : simmate-database-00-pool USER : doadmin PASSWORD : ryGEc5PDxC2IHDSM PORT : 25061 OPTIONS : sslmode : require And if (and only if) you built a brand new database in step 2, reset your database in order to build initial tables. Use the command simmate database reset to do this. do NOT run this command if you joined a collaborative database! Set up using Digital Ocean \u00b6 We recommend using Postgres through DigitalOcean . If you do not have a Digital Ocean account, we ask that you sign up using our referral link . The button below will then take you to the relevant page. Note, we are not affiliated with Digital Ocean -- it's just what our team happens to use.","title":"Quickstart"},{"location":"getting_started/use_a_cloud_database/quick_start/#use-a-cloud-database","text":"In this tutorial, you will learn how to switch from saving results locally to saving them to a collaborative remote (or \"cloud\") database.","title":"Use a cloud database"},{"location":"getting_started/use_a_cloud_database/quick_start/#the-quick-tutorial","text":"Consider collaborating! Simmate is built for sharing results, so email simmate.team@gmail.com to discuss joining our effort. This will let you avoid the complexities of managing your own database. If you decide to join, you'll only have to complete steps 3 and 4 of this tutorial. Set up a cloud database that is supported by django . We highly recommend setting up a connection pool for your database as well. If you need help with this setup, you can use our \"deploy\" button in the next section. 10GB is plenty to get started. Make sure you have extra database dependencies installed. For postgres, run the command: conda install -n my_env -c conda-forge psycopg2 Add the file ~/simmate/my_env-database.yaml with your connection details that match django format . As an example, this my_env-database.yaml file gives a default database to use: default : ENGINE : django.db.backends.postgresql_psycopg2 HOST : simmate-database-do-user-8843535-0.b.db.ondigitalocean.com NAME : simmate-database-00-pool USER : doadmin PASSWORD : ryGEc5PDxC2IHDSM PORT : 25061 OPTIONS : sslmode : require And if (and only if) you built a brand new database in step 2, reset your database in order to build initial tables. Use the command simmate database reset to do this. do NOT run this command if you joined a collaborative database!","title":"The quick tutorial"},{"location":"getting_started/use_a_cloud_database/quick_start/#set-up-using-digital-ocean","text":"We recommend using Postgres through DigitalOcean . If you do not have a Digital Ocean account, we ask that you sign up using our referral link . The button below will then take you to the relevant page. Note, we are not affiliated with Digital Ocean -- it's just what our team happens to use.","title":"Set up using Digital Ocean"}]}